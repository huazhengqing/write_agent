import os
import json
import asyncio
import functools
from loguru import logger
import torch
import litellm
from diskcache import Cache
from pydantic import BaseModel, Field
from typing import List, TypedDict, Optional, Any, TYPE_CHECKING
from langchain_community.utilities import SearxSearchWrapper
from sentence_transformers import SentenceTransformer, util
from utils.models import Task
from utils.prompt_loader import load_prompts
from utils.llm import get_llm_messages, get_llm_params, llm_acompletion, LLM_TEMPERATURES, Embedding_PARAMS
from utils.rag import get_rag


###############################################################################


# æ¯ä¸ª(å­)ä»»åŠ¡çš„æœ€å¤§æœç´¢-æŠ“å–-è§„åˆ’å¾ªç¯æ¬¡æ•°, ç”¨äºé˜²æ­¢æ— é™å¾ªç¯ã€‚
MAX_SEARCH_TURNS = 3
# å¹¶å‘æŠ“å–ç½‘é¡µçš„æœ€å¤§æ•°é‡, ä»¥æ§åˆ¶èµ„æºä½¿ç”¨å’Œé¿å…å¯¹ç›®æ ‡æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›ã€‚
MAX_SCRAPE_CONCURRENCY = 5
# å•ä¸ªç½‘ç»œè¯·æ±‚(å¦‚æŠ“å–ç½‘é¡µ)çš„è¶…æ—¶æ—¶é—´(ç§’)ã€‚
REQUEST_TIMEOUT = 30
# httpx æŠ“å–æ—¶è®¤ä¸ºå†…å®¹æœ‰æ•ˆçš„æœ€å°é•¿åº¦, ç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦é™çº§åˆ° Playwright
MIN_CONTENT_LENGTH_FOR_HTTPX = 100


###############################################################################


class Plan(BaseModel):
    thought: str = Field(description="ä½ çš„æ€è€ƒè¿‡ç¨‹, åˆ†æå·²æœ‰ä¿¡æ¯å’Œä¸‹ä¸€æ­¥è®¡åˆ’ã€‚")
    queries: List[str] = Field(description="æ ¹æ®æ€è€ƒç”Ÿæˆçš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ã€‚å¦‚æœè®¤ä¸ºä¿¡æ¯è¶³å¤Ÿ, åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚")

class ProcessedContent(BaseModel):
    url: str = Field(description="å†…å®¹çš„åŸå§‹URLã€‚")
    relevance_score: float = Field(description="å†…å®¹ä¸ç ”ç©¶ç„¦ç‚¹çš„ç›¸å…³æ€§å¾—åˆ†(0.0åˆ°1.0), åˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ã€‚")
    summary: str = Field(description="æå–æˆ–ç”Ÿæˆçš„æ ¸å¿ƒå†…å®¹æ‘˜è¦, åº”å»é™¤å™ªéŸ³å¹¶çªå‡ºå…³é”®ä¿¡æ¯ã€‚")
    is_relevant: bool = Field(description="å†…å®¹æ˜¯å¦ä¸ç ”ç©¶ç„¦ç‚¹ç›´æ¥ç›¸å…³ã€‚")

class ProcessedResults(BaseModel):
    processed_contents: List[ProcessedContent] = Field(description="å¤„ç†å’Œè¯„ä¼°åçš„å†…å®¹åˆ—è¡¨ã€‚")


###############################################################################


class SearchAgentState(TypedDict):
    # --- æ ¸å¿ƒä»»åŠ¡ä¿¡æ¯ ---
    task: Task                          # ä¼ å…¥çš„åŸå§‹ä»»åŠ¡å¯¹è±¡, ç”¨äºè·å–ä¸Šä¸‹æ–‡å’Œå­˜å‚¨è®°å¿†ã€‚    
    current_focus: str                  # å½“å‰ç ”ç©¶å¾ªç¯çš„å…·ä½“ç„¦ç‚¹, åœ¨å¤æ‚ä»»åŠ¡ä¸­å¯èƒ½åªæ˜¯åŸå§‹ä»»åŠ¡çš„ä¸€éƒ¨åˆ†ã€‚
    final_report: Optional[str]         # æœ€ç»ˆç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Š, åœ¨ synthesize_node ä¸­å¡«å……ã€‚
    # --- ç ”ç©¶å¾ªç¯çŠ¶æ€ (ç®€å•è·¯å¾„å’Œå¤æ‚è·¯å¾„çš„å­ä»»åŠ¡å¾ªç¯å…±ç”¨) ---
    plan: Plan                          # å½“å‰çš„è¡ŒåŠ¨è®¡åˆ’(æ€è€ƒ+æŸ¥è¯¢), ç”± planner_node ç”Ÿæˆã€‚
    urls_to_scrape: Optional[List[str]] # ä» search_node è¿”å›çš„å¾…æŠ“å–URLåˆ—è¡¨ã€‚
    latest_scraped_content: List[dict]  # ä»æœ€æ–°ä¸€æ¬¡ scrape_node è¿è¡Œä¸­æŠ“å–çš„åŸå§‹å†…å®¹ã€‚
    latest_processed_content: List[dict] # ä»æœ€æ–°ä¸€æ¬¡ information_processor_node è¿è¡Œä¸­å¤„ç†è¿‡çš„å†…å®¹ã€‚
    processed_content_accumulator: List[dict] # ç´¯ç§¯æ‰€æœ‰è½®æ¬¡ä¸­ç»è¿‡å¤„ç†å’Œç­›é€‰çš„ç›¸å…³ä¿¡æ¯ã€‚
    previous_rolling_summary: Optional[str] # ä¸Šä¸€è½®çš„æ»šåŠ¨æ€»ç»“, ç”¨äºæ£€æµ‹ç ”ç©¶æ˜¯å¦åœæ»ã€‚
    rolling_summary: Optional[str]      # æ»šåŠ¨æ€»ç»“, ç”± rolling_summary_node ç”Ÿæˆ, æŒç»­æ›´æ–°çš„çŸ¥è¯†åº“, ç”¨äºæŒ‡å¯¼ä¸‹ä¸€è½®è§„åˆ’ã€‚
    turn_count: int                     # å½“å‰ä»»åŠ¡/å­ä»»åŠ¡çš„ç ”ç©¶å¾ªç¯è½®æ¬¡è®¡æ•°, ç”¨äºé˜²æ­¢æ— é™å¾ªç¯ã€‚
    reasoning_history: List[str]        # è®°å½•â€œæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿâ€é“¾æ¡, ç”¨äºæ„å»ºä¸Šä¸‹æ–‡å’Œæœ€ç»ˆçš„æ¨ç†è¿‡ç¨‹å±•ç¤ºã€‚


###############################################################################


SEARCH_CACHE = Cache(os.path.join(".cache", 'search_cache'), size_limit=int(128 * 1024 * 1024))
SCRAPE_CACHE = Cache(os.path.join(".cache", 'scrape_cache'), size_limit=int(128 * 1024 * 1024))
_local_embedding_model: Optional[SentenceTransformer] = None

_search_tool_instance = SearxSearchWrapper(searx_host=os.environ.get("SearXNG", "http://127.0.0.1:8080"))


def async_retry(retries=3, backoff_in_seconds=1):
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            x = 0
            while True:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if x == retries:
                        logger.error(f"å‡½æ•° {func.__name__} åœ¨ {retries} æ¬¡é‡è¯•åå¤±è´¥ã€‚")
                        raise e
                    
                    sleep = backoff_in_seconds * (2 ** x)
                    await asyncio.sleep(sleep)
                    x += 1
        return wrapper
    return decorator


async def scrape_webpages(urls: List[str]) -> List[dict]:
    """
    ä½¿ç”¨æ··åˆç­–ç•¥å¹¶å‘æŠ“å–ç½‘é¡µå†…å®¹ã€‚
    1.  é¦–å…ˆå°è¯•ä½¿ç”¨ `httpx` è¿›è¡Œå¿«é€Ÿã€è½»é‡çº§çš„æŠ“å–ã€‚
    2.  å¦‚æœ `httpx` æŠ“å–å¤±è´¥(å¦‚ç½‘ç»œé”™è¯¯)æˆ–æå–çš„å†…å®¹è¿‡çŸ­(é€šå¸¸æ„å‘³ç€é¡µé¢éœ€è¦JSæ¸²æŸ“), 
        åˆ™è‡ªåŠ¨é™çº§åˆ°ä½¿ç”¨ `Playwright` è¿›è¡Œæ·±åº¦æŠ“å–, å®ƒå¯ä»¥æ‰§è¡ŒJavaScriptã€‚
    è¿™ç§ç­–ç•¥æ—¨åœ¨å…¼é¡¾é€Ÿåº¦å’ŒæŠ“å–æˆåŠŸç‡ã€‚
    """
    import httpx
    from trafilatura import extract
    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

    if not urls:
        return []

    async def scrape_single_hybrid(client, browser, url: str) -> Optional[dict]:
        """å¯¹å•ä¸ªURLæ‰§è¡Œæ··åˆæŠ“å–ç­–ç•¥ã€‚"""
        # 1. æ£€æŸ¥ç¼“å­˜
        if url in SCRAPE_CACHE:
            logger.info(f"ç¼“å­˜å‘½ä¸­ (æŠ“å–): {url}")
            return SCRAPE_CACHE[url]

        # 2. ä¼˜å…ˆä½¿ç”¨ httpx å°è¯•å¿«é€ŸæŠ“å–
        httpx_content = None
        final_url = url
        try:
            logger.info(f"ä½¿ç”¨ httpx å¿«é€ŸæŠ“å–: {url}")
            response = await client.get(url)
            response.raise_for_status()
            httpx_content = extract(response.text)
            final_url = str(response.url)
        except httpx.RequestError as e:
            logger.warning(f"âš ï¸ httpx è¯·æ±‚å¤±è´¥ ({e}), å°†é™çº§åˆ° Playwright: {url}")
        except Exception as e:
            logger.warning(f"âš ï¸ httpx å¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({e}), å°†é™çº§åˆ° Playwright: {url}")

        # å¦‚æœ httpx æˆåŠŸä¸”å†…å®¹è¶³å¤Ÿ, ç›´æ¥è¿”å›
        if httpx_content and len(httpx_content) >= MIN_CONTENT_LENGTH_FOR_HTTPX:
            logger.info(f"âœ… httpx æŠ“å–æˆåŠŸ: {url}")
            result = {"url": final_url, "content": httpx_content}
            SCRAPE_CACHE[url] = result
            return result
        elif httpx_content:
            logger.info(f"âš ï¸ httpx å†…å®¹è¿‡çŸ­, é™çº§åˆ° Playwright: {url}")

        # 3. å¦‚æœ httpx å¤±è´¥æˆ–å†…å®¹ä¸è¶³, é™çº§åˆ° Playwright
        page = None
        try:
            logger.info(f"ä½¿ç”¨ Playwright æ·±åº¦æŠ“å–: {url}")
            page = await browser.new_page()
            await page.route("**/*", lambda route: route.abort() if route.request.resource_type in {"image", "stylesheet", "font", "media"} else route.continue_())
            await page.goto(url, wait_until="domcontentloaded", timeout=REQUEST_TIMEOUT * 1000)
            await asyncio.sleep(3)
            
            html_content = await page.content()
            text_content = extract(html_content)

            if text_content and len(text_content) >= MIN_CONTENT_LENGTH_FOR_HTTPX:
                logger.info(f"âœ… Playwright æŠ“å–æˆåŠŸ: {url}")
                result = {"url": page.url, "content": text_content}
                SCRAPE_CACHE[url] = result
                return result
            else:
                logger.warning(f"Playwright æŠ“å–åå†…å®¹ä»è¿‡çŸ­æˆ–å¤±è´¥: {url}")
                return None
        except (PlaywrightTimeoutError, Exception) as e:
            logger.warning(f"Playwright å¤„ç† URL æ—¶å‘ç”Ÿé”™è¯¯: {url}, é”™è¯¯: {e}")
            return None
        finally:
            if page:
                await page.close()

    # --- å¹¶å‘æ‰§è¡Œä¸ä¸Šä¸‹æ–‡ç®¡ç† ---
    semaphore = asyncio.Semaphore(MAX_SCRAPE_CONCURRENCY)

    async def scrape_with_semaphore(client, browser, url: str) -> Optional[dict]:
        async with semaphore:
            return await scrape_single_hybrid(client, browser, url)

    # åŒæ—¶ç®¡ç† httpx å’Œ playwright çš„ä¸Šä¸‹æ–‡
    # å®‰å…¨åœ°ä»ç¯å¢ƒå˜é‡ä¸­è·å– SSL éªŒè¯è®¾ç½®, é»˜è®¤ä¸º True
    verify_ssl = os.environ.get("verify_ssl", "false").lower() == "true"
    async with async_playwright() as p, httpx.AsyncClient(http2=True, verify=verify_ssl, follow_redirects=True, timeout=REQUEST_TIMEOUT) as client:
        browser = await p.chromium.launch(headless=True)
        try:
            tasks = [scrape_with_semaphore(client, browser, url) for url in urls]
            results = await asyncio.gather(tasks)
            return [res for res in results if res is not None]
        finally:
            await browser.close()


###############################################################################


PROMPT_STAGNATION_DETECTION = """
# ä»»åŠ¡: è¯„ä¼°ä¿¡æ¯å¢ç›Š
å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªæ€»ç»“, åˆ¤æ–­â€œå½“å‰æ€»ç»“â€æ˜¯å¦æ¯”â€œä¸Šä¸€è½®æ€»ç»“â€æä¾›äº†æ˜¾è‘—çš„ã€æœ‰ä»·å€¼çš„æ–°ä¿¡æ¯ã€‚

## ä¸Šä¸€è½®æ€»ç»“:
{prev_summary}

## å½“å‰æ€»ç»“:
{current_summary}

å¦‚æœâ€œå½“å‰æ€»ç»“â€æ²¡æœ‰æä¾›æ˜¾è‘—æ–°ä¿¡æ¯ (ä¾‹å¦‚, åªæ˜¯é‡è¿°ã€ç»†åŒ–æˆ–è¡¥å……äº†æ— å…³ç´§è¦çš„ç»†èŠ‚), åˆ™åˆ¤å®šä¸ºåœæ»ã€‚

åœæ»äº†å—? (åªå›ç­” true æˆ– false)
"""

###############################################################################


# å›¾èŠ‚ç‚¹

async def get_structured_output_with_retry(messages: List[dict], response_model: BaseModel, temperature: float):
    """
    ä¸€ä¸ªç”¨äºè·å–ç»“æ„åŒ–è¾“å‡ºçš„åŒ…è£…å‡½æ•°, å®ƒä¼šé…ç½®LLMä½¿ç”¨å·¥å…·è°ƒç”¨, 
    ç„¶åè°ƒç”¨å¸¦æœ‰å†…ç½®é‡è¯•å’ŒéªŒè¯é€»è¾‘çš„ `llm_acompletion`ã€‚
    """
    # `litellm` çš„ `response_model` å‚æ•°ä¼šè‡ªåŠ¨å¤„ç†å·¥å…·çš„åˆ›å»ºå’Œé€‰æ‹©, æ— éœ€æ‰‹åŠ¨æ„å»º `tools` å’Œ `tool_choice`
    llm_params = get_llm_params(messages, temperature=temperature)
    message = await llm_acompletion(llm_params, response_model=response_model)
    return getattr(message, 'validated_data', None)

async def planner_node(state: SearchAgentState) -> dict:
    """
    è§„åˆ’èŠ‚ç‚¹, æ˜¯ç ”ç©¶å¾ªç¯çš„æ ¸å¿ƒã€‚
    å®ƒèšåˆäº†æœ€å…³é”®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯(ä»»åŠ¡æè¿°ã€æ»šåŠ¨æ€»ç»“ã€æœ€è¿‘çš„å†å²è®°å½•ç­‰), 
    ç„¶åè°ƒç”¨ LLM éµå¾ªâ€œåˆ†æ-ç­–ç•¥-è¡ŒåŠ¨â€çš„æ¡†æ¶, ç”Ÿæˆä¸‹ä¸€æ­¥çš„æ€è€ƒå’Œæœç´¢æŸ¥è¯¢ã€‚
    å¦‚æœ LLM è®¤ä¸ºå½“å‰ç„¦ç‚¹çš„ä¿¡æ¯å·²è¶³å¤Ÿ, å®ƒå°†è¿”å›ä¸€ä¸ªç©ºçš„æŸ¥è¯¢åˆ—è¡¨, ä»è€Œè§¦å‘ç ”ç©¶å¾ªç¯çš„ç»ˆæ­¢ã€‚
    """
    turn = state['turn_count'] + 1
    task = state['task']

    logger.info(f"â–¶ï¸ 1. è¿›å…¥è§„åˆ’èŠ‚ç‚¹ (ç¬¬ {turn} è½®)...")
    
    context_dict = {
        'current_focus': state['current_focus'], 
        'rolling_summary': state.get('rolling_summary') or "æ— , è¿™æ˜¯ç¬¬ä¸€æ¬¡ç ”ç©¶, è¯·å¼€å§‹æ¢ç´¢ã€‚",
        # åªä¼ é€’æœ€è¿‘2è½®çš„æ€è€ƒå†å², ä»¥ç²¾ç®€ä¸Šä¸‹æ–‡, é¿å…LLMåœ¨é•¿å¯¹è¯ä¸­è¿·å¤±
        'reasoning_history': "\n\n".join(state['reasoning_history'][-2:]) or "æ— ",
    }

    SYSTEM_PROMPT, USER_PROMPT = load_prompts(task.category, "search_cn", "SYSTEM_PROMPT", "USER_PROMPT")
    context = await get_rag().get_context_base(task)
    context.update(context_dict)
    messages = get_llm_messages(SYSTEM_PROMPT, USER_PROMPT, None, context)
    plan = await get_structured_output_with_retry(messages, Plan, temperature=LLM_TEMPERATURES["reasoning"])
    
    # å¦‚æœè§£æå¤±è´¥, åˆ›å»ºä¸€ä¸ªç©ºçš„ Plan å¯¹è±¡ä»¥é¿å…ä¸‹æ¸¸èŠ‚ç‚¹å‡ºé”™
    if not plan:
        plan = Plan(thought="è§„åˆ’å¤±è´¥, å°è¯•ç»ˆæ­¢å½“å‰ç ”ç©¶ã€‚", queries=[])

    logger.info(f"ğŸ¤” æ€è€ƒ: {plan.thought}")
    logger.info(f"ğŸ” ç”ŸæˆæŸ¥è¯¢: {plan.queries}")

    # 5. æ›´æ–°å›¾çŠ¶æ€, åŒ…æ‹¬æ–°çš„è®¡åˆ’å’Œå¢åŠ çš„è½®æ¬¡è®¡æ•°
    return {"plan": plan, "turn_count": turn}

@async_retry(retries=2, backoff_in_seconds=2)
async def _search_with_retry(query: str):
    return _search_tool_instance.results(query)

async def search_node(state: SearchAgentState) -> dict:
    queries = state['plan'].queries
    logger.info(f"â–¶ï¸ 2. è¿›å…¥æœç´¢èŠ‚ç‚¹, æ‰§è¡ŒæŸ¥è¯¢: {queries}...")
    
    try:
        # 1. ä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ›å»ºä¸€ä¸ªå¼‚æ­¥æœç´¢ä»»åŠ¡, å¹¶å¤„ç†ç¼“å­˜
        search_tasks = []
        for query in queries:
            if query in SEARCH_CACHE:
                logger.info(f"ç¼“å­˜å‘½ä¸­ (æœç´¢): {query}")
                future = asyncio.Future()
                future.set_result(SEARCH_CACHE[query])
                search_tasks.append(future)
            else:
                search_tasks.append(_search_with_retry(query))

        # 2. ä½¿ç”¨ asyncio.gather å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
        # ä½¿ç”¨ return_exceptions=True, è¿™æ ·å³ä½¿ä¸ªåˆ«æœç´¢å¤±è´¥, ä¹Ÿä¸ä¼šä¸­æ–­æ•´ä¸ªæ‰¹æ¬¡ã€‚
        results_or_exceptions = await asyncio.gather(*search_tasks, return_exceptions=True)

        # 3. æå–æ‰€æœ‰URLå¹¶æ ¼å¼åŒ–ç»“æœç”¨äºæ—¥å¿—å’Œå†å²è®°å½•
        all_urls = []
        all_search_results_str_parts = []
        for query, result_or_exc in zip(queries, results_or_exceptions):
            if isinstance(result_or_exc, Exception):
                logger.error(f"æœç´¢æŸ¥è¯¢ '{query}' å¤±è´¥: {result_or_exc}")
                all_search_results_str_parts.append(f"æŸ¥è¯¢ '{query}' çš„ç»“æœ: å¤±è´¥ ({result_or_exc})")
                continue
            
            results = result_or_exc
            # æ›´æ–°ç¼“å­˜
            SEARCH_CACHE[query] = results
            
            query_urls = [res["link"] for res in results if res.get("link")]
            all_urls.extend(query_urls)
            
            # æ ¼å¼åŒ–å­—ç¬¦ä¸²ç»“æœç”¨äºå†å²è®°å½•
            formatted_results = "\n".join([f"Title: {res['title']}, Link: {res['link']}" for res in results])
            all_search_results_str_parts.append(f"æŸ¥è¯¢ '{query}' çš„ç»“æœ:\n{formatted_results}")

        all_search_results_str = "\n\n".join(all_search_results_str_parts)
        history_entry = f"--- è½®æ¬¡ {state['turn_count']} ---\næ€è€ƒ: {state['plan'].thought}\nè¡ŒåŠ¨: æ‰§è¡Œæœç´¢æŸ¥è¯¢ {queries}\nè§‚å¯Ÿ: (æœç´¢ç»“æœæ‘˜è¦)\n{all_search_results_str}"

        # 5. æ›´æ–°å›¾çŠ¶æ€
        return {
            "urls_to_scrape": all_urls, # ç›´æ¥ä¼ é€’URLåˆ—è¡¨
            "reasoning_history": state['reasoning_history'] + [history_entry]
        }
    except Exception as e:
        # æ•è·æœç´¢è¿‡ç¨‹ä¸­çš„ä»»ä½•å¼‚å¸¸
        logger.error(f"æ‰§è¡Œç½‘ç»œæœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {"urls_to_scrape": []}

async def scrape_node(state: SearchAgentState) -> dict:
    """
    æŠ“å–èŠ‚ç‚¹, è´Ÿè´£ä»æœç´¢ç»“æœä¸­æå– URL å¹¶å¹¶å‘æŠ“å–ç½‘é¡µå†…å®¹ã€‚
    - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¨³å¥åœ°è§£æ URLã€‚
    - ä½¿ç”¨ `scrape_webpages` è¾…åŠ©å‡½æ•°è¿›è¡Œå¸¦ç¼“å­˜ã€é‡è¯•å’Œå¹¶å‘æ§åˆ¶çš„å¼‚æ­¥æŠ“å–ã€‚
    - å°†æŠ“å–åˆ°çš„å†…å®¹æ‘˜è¦è®°å½•åˆ°æ¨ç†å†å²ä¸­ã€‚
    """
    logger.info("â–¶ï¸ 3. è¿›å…¥æŠ“å–èŠ‚ç‚¹...")
    try:
        # 1. ç›´æ¥ä» search_node è·å– URL åˆ—è¡¨
        urls_to_scrape = state.get('urls_to_scrape')
        if not urls_to_scrape:
            logger.warning("åœ¨æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°å¯æŠ“å–çš„ URLã€‚")
            return {}

        logger.info(f"ğŸ” å‘ç° {len(urls_to_scrape)} ä¸ª URL, å¼€å§‹æŠ“å–...")

        # 2. å¼‚æ­¥æŠ“å–æ‰€æœ‰ URL
        scraped_data = await scrape_webpages(urls_to_scrape)
        logger.info(f"âœ… æˆåŠŸæŠ“å– {len(scraped_data)} ä¸ªé¡µé¢ã€‚")

        # 3. æ›´æ–°å›¾çŠ¶æ€, ç”¨æ–°æŠ“å–çš„å†…å®¹è¦†ç›–æ—§çš„
        # æ³¨æ„: æ­¤å¤„çš„æ¨ç†å†å²è®°å½•å·²è¢«ç®€åŒ–, å…¶æ ¸å¿ƒä½œç”¨ç”± search_node å’Œ rolling_summary_node æ‰¿æ‹…, 
        # ä»¥é¿å…å†å²è®°å½•å†—ä½™ã€‚
        return {
            "latest_scraped_content": scraped_data,
        }
    except Exception as e:
        # æ•è·æŠ“å–è¿‡ç¨‹ä¸­çš„ä»»ä½•å¼‚å¸¸
        logger.error(f"æŠ“å–ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}

async def information_processor_node(state: SearchAgentState) -> dict:
    """
    ä¿¡æ¯å¤„ç†èŠ‚ç‚¹, å¯¹æŠ“å–åˆ°çš„åŸå§‹ç½‘é¡µå†…å®¹è¿›è¡Œè¯„ä¼°å’Œæç‚¼ã€‚
    - è°ƒç”¨ LLM å¯¹æ¯ä»½å†…å®¹è¿›è¡Œç›¸å…³æ€§æ‰“åˆ†å’Œåˆ¤æ–­ã€‚
    - æå–ç›¸å…³å†…å®¹çš„æ ¸å¿ƒæ‘˜è¦, å»é™¤å™ªéŸ³ã€‚
    - å°†å¤„ç†åçš„ç›¸å…³ä¿¡æ¯è¿½åŠ åˆ° `processed_content_accumulator` ä¸­, ä»¥ä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨ã€‚
    """
    logger.info("â–¶ï¸ 4. è¿›å…¥ä¿¡æ¯å¤„ç†èŠ‚ç‚¹...")
    
    if not state.get('latest_scraped_content'):
        logger.info("... æ²¡æœ‰æ–°çš„æŠ“å–å†…å®¹éœ€è¦å¤„ç†, è·³è¿‡ã€‚")
        return {}
    
    # å‡†å¤‡ Prompt
    # å°†æŠ“å–å†…å®¹åˆ—è¡¨è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ä»¥ä¾¿æ³¨å…¥ Prompt
    scraped_content_json = json.dumps(
        [{"url": item["url"], "content": item["content"]} for item in state['latest_scraped_content']],
        ensure_ascii=False,
        indent=2
    )

    task = state['task']
    
    PROMPT_INFORMATION_PROCESSOR = load_prompts(task.category, "search_cn", "PROMPT_INFORMATION_PROCESSOR")
    prompt = PROMPT_INFORMATION_PROCESSOR.format(
        research_focus=state["current_focus"],
        rolling_summary=state["rolling_summary"] or "",
        content=scraped_content_json
    )

    # è°ƒç”¨ LLM
    messages = [{"role": "user", "content": prompt}]
    processed_results = await get_structured_output_with_retry(messages, ProcessedResults, temperature=LLM_TEMPERATURES["reasoning"])

    if not processed_results or not processed_results.processed_contents:
        logger.warning("ä¿¡æ¯å¤„ç†èŠ‚ç‚¹æœªèƒ½ä»LLMè·å¾—æœ‰æ•ˆçš„å¤„ç†ç»“æœã€‚")
        return {}

    # è¿‡æ»¤ä¸ç›¸å…³çš„, å¹¶æŒ‰ç›¸å…³æ€§æ’åº
    relevant_content = [item.model_dump() for item in processed_results.processed_contents if item.is_relevant]
    relevant_content.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    logger.info(f"âœ… ä¿¡æ¯å¤„ç†å®Œæˆ, è·å¾— {len(relevant_content)} æ¡ç›¸å…³ä¿¡æ¯ã€‚")

    # æ›´æ–°çŠ¶æ€, å°†æ–°å¤„ç†çš„å†…å®¹è¿½åŠ åˆ°ç´¯åŠ å™¨ä¸­, å¹¶å•ç‹¬å­˜æ”¾ä»¥ä¾›æ»šåŠ¨æ€»ç»“èŠ‚ç‚¹ä½¿ç”¨
    return {
        "processed_content_accumulator": state['processed_content_accumulator'] + relevant_content,
        "latest_processed_content": relevant_content
    }

async def rolling_summary_node(state: SearchAgentState) -> dict:
    """
    æ»šåŠ¨æ€»ç»“èŠ‚ç‚¹, å®ç°â€œåæ€-è§„åˆ’â€å¾ªç¯çš„å…³é”®ã€‚é‡‡ç”¨â€œå¢é‡ç²¾ç‚¼â€æ¨¡å¼ã€‚
    - åœ¨æ¯è½®ä¿¡æ¯å¤„ç†åè°ƒç”¨ã€‚
    - å®ƒæ¥æ”¶â€œä¸Šä¸€è½®çš„æ‘˜è¦â€å’Œâ€œæœ¬è½®æ–°å¢çš„ä¿¡æ¯â€ã€‚
    - è°ƒç”¨ LLM å°†æ–°ä¿¡æ¯æ•´åˆè¿›æ—§æ‘˜è¦, ç”Ÿæˆä¸€ä»½æ›´æ–°ã€å»é‡åçš„æ‘˜è¦, å¹¶è¯†åˆ«ä¿¡æ¯ç¼ºå£ã€‚
    - è¿™ç§æ–¹å¼é¿å…äº†å°†æ‰€æœ‰å†å²ä¿¡æ¯é‡å¤å‘é€ç»™LLM, æœ‰æ•ˆæ§åˆ¶äº†ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
    """
    # ä»…å½“æœ‰æ–°å¤„ç†è¿‡çš„å†…å®¹æ—¶æ‰è§¦å‘æ­¤èŠ‚ç‚¹ã€‚
    if not state.get('latest_processed_content'):
        logger.info("... æ²¡æœ‰æ–°å¤„ç†çš„ä¿¡æ¯, è·³è¿‡æ»šåŠ¨æ€»ç»“ã€‚")
        # å³ä½¿è·³è¿‡, ä¹Ÿè¦ç¡®ä¿ previous_rolling_summary è¢«ä¼ é€’, é¿å…çŠ¶æ€ä¸¢å¤±
        return {}

    logger.info("â–¶ï¸ 5. è¿›å…¥æ»šåŠ¨æ€»ç»“èŠ‚ç‚¹...")

    # åœ¨ç”Ÿæˆæ–°æ€»ç»“ä¹‹å‰, ä¿å­˜å½“å‰çš„æ€»ç»“ä½œä¸ºâ€œä¸Šä¸€è½®â€çš„æ€»ç»“
    previous_summary = state.get('rolling_summary') or "æ— , è¿™æ˜¯ç ”ç©¶çš„å¼€å§‹ã€‚"

    # åªä½¿ç”¨æœ¬è½®æ–°å¢çš„ä¿¡æ¯è¿›è¡Œç²¾ç‚¼
    new_info_list = state.get('latest_processed_content', [])
    new_info_str = "\n\n---\n\n".join(
        f"æ¥æº URL: {item['url']}\nç›¸å…³æ€§: {item.get('relevance_score', 'N/A')}\næ‘˜è¦:\n{item['summary']}"
        for item in new_info_list
    )

    if not new_info_str:
        logger.info("... æœ¬è½®æœªå‘ç°æ–°çš„æœ‰æ•ˆä¿¡æ¯, è·³è¿‡æ»šåŠ¨æ€»ç»“ã€‚")
        return {"previous_rolling_summary": previous_summary}

    task = state['task']
    
    PROMPT_ROLLING_SUMMARY = load_prompts(task.category, "search_cn", "PROMPT_ROLLING_SUMMARY")
    prompt = PROMPT_ROLLING_SUMMARY.format(
        research_focus=state["current_focus"],
        previous_summary=previous_summary,
        new_information=new_info_str
    )

    llm_params = get_llm_params([{"role": "user", "content": prompt}], temperature=LLM_TEMPERATURES["summarization"])
    message = await llm_acompletion(llm_params)
    summary = message.content

    logger.info(f"ğŸ”„ ç”Ÿæˆæ»šåŠ¨æ€»ç»“: {summary[:200]}...")
    return {"rolling_summary": summary, "previous_rolling_summary": previous_summary}

async def synthesize_node(state: SearchAgentState) -> dict:
    """
    ç»¼åˆæŠ¥å‘ŠèŠ‚ç‚¹, æ˜¯å·¥ä½œæµçš„ç»ˆç‚¹ä¹‹ä¸€ã€‚
    - åŸºäºæœ€ç»ˆçš„æ»šåŠ¨æ€»ç»“ä½œä¸ºä¸»è¦å†…å®¹, å¹¶è¾…ä»¥æ‰€æœ‰å¤„ç†è¿‡çš„æ‘˜è¦ä½œä¸ºè¡¥å……ææ–™ã€‚
    - è¿™ç§æ–¹æ³•åœ¨ä¿è¯æŠ¥å‘Šè´¨é‡çš„åŒæ—¶, æ˜¾è‘—å‡å°‘äº†æœ€ç»ˆPromptçš„Tokenæ¶ˆè€—ã€‚
    - è°ƒç”¨ LLM ç”Ÿæˆä¸€ä»½å…¨é¢ã€è¿è´¯çš„æœ€ç»ˆç ”ç©¶æŠ¥å‘Š, å¹¶è¦æ±‚æŒ‡å‡ºä¿¡æ¯å†²çªã€‚
    """
    logger.info("â–¶ï¸ 6. è¿›å…¥ç»¼åˆæŠ¥å‘ŠèŠ‚ç‚¹...")

    supplementary_summaries = "\n\n---\n\n".join(
        f"æ¥æº URL: {item['url']}\næ‘˜è¦:\n{item['summary']}"
        for item in state['processed_content_accumulator']
    ) or "æ— è¡¥å……ææ–™ã€‚"

    task = state['task']
    context_dict = {
        'current_focus': state['current_focus'],
        'rolling_summary': state.get('rolling_summary') or "ç ”ç©¶æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦ã€‚", 
        'supplementary_summaries': supplementary_summaries, 
    }
    
    SYSTEM_PROMPT, USER_PROMPT = load_prompts(task.category, "search_cn", "SYSTEM_PROMPT_SYNTHESIZE", "USER_PROMPT_SYNTHESIZE")
    context = await get_rag().get_context_base(task)
    context.update(context_dict)
    messages = get_llm_messages(SYSTEM_PROMPT, USER_PROMPT, None, context)
    llm_params = get_llm_params(messages, temperature=LLM_TEMPERATURES["synthesis"])
    message = await llm_acompletion(llm_params)
    final_report = message.content

    logger.info("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚")

    # 3. æ›´æ–°å›¾çŠ¶æ€
    return {"final_report": final_report}


###############################################################################


# æ¡ä»¶è·¯ç”±å‡½æ•°

async def should_continue_search(state: SearchAgentState) -> str:
    """
    æ¡ä»¶è·¯ç”±å‡½æ•°: åœ¨è§„åˆ’åå†³å®šæ˜¯ç»§ç»­ç ”ç©¶å¾ªç¯è¿˜æ˜¯ç»“æŸã€‚
    - å¦‚æœ `plan.queries` ä¸ºç©º, è¡¨ç¤ºè§„åˆ’å™¨è®¤ä¸ºä¿¡æ¯è¶³å¤Ÿ, ç»“æŸå¾ªç¯ã€‚
    - å¦‚æœè¾¾åˆ°æœ€å¤§æœç´¢è½®æ¬¡, ä¸ºé˜²æ­¢æ— é™å¾ªç¯, å¼ºåˆ¶ç»“æŸã€‚
    - æ–°å¢: å¦‚æœç ”ç©¶åœæ»(æ–°æ—§æ€»ç»“æ— æ˜¾è‘—å·®å¼‚), ä¹Ÿç»“æŸå¾ªç¯ã€‚è¿™é€šè¿‡ä¸¤æ­¥å®ç°: 
      1. è½»é‡çº§çš„Jaccardç›¸ä¼¼åº¦æ£€æŸ¥, å¿«é€Ÿè¿‡æ»¤æ‰å‡ ä¹ç›¸åŒçš„æ€»ç»“ã€‚
      2. å¦‚æœä¸å¤Ÿç›¸ä¼¼, åˆ™é€šè¿‡LLMè¿›è¡Œæ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰åˆ¤æ–­ã€‚
    """
    # æ¡ä»¶3: ç ”ç©¶åœæ»æ£€æµ‹
    prev_summary = state.get('previous_rolling_summary')
    current_summary = state.get('rolling_summary')

    # åªæœ‰åœ¨æœ‰ä¸¤è½®æ€»ç»“å¯æ¯”è¾ƒæ—¶æ‰è¿›è¡Œ
    if prev_summary and current_summary and state['turn_count'] > 1:
        # ä¼˜åŒ–: åœ¨è°ƒç”¨æ˜‚è´µçš„LLMä¹‹å‰, å…ˆè¿›è¡Œé«˜æ•ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ã€‚
        # è¿™æ¯”ç®€å•çš„è¯æ±‡åŒ¹é…(å¦‚Jaccard)æ›´å‡†ç¡®, èƒ½æ›´å¥½åœ°åˆ¤æ–­å†…å®¹æ˜¯å¦çœŸçš„æ²¡æœ‰æ–°æ„ã€‚
        similarity_threshold = 0.98
        
        cosine_sim = 0.0
        try:
            params = Embedding_PARAMS.copy()
            params["input"] = [prev_summary, current_summary]
            response = await litellm.aembedding(**params)
            embedding_vectors = [item['embedding'] for item in response.data]
            embedding1 = torch.tensor(embedding_vectors[0])
            embedding2 = torch.tensor(embedding_vectors[1])
            cosine_sim = util.pytorch_cos_sim(embedding1, embedding2).item()
        except Exception as e:
            logger.error(f"è®¡ç®—æ‘˜è¦ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}ã€‚è·³è¿‡åœæ»æ£€æµ‹ã€‚")

        logger.info(f"æ–°æ—§æ‘˜è¦çš„è¯­ä¹‰ç›¸ä¼¼åº¦: {cosine_sim:.4f}")

        if cosine_sim > similarity_threshold:
            logger.info(f"â¹ï¸ ç ”ç©¶åœæ», æ–°æ—§æ‘˜è¦è¯­ä¹‰ç›¸ä¼¼åº¦ ({cosine_sim:.4f}) é«˜äºé˜ˆå€¼ ({similarity_threshold}), ç»“æŸç ”ç©¶ã€‚")
            return "end_task"

        # å¦‚æœè¯­ä¹‰ç›¸ä¼¼åº¦ä¸é«˜, å†ä½¿ç”¨LLMè¿›è¡Œæœ€ç»ˆçš„ã€æ›´æ·±å±‚æ¬¡çš„åˆ¤æ–­
        logger.info("...æ‘˜è¦ç›¸ä¼¼åº¦ä¸é«˜, ä½¿ç”¨ LLM è¿›è¡Œæ·±åº¦åœæ»æ£€æµ‹...")
        prompt = PROMPT_STAGNATION_DETECTION.format(
            prev_summary=prev_summary,
            current_summary=current_summary
        )
        llm_params = get_llm_params([{"role": "user", "content": prompt}], temperature=LLM_TEMPERATURES["classification"])
        
        def stagnation_validator(content: str):
            """éªŒè¯LLMçš„å“åº”æ˜¯å¦ä¸º 'true' æˆ– 'false'ã€‚"""
            content = content.strip().lower()
            if content not in ['true', 'false']:
                raise ValueError(f"æ— æ•ˆçš„åœæ»æ£€æµ‹å“åº”: '{content}'")

        try:
            message = await llm_acompletion(llm_params, validator=stagnation_validator)
            is_stagnant = message.content.strip().lower() == 'true'
        except Exception as e:
            logger.error(f"åœæ»æ£€æµ‹åœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥: {e}ã€‚å°†é»˜è®¤ç»§ç»­ç ”ç©¶ä»¥é¿å…å¡æ­»ã€‚")
            is_stagnant = False # é»˜è®¤ä¸ºä¸-åœæ», é¿å…å› æ£€æµ‹å¤±è´¥è€Œå¡ä½

        if is_stagnant:
            logger.info("â¹ï¸ ç ”ç©¶åœæ»(LLMåˆ¤æ–­), æ–°ä¸€è½®æœªå‘ç°æ˜¾è‘—ä¿¡æ¯, ç»“æŸå½“å‰ä»»åŠ¡ç ”ç©¶ã€‚")
            return "end_task"

    # æ¡ä»¶1: è§„åˆ’å™¨è®¤ä¸ºä¿¡æ¯å·²è¶³å¤Ÿ, ä¸»åŠ¨åœæ­¢ã€‚
    if not state['plan'].queries:
        logger.info("â¹ï¸ è§„åˆ’å™¨å†³å®šç»“æŸå½“å‰ä»»åŠ¡ç ”ç©¶ã€‚")
        return "end_task"
    
    # æ¡ä»¶2: è¾¾åˆ°é¢„è®¾çš„æœ€å¤§æœç´¢è½®æ¬¡, è¢«åŠ¨åœæ­¢ã€‚
    if state['turn_count'] >= MAX_SEARCH_TURNS:
        logger.info(f"â¹ï¸ å·²è¾¾åˆ°æœ€å¤§æœç´¢è½®æ¬¡ ({MAX_SEARCH_TURNS}), ç»“æŸå½“å‰ä»»åŠ¡ç ”ç©¶ã€‚")
        return "end_task"
    
    # é»˜è®¤: ç»§ç»­ç ”ç©¶å¾ªç¯ã€‚
    logger.info("â†ªï¸ ç»§ç»­ä¸ºå½“å‰ä»»åŠ¡è¿›è¡Œæ–°ä¸€è½®æœç´¢ã€‚")
    return "continue_search"


###############################################################################


async def search(task: Task) -> Task:
    """
    1.  å¯åŠ¨ (Entry Point): å·¥ä½œæµä» `planner` èŠ‚ç‚¹å¼€å§‹ã€‚
    2.  ç ”ç©¶å¾ªç¯ (The Main Loop):
        -   `planner`: åˆ¶å®šæœç´¢è®¡åˆ’ã€‚
        -   `should_continue_search` (æ¡ä»¶è¾¹): åˆ¤æ–­æ˜¯å¦ç»§ç»­ã€‚
            -   è‹¥ç»§ç»­ (`continue_search`): è¿›å…¥ `search`ã€‚
            -   è‹¥ç»“æŸ (`end_task`): è·³è½¬åˆ° `synthesize`ã€‚
        -   `search` -> `scrape` -> `information_processor` -> `rolling_summary` -> `planner`: æ„æˆç ”ç©¶å¾ªç¯ã€‚
    3.  æŠ¥å‘Šç”Ÿæˆä¸ç»“æŸ (Termination):
        -   `synthesize`: æ±‡é›†æ‰€æœ‰ä¿¡æ¯, ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š, æµç¨‹ç»“æŸ (`END`)ã€‚
    """
    logger.info(f"å¼€å§‹\n{task.model_dump_json(indent=2, exclude_none=True)}")
    
    from langgraph.graph import StateGraph, END
    
    # 1. å®šä¹‰å·¥ä½œæµå›¾ (StateGraph)
    workflow = StateGraph(SearchAgentState)

    # 2. å‘å›¾ä¸­æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("planner", planner_node)
    workflow.add_node("search", search_node)
    workflow.add_node("scrape", scrape_node)    
    workflow.add_node("information_processor", information_processor_node)
    workflow.add_node("rolling_summary", rolling_summary_node) # æ–°å¢æ»šåŠ¨æ€»ç»“èŠ‚ç‚¹
    workflow.add_node("synthesize", synthesize_node)
    
    # 3. å®šä¹‰æµç¨‹çš„è¾¹ (Edges), å³èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»
    workflow.set_entry_point("planner")
    
    # "planner" èŠ‚ç‚¹åçš„æ¡ä»¶åˆ†æ”¯, å†³å®šæ˜¯ç»§ç»­æœç´¢ã€ç»“æŸç®€å•ä»»åŠ¡è¿˜æ˜¯ç»“æŸå­ä»»åŠ¡ã€‚
    workflow.add_conditional_edges(
        "planner", # type: ignore
        should_continue_search,
        {
            "continue_search": "search", # ç»§ç»­æœç´¢å¾ªç¯
            "end_task": "synthesize",    # ä»»åŠ¡ç»“æŸ, ç”ŸæˆæŠ¥å‘Š
        }
    )
    
    # "search" -> "scrape" -> "information_processor" -> "rolling_summary" -> "planner" æ„æˆç ”ç©¶å¾ªç¯çš„ä¸»ä½“ã€‚
    workflow.add_edge("search", "scrape")
    workflow.add_edge("scrape", "information_processor")
    workflow.add_edge("information_processor", "rolling_summary")
    workflow.add_edge("rolling_summary", "planner") # æ€»ç»“åè¿”å›è§„åˆ’, å½¢æˆé—­ç¯
    
    # "synthesize" (ç”ŸæˆæŠ¥å‘Š) æ˜¯ç»ˆç‚¹èŠ‚ç‚¹, æµç¨‹åœ¨æ­¤ç»“æŸã€‚
    workflow.add_edge("synthesize", END)
    
    # 4. ç¼–è¯‘å›¾
    app = workflow.compile()
    
    # 5. åˆå§‹åŒ–çŠ¶æ€å¹¶è¿è¡Œå›¾
    initial_state = SearchAgentState(
        task=task,                              # æ ¸å¿ƒ: ä¼ å…¥çš„ä»»åŠ¡å¯¹è±¡
        current_focus=task.goal,                # æ ¸å¿ƒ: å½“å‰ç ”ç©¶å¾ªç¯çš„ç„¦ç‚¹, åˆå§‹ä¸ºä»»åŠ¡ç›®æ ‡
        final_report=None,                      # æœ€ç»ˆæŠ¥å‘Š, åˆå§‹ä¸ºç©º
        plan=Plan(thought="", queries=[]),      # å½“å‰çš„è¡ŒåŠ¨è®¡åˆ’, åˆå§‹ä¸ºç©º
        urls_to_scrape=[],                      # å¾…æŠ“å–çš„URLåˆ—è¡¨
        latest_scraped_content=[],              # æœ€æ–°æŠ“å–çš„ç½‘é¡µå†…å®¹åˆ—è¡¨
        latest_processed_content=[],            # æœ€æ–°å¤„ç†çš„å†…å®¹
        processed_content_accumulator=[],       # ç´¯ç§¯çš„å¤„ç†åå†…å®¹
        previous_rolling_summary=None,          # ä¸Šä¸€è½®æ€»ç»“, åˆå§‹ä¸ºç©º
        rolling_summary="",                     # æ»šåŠ¨æ€»ç»“, åˆå§‹ä¸ºç©ºå­—ç¬¦ä¸²
        turn_count=0,                           # å½“å‰ä»»åŠ¡/å­ä»»åŠ¡çš„æœç´¢è½®æ¬¡è®¡æ•°
        reasoning_history=[]                    # è®°å½•æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿçš„é“¾æ¡
    )
    
    try:
        final_state = await app.ainvoke(initial_state)
    except Exception as e:
        logger.error(f"æ‰§è¡Œç ”ç©¶ä»»åŠ¡ '{task.goal}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)

        updated_task = task.model_copy(deep=True)
        updated_task.results = {
            "result": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}",
            "reasoning": "",
        }
        return updated_task
    
    # 6. æ±‡æ€»æ¨ç†å†å², ç”Ÿæˆå¯è¯»çš„æ‰§è¡Œè½¨è¿¹
    reasoning_str = "\n\n".join(final_state.get('reasoning_history', []))

    # 7. æ›´æ–°ä»»åŠ¡å¯¹è±¡å¹¶è¿”å›ç»“æœ
    updated_task = task.model_copy(deep=True)
    updated_task.results["search"] = final_state['final_report']
    updated_task.results["search_reasoning"] = reasoning_str

    logger.info(f"å®Œæˆ\n{updated_task.model_dump_json(indent=2, exclude_none=True)}")
    return updated_task


###############################################################################
