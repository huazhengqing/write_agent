from functools import lru_cache
from pathlib import Path
from loguru import logger
from typing import Callable, Dict, List
from llama_index.core import Document
from llama_index.core import Settings
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.schema import BaseNode



from rag.vector import init_llama_settings
init_llama_settings()



@lru_cache(maxsize=30)
def file_metadata_default(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    from datetime import datetime
    stat = file_path.stat()
    return {
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S"),
        "modification_date": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
    }



@lru_cache(maxsize=30)
def _load_and_filter_documents(
    input_dir: str,
    metadata_func: Callable[[str], dict]
) -> List[Document]:
    logger.info(f"å¼€å§‹ä»ç›®å½• '{input_dir}' åŠ è½½å’Œè¿‡æ»¤æ–‡æ¡£...")
    from llama_index.core import SimpleDirectoryReader
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=file_metadata_default,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return []
    
    logger.debug(f"ä» '{input_dir}' åˆå§‹åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£ã€‚")
    valid_docs = []
    for doc in documents:
        file_path_str = doc.metadata.get("file_path") 
        if not file_path_str:
            logger.warning(f"æ–‡æ¡£ç¼ºå°‘æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ID, è·³è¿‡ã€‚å…ƒæ•°æ®: {doc.metadata}")
            continue
        
        file_path = Path(file_path_str)
        custom_meta = metadata_func(file_path_str)
        doc.metadata.update(custom_meta)
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©º, å·²è·³è¿‡ã€‚")
            continue
        
        valid_docs.append(doc)
    
    logger.success(f"å®Œæˆæ–‡æ¡£åŠ è½½å’Œè¿‡æ»¤, å…±è·å¾— {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ã€‚")
    return valid_docs



def _parse_docs_to_nodes_by_format(documents: List[Document]) -> List[BaseNode]:
    from rag.splitter import get_vector_node_parser
    logger.info("å¼€å§‹æŒ‰æ–‡ä»¶æ ¼å¼è§£ææ–‡æ¡£ä¸ºèŠ‚ç‚¹...")
    docs_by_format: Dict[str, List[Document]] = {
        "md": [], 
        "txt": [], 
        "json": []
    }
    for doc in documents:
        file_name = doc.metadata.get("file_name", "")
        file_extension = Path(file_name).suffix.lstrip('.')
        if file_extension in docs_by_format:
            docs_by_format[file_extension].append(doc)
        else:
            logger.warning(f"æ£€æµ‹åˆ°æœªæ”¯æŒçš„æ–‡ä»¶æ‰©å±•å '{file_extension}', å°†å¿½ç•¥ã€‚")

    all_nodes = []
    from rag.vector_add import filter_invalid_nodes
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨å¤„ç† {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶...")
        nodes_for_format = []
        for doc in format_docs:
            node_parser = get_vector_node_parser(content_format, content_length=len(doc.text))
            parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
            nodes_for_format.extend(filter_invalid_nodes(parsed_nodes))
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)
    
    logger.success(f"æ–‡æ¡£è§£æå®Œæˆ, æ€»å…±ç”Ÿæˆ {len(all_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    return all_nodes



def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    metadata_func: Callable[[str], dict] = file_metadata_default,
) -> bool:
    logger.info(f"å¼€å§‹ä»ç›®å½• '{input_dir}' æ·»åŠ å†…å®¹åˆ°å‘é‡åº“...")
    documents = _load_and_filter_documents(input_dir, metadata_func)
    if not documents:
        return False

    all_nodes = _parse_docs_to_nodes_by_format(documents)
    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    if not unique_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ è¿‡æ»¤åæ²¡æœ‰å”¯ä¸€çš„èŠ‚ç‚¹å¯ä¾›ç´¢å¼•ã€‚")
        return False

    from llama_index.core.ingestion import IngestionPipeline
    pipeline = IngestionPipeline(vector_store=vector_store, transformations=[Settings.embed_model])
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True
