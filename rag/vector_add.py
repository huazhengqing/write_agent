from typing import Any, Dict, List, Literal, Optional
from loguru import logger
from llama_index.core import Document, Settings
from llama_index.core.schema import BaseNode
from llama_index.core.vector_stores.types import VectorStore



from rag.vector import init_llama_settings
init_llama_settings()



def filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    valid_nodes = []
    initial_count = len(nodes)
    for node in nodes:
        import re
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    removed_count = initial_count - len(valid_nodes)
    if removed_count > 0:
        logger.debug(f"è¿‡æ»¤æ‰ {removed_count} ä¸ªæ— æ•ˆæˆ–ç©ºèŠ‚ç‚¹ã€‚")
    return valid_nodes



def _parse_content_to_nodes(
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"],
    doc_id: Optional[str] = None,
) -> List[BaseNode]:
    logger.info(f"å¼€å§‹ä¸º doc_id '{doc_id}' è§£æå†…å®¹ä¸ºèŠ‚ç‚¹ (æ ¼å¼: {content_format})...")
    from datetime import datetime
    if "time" not in metadata:
        metadata["time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    doc = Document(text=content, metadata=metadata, id_=doc_id)
    from rag.splitter import get_vector_node_parser
    node_parser = get_vector_node_parser(content_format, content_length=len(content))
    nodes = filter_invalid_nodes(node_parser.get_nodes_from_documents([doc], show_progress=False))
    logger.info(f"ä¸º doc_id '{doc_id}' è§£æå‡º {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    return nodes



def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"] = "md",
    doc_id: Optional[str] = None,
) -> bool:
    logger.info(f"å¼€å§‹å‘å‘é‡åº“æ·»åŠ å†…å®¹, doc_id='{doc_id}', format='{content_format}'...")
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯, è·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False

    import hashlib
    new_content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
    doc_cache = getattr(vector_store, "cache", None)
    if doc_cache and doc_cache.get(new_content_hash):
        return True

    effective_doc_id = doc_id or new_content_hash

    nodes_to_insert = _parse_content_to_nodes(content, metadata, content_format, effective_doc_id)
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (id: {effective_doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹, è·³è¿‡æ·»åŠ ã€‚")
        return False

    if doc_id:
        logger.info(f"ä¸º doc_id '{doc_id}' æ‰§è¡Œæ›´æ–°æ“ä½œ, å°†é¦–å…ˆåˆ é™¤æ—§èŠ‚ç‚¹ã€‚")
        vector_store.delete(doc_id)

    from llama_index.core.ingestion import IngestionPipeline
    pipeline = IngestionPipeline(vector_store=vector_store, transformations=[Settings.embed_model])
    pipeline.run(nodes=nodes_to_insert)

    if doc_cache:
        doc_cache.set(new_content_hash, True)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (id: {effective_doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True



def vector_delete(
    vector_store: VectorStore,
    doc_id: str,
) -> None:
    """ä»å‘é‡åº“ä¸­åˆ é™¤æŒ‡å®š doc_id çš„æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹ã€‚"""
    logger.info(f"å¼€å§‹ä»å‘é‡åº“åˆ é™¤å†…å®¹, doc_id='{doc_id}'...")
    if not doc_id:
        logger.warning("doc_id ä¸ºç©º, è·³è¿‡åˆ é™¤ã€‚")
        return
    
    vector_store.delete(doc_id)
    logger.success(f"æˆåŠŸä»å‘é‡åº“ä¸­åˆ é™¤ doc_id='{doc_id}' çš„ç›¸å…³èŠ‚ç‚¹ã€‚")
