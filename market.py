import os
import json
import asyncio
from dotenv import load_dotenv
from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_community.embeddings.litellm import LiteLLMEmbeddings
from langchain_community.vectorstores import Chroma
from pathlib import Path
from datetime import datetime
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.documents import Document
from utils.llm import get_llm_params, get_embedding_params, get_llm_messages, llm_acompletion


load_dotenv()
embedding_params = get_embedding_params(embedding='bge-m3')
embeddings = LiteLLMEmbeddings(**embedding_params)
search_tool = TavilySearchResults(max_results=5)
chroma_db_path = ".chroma_db/story"
Path(chroma_db_path).mkdir(parents=True, exist_ok=True)
vector_store = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings)


class BroadScanReport(BaseModel):
    platform: str = Field(..., description="å¹³å°åç§°")
    main_tone: str = Field(..., description="ä¸€å¥è¯æ€»ç»“å¹³å°ä¸»æµé£æ ¼")
    hot_genres: List[str] = Field(..., description="åˆ—å‡º3-5ä¸ªå½“å‰æœ€çƒ­é—¨çš„é¢˜æå¤§ç±»")
    official_direction: str = Field(..., description="æ€»ç»“è¿‘æœŸçš„å®˜æ–¹å¾æ–‡ã€æ¿€åŠ±æ´»åŠ¨æ–¹å‘ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º'æ— '")
    opportunity_assessment: str = Field(..., description="å¯¹å½“å‰åœ¨è¯¥å¹³å°å‘å±•çš„æœºä¼šè¿›è¡Œä¸€å¥è¯è¯„ä¼°")

class BestOpportunity(BaseModel):
    platform: str = Field(..., description="é€‰æ‹©çš„å¹³å°åç§°")
    genre: str = Field(..., description="å»ºè®®ä»è¯¥å¹³å°å…¥æ‰‹çš„é¢˜æå¤§ç±»")
    reason: str = Field(..., description="åšå‡ºæ­¤é€‰æ‹©çš„ç†ç”±")


# å¹¿åŸŸæ‰«æçš„æç¤ºè¯
BROAD_SCAN_SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½å®è§‚å¸‚åœºç­–ç•¥å¸ˆï¼Œä¸“ç²¾äºç½‘ç»œå°è¯´å¹³å°åˆ†æã€‚

# ä»»åŠ¡
æ ¹æ®æä¾›çš„ã€å¹³å°åç§°ã€‘å’Œã€å®æ—¶æœç´¢æ•°æ®ã€‘ï¼Œåˆ†æè¯¥å¹³å°çš„æ•´ä½“å¸‚åœºç¯å¢ƒã€‚

# è¾“å‡ºè¦æ±‚
ä¸¥æ ¼æŒ‰ç…§Pydanticæ¨¡å‹çš„JSONæ ¼å¼è¾“å‡ºã€‚
"""
BROAD_SCAN_USER_PROMPT = """
# å¹³å°åç§°
{platform}

# å®æ—¶æœç´¢æ•°æ®
{search_results}
"""

# å†³ç­–æœ€ä½³æœºä¼šçš„æç¤ºè¯
CHOOSE_BEST_OPPORTUNITY_SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„ç½‘æ–‡å¸‚åœºæˆ˜ç•¥å®¶ï¼Œæ‹¥æœ‰æ•é”çš„å•†ä¸šå—…è§‰ã€‚

# ä»»åŠ¡
æ ¹æ®æä¾›çš„ã€å¹¿åŸŸæ‰«æå¯¹æ¯”æŠ¥å‘Šã€‘ï¼Œåˆ†æå¹¶é€‰æ‹©ä¸€ä¸ªæœ€å…·æ½œåŠ›çš„å¹³å°å’Œé¢˜ææ–¹å‘è¿›è¡Œæ·±åº¦é’»å–ã€‚
ä½ çš„å†³ç­–åº”åŸºäºæŠ¥å‘Šä¸­çš„â€œçƒ­é—¨é¢˜æâ€å’Œâ€œæœºä¼šè¯„ä¼°â€ã€‚

# è¾“å‡ºè¦æ±‚
ä¸¥æ ¼æŒ‰ç…§Pydanticæ¨¡å‹çš„JSONæ ¼å¼è¾“å‡ºã€‚
"""
CHOOSE_BEST_OPPORTUNITY_USER_PROMPT = """
# å¹¿åŸŸæ‰«æå¯¹æ¯”æŠ¥å‘Š
{platform_reports}
"""


# æ·±åº¦é’»å–çš„æç¤ºè¯ (ä¸åŸç‰ˆç±»ä¼¼ï¼Œä½†æ›´èšç„¦)
DEEP_DIVE_SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„ç½‘ç»œå°è¯´å¸‚åœºåˆ†æå¸ˆï¼Œå¯¹ã€{platform}ã€‘å¹³å°çš„ã€{genre}ã€‘é¢˜ææœ‰æ·±å…¥ç ”ç©¶ã€‚

# ä»»åŠ¡
æ ¹æ®æä¾›çš„ã€å®æ—¶æœç´¢æ•°æ®ã€‘ï¼ˆåŒ…å«çƒ­é—¨ä½œå“ã€è¯„è®ºæ‘˜è¦ï¼‰ï¼Œäº§å‡ºä¸€ä»½å…³äºè¯¥ç»†åˆ†å¸‚åœºçš„æ·±åº¦æ´å¯ŸæŠ¥å‘Šã€‚

# è¾“å‡ºè¦æ±‚ (Markdownæ ¼å¼)
## ã€{platform}ã€‘å¹³å° - ã€{genre}ã€‘é¢˜ææ·±åº¦åˆ†ææŠ¥å‘Š

### 1. æ ¸å¿ƒæ ‡ç­¾ä¸å…ƒç´ 
- [æç‚¼3-5ä¸ªæœ€å…³é”®çš„æ ‡ç­¾]

### 2. æ ¸å¿ƒçˆ½ç‚¹ä¸è¯»è€…å¿ƒç†
- [æç‚¼2-3ä¸ªæœ€å—è¿½æ§çš„çˆ½ç‚¹ï¼Œå¹¶åˆ†æèƒŒåå¿ƒç†]

### 3. æ–°å…´æœºä¼šä¸è“æµ·æ–¹å‘
- [å‘ç°æ•°æ®ä¸­æš—ç¤ºçš„ã€å°šæœªé¥±å’Œçš„é¢˜æç»„åˆæˆ–åˆ›æ–°æ–¹å‘]

### 4. å¸¸è§â€œæ¯’ç‚¹â€ä¸é£é™©è§„é¿
- [æ€»ç»“è¯»è€…æœ€åæ„Ÿçš„3ä¸ªæƒ…èŠ‚æˆ–è®¾å®š]
"""
DEEP_DIVE_USER_PROMPT = """
# å®æ—¶æœç´¢æ•°æ®
{search_results}
"""

# æœºä¼šç”Ÿæˆçš„æç¤ºè¯
OPPORTUNITY_GENERATION_SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„é‡‘ç‰Œå°è¯´ç­–åˆ’äººã€‚

# ä»»åŠ¡
æ ¹æ®æˆ‘æä¾›çš„ã€å¸‚åœºæ·±åº¦åˆ†ææŠ¥å‘Šã€‘ï¼Œæ¿€å‘åˆ›æ„ï¼Œæ„æ€ 3 ä¸ªå…·æœ‰å•†ä¸šæ½œåŠ›çš„å°è¯´é€‰é¢˜ã€‚

# è¾“å‡ºè¦æ±‚ (Markdownæ ¼å¼)
ä¸ºæ¯ä¸ªé€‰é¢˜æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
- **é€‰é¢˜åç§°**: [ä¸€ä¸ªå¸å¼•äººçš„åå­—]
- **æ ¸å¿ƒåˆ›æ„**: [ä¸€å¥è¯æ¦‚æ‹¬æ•…äº‹æœ€æœ‰è¶£çš„ç‚¹]
- **ä¸»è§’è®¾å®š**: [ç®€è¦æè¿°ä¸»è§’çš„èº«ä»½å’Œç‰¹ç‚¹]
- **æ ¸å¿ƒå†²çª**: [æ•…äº‹çš„ä¸»è¦çŸ›ç›¾æ˜¯ä»€ä¹ˆ]
- **å¸‚åœºè¯„çº§**: [S/A/Bçº§]
- **è¯„çº§ç†ç”±**: [ç»“åˆåˆ†ææŠ¥å‘Šï¼Œè§£é‡Šä¸ºä»€ä¹ˆè¿™ä¹ˆè¯„çº§]

"""
OPPORTUNITY_GENERATION_USER_PROMPT = """
---
# å¸‚åœºæ·±åº¦åˆ†ææŠ¥å‘Š
{market_report}
"""

# å°è¯´åˆ›æ„ç”Ÿæˆæç¤ºè¯
NOVEL_CONCEPT_SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é¡¶çº§å°è¯´ç¼–è¾‘å’Œç­–åˆ’äººï¼Œæ“…é•¿å°†ä¸€ä¸ªå¥½çš„ç‚¹å­æ‰©å±•æˆä¸€ä¸ªå®Œæ•´ä¸”å¸å¼•äººçš„æ•…äº‹æ¦‚å¿µã€‚

# ä»»åŠ¡
æ ¹æ®æˆ‘æä¾›çš„ã€åˆæ­¥é€‰é¢˜åˆ—è¡¨ã€‘ï¼Œè¯·é€‰æ‹©å…¶ä¸­å¸‚åœºè¯„çº§æœ€é«˜çš„é‚£ä¸ªé€‰é¢˜ï¼Œå¹¶å°†å…¶æ‰©å±•ä¸ºä¸€ä¸ªæ›´è¯¦ç»†çš„ã€å°è¯´åˆ›æ„ã€‘ã€‚

# è¾“å‡ºè¦æ±‚ (Markdownæ ¼å¼)
## å°è¯´åˆ›æ„ï¼š[é€‰é¢˜åç§°]

### 1. ä¸€å¥è¯ç®€ä»‹ (Logline)
- [ç”¨ä¸€å¥è¯æ¦‚æ‹¬æ•…äº‹çš„æ ¸å¿ƒå–ç‚¹ï¼Œä½¿å…¶å¬èµ·æ¥éå¸¸å¸å¼•äºº]

### 2. è¯¦ç»†æ•…äº‹æ¢—æ¦‚ (Synopsis)
- [ç”¨200-300å­—è¯¦ç»†æè¿°æ•…äº‹çš„èµ·å› ã€å‘å±•å’Œæ ¸å¿ƒå†²çªã€‚ä¸»è§’å¦‚ä½•è·å¾—èƒ½åŠ›/æœºé‡ï¼Œä»–é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Œæ•…äº‹çš„é«˜æ½®å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ]

### 3. ä¸»è§’è®¾å®š (Character Profile)
- **èƒŒæ™¯ä¸åŠ¨æœº**: [ä¸»è§’çš„å‡ºèº«ã€èŒä¸šã€ä»¥åŠä»–å†…å¿ƒæœ€æ·±çš„æ¸´æœ›æˆ–ææƒ§æ˜¯ä»€ä¹ˆï¼Ÿ]
- **æ€§æ ¼ä¸èƒ½åŠ›**: [ä¸»è§’çš„æ€§æ ¼ç‰¹ç‚¹ï¼ˆä¾‹å¦‚ï¼šæ€ä¼æœæ–­ã€è‹Ÿé“è‡³ä¸Šã€å¹½é»˜è…¹é»‘ï¼‰ï¼Œä»¥åŠä»–çš„æ ¸å¿ƒèƒ½åŠ›/é‡‘æ‰‹æŒ‡çš„å…·ä½“è®¾å®š]
- **æˆé•¿å¼§å…‰**: [åœ¨æ•…äº‹çš„æœ€åï¼Œä¸»è§’ä¼šåœ¨æ€æƒ³æˆ–èƒ½åŠ›ä¸Šè·å¾—æ€æ ·çš„æˆé•¿ï¼Ÿ]

### 4. æ ¸å¿ƒçœ‹ç‚¹ä¸å–ç‚¹ (Key Selling Points)
- [åˆ—å‡º3-4ä¸ªèƒ½å¸å¼•ç›®æ ‡è¯»è€…çš„å…³é”®å…ƒç´ ï¼Œä¾‹å¦‚ï¼šåˆ›æ–°çš„ç³»ç»Ÿè®¾å®šã€æè‡´çš„æƒ…ç»ªåè½¬ã€æ–°é¢–çš„ä¸–ç•Œè§‚ã€ç‹¬ç‰¹çš„çˆ½ç‚¹èŠ‚å¥ç­‰]

### 5. å¼€ç¯‡ç« èŠ‚æ„æ€ (Opening Chapter Idea)
- [è®¾è®¡ä¸€ä¸ªæŠ“äººçœ¼çƒçš„å¼€ç¯‡ã€‚ç¬¬ä¸€ç« åº”è¯¥å‘ç”Ÿä»€ä¹ˆäº‹ï¼Ÿå¦‚ä½•å¿«é€Ÿå±•ç°ä¸»è§’çš„ç‰¹ç‚¹ã€å¼•å…¥æ ¸å¿ƒè®¾å®šï¼Œå¹¶ç•™ä¸‹æ‚¬å¿µï¼Ÿ]

"""
NOVEL_CONCEPT_USER_PROMPT = """
---
# åˆæ­¥é€‰é¢˜åˆ—è¡¨
{selected_opportunity}
"""


async def broad_scan_platform(platform: str) -> dict:
    print(f"  - æ­£åœ¨æ‰«æå¹³å°: {platform}")
    query = f"{platform}å°è¯´çƒ­é—¨æ¦œå•ã€æ–°ä¹¦æ¦œã€å®˜æ–¹å¾æ–‡æ´»åŠ¨"
    # TavilySearchResults._arun è¿”å›çš„æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
    search_results = await search_tool.ainvoke(query)
    
    messages = get_llm_messages(
        SYSTEM_PROMPT=BROAD_SCAN_SYSTEM_PROMPT,
        USER_PROMPT=BROAD_SCAN_USER_PROMPT,
        context_dict_user={"platform": platform, "search_results": search_results}
    )
    llm_params = get_llm_params(llm='reasoning', temperature=0.3, messages=messages)
    
    try:
        response_message = await llm_acompletion(llm_params, response_model=BroadScanReport)
        return response_message.validated_data.model_dump()
    except Exception as e:
        print(f"  - è­¦å‘Š: {platform} å¹³å°åˆ†ææŠ¥å‘Šç”Ÿæˆæˆ–è§£æå¤±è´¥: {e}")
        return {"error": f"Failed to get or parse LLM output: {e}", "platform": platform}

async def broad_scan(platforms: list[str]) -> dict:
    print("ğŸš€ å¯åŠ¨å¹¿åŸŸæ‰«æ...")
    tasks = [broad_scan_platform(p) for p in platforms]
    results = await asyncio.gather(*tasks)
    
    scan_results = {}
    for res in results:
        # ä½¿ç”¨ res.get("platform", "unknown") æ¥å¤„ç†é”™è¯¯æƒ…å†µ
        platform_name = res.get("platform", "unknown")
        scan_results[platform_name] = res
            
    print("âœ… å¹¿åŸŸæ‰«æå®Œæˆï¼")
    return scan_results

async def choose_best_opportunity(platform_reports: dict) -> Optional[dict]:
    print("\nğŸš€ å†³ç­–æœ€ä½³å¸‚åœºæœºä¼š...")
    reports_str = json.dumps(platform_reports, indent=2, ensure_ascii=False)
    
    messages = get_llm_messages(
        SYSTEM_PROMPT=CHOOSE_BEST_OPPORTUNITY_SYSTEM_PROMPT,
        USER_PROMPT=CHOOSE_BEST_OPPORTUNITY_USER_PROMPT,
        context_dict_user={"platform_reports": reports_str}
    )
    llm_params = get_llm_params(llm='reasoning', temperature=0.3, messages=messages)

    try:
        response_message = await llm_acompletion(llm_params, response_model=BestOpportunity)
        choice = response_message.validated_data.model_dump()
        print(f"  - âœ… LLMå†³ç­–å®Œæˆï¼šé€‰æ‹©å¹³å° '{choice['platform']}'ï¼Œé¢˜æ '{choice['genre']}'ã€‚")
        print(f"  - ç†ç”±: {choice['reason']}")
        return choice
    except Exception as e:
        print(f"  - è­¦å‘Š: è§£æLLMå†³ç­–ç»“æœå¤±è´¥: {e}")
        return None

async def deep_dive_analysis(platform: str, genre: str) -> str:
    print(f"\nğŸš€ å¯¹ã€{platform} - {genre}ã€‘å¯åŠ¨æ·±åº¦é’»å–...")
    query = f"{platform}å°è¯´ {genre} é¢˜æçƒ­é—¨ä½œå“ã€è¯»è€…è¯„è®ºã€æ•…äº‹å¤§çº²"
    search_results = await search_tool.ainvoke(query)
    
    messages = get_llm_messages(
        SYSTEM_PROMPT=DEEP_DIVE_SYSTEM_PROMPT,
        USER_PROMPT=DEEP_DIVE_USER_PROMPT,
        context_dict_system={"platform": platform, "genre": genre},
        context_dict_user={"search_results": search_results}
    )
    llm_params = get_llm_params(llm='reasoning', temperature=0.3, messages=messages)
    response_message = await llm_acompletion(llm_params)
    report = response_message.content
    
    # å­˜å…¥å‘é‡æ•°æ®åº“
    report_doc = Document(page_content=report, metadata={"platform": platform, "genre": genre})
    await vector_store.aadd_documents([report_doc])
    print("  - âœ… æŠ¥å‘Šå·²å­˜å…¥å‘é‡æ•°æ®åº“ã€‚")
    
    # ä¿å­˜ä¸ºMarkdownæ–‡ä»¶
    report_dir = Path(".market/story/")
    report_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"{platform.replace(' ', '_')}_{genre.replace(' ', '_')}_{timestamp}.md" # noqa
    file_path = report_dir / file_name
    await asyncio.to_thread(file_path.write_text, report, encoding="utf-8")
    print(f"  - âœ… æŠ¥å‘Šå·²ä¿å­˜ä¸ºMarkdownæ–‡ä»¶: {file_path}")
    
    print("âœ… æ·±åº¦åˆ†æå®Œæˆï¼")
    return report

async def generate_opportunities(market_report: str) -> str:
    print("\nğŸš€ å¯åŠ¨åˆ›æ„è„‘æš´ï¼Œç”Ÿæˆå°è¯´é€‰é¢˜...")
    messages = get_llm_messages(
        SYSTEM_PROMPT=OPPORTUNITY_GENERATION_SYSTEM_PROMPT,
        USER_PROMPT=OPPORTUNITY_GENERATION_USER_PROMPT,
        context_dict_user={"market_report": market_report}
    )
    llm_params = get_llm_params(llm='reasoning', temperature=0.3, messages=messages)
    response_message = await llm_acompletion(llm_params)
    opportunities = response_message.content
    print("âœ… å°è¯´é€‰é¢˜ç”Ÿæˆå®Œæ¯•ï¼")
    return opportunities

async def generate_novel_concept(selected_opportunity: str) -> str:
    print("\nğŸš€ æ·±åŒ–é€‰é¢˜ï¼Œç”Ÿæˆè¯¦ç»†å°è¯´åˆ›æ„...")
    messages = get_llm_messages(
        SYSTEM_PROMPT=NOVEL_CONCEPT_SYSTEM_PROMPT,
        USER_PROMPT=NOVEL_CONCEPT_USER_PROMPT,
        context_dict_user={"selected_opportunity": selected_opportunity}
    )
    llm_params = get_llm_params(llm='reasoning', temperature=0.3, messages=messages)
    response_message = await llm_acompletion(llm_params)
    concept = response_message.content
    print("âœ… è¯¦ç»†å°è¯´åˆ›æ„ç”Ÿæˆå®Œæ¯•ï¼")
    return concept

async def query_reports(query: str, n_results: int = 1) -> list[Document]:
    print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢é¢˜æåº“ï¼Œé—®é¢˜: '{query}'")
    results = await vector_store.asimilarity_search(query, k=n_results)
    print(f"âœ… æŸ¥è¯¢åˆ° {len(results)} ä»½ç›¸å…³æŠ¥å‘Šã€‚")
    return results


async def main():
    # å¹¿åŸŸæ‰«æ
    platforms_to_scan = ["ç•ªèŒ„å°è¯´", "èµ·ç‚¹ä¸­æ–‡ç½‘"]
    platform_reports = await broad_scan(platforms_to_scan)
    print("\n--- å¹¿åŸŸæ‰«æå¯¹æ¯”æŠ¥å‘Š ---")
    print(json.dumps(platform_reports, indent=2, ensure_ascii=False))

    # ç”±LLMå†³ç­–æœ€ä½³æœºä¼š
    best_opportunity = await choose_best_opportunity(platform_reports)
    if best_opportunity:
        chosen_platform = best_opportunity["platform"]
        chosen_genre = best_opportunity["genre"]

        # æ·±åº¦é’»å–
        deep_dive_report = await deep_dive_analysis(chosen_platform, chosen_genre)
        print("\n--- æ·±åº¦åˆ†ææŠ¥å‘Š ---")
        print(deep_dive_report)

        # æœºä¼šç”Ÿæˆ
        final_opportunities = await generate_opportunities(deep_dive_report)
        print("\n--- å°è¯´é€‰é¢˜å»ºè®® ---")
        print(final_opportunities)

        # æ·±åŒ–åˆ›æ„
        # è®©AIé€‰æ‹©ä¸Šä¸€æ­¥ç”Ÿæˆçš„é€‰é¢˜åˆ—è¡¨ä¸­çš„æœ€ä¼˜è§£è¿›è¡Œæ·±åŒ–
        if final_opportunities:
            detailed_concept = await generate_novel_concept(final_opportunities)
            print("\n--- è¯¦ç»†å°è¯´åˆ›æ„ ---")
            print(detailed_concept)
    else:
        print("\næœªèƒ½ä»å¹¿åŸŸæ‰«æä¸­ç¡®å®šæœ€ä½³æœºä¼šï¼Œå·¥ä½œæµç»ˆæ­¢ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
