import os
import re
import sys
from typing import Any, Dict, List, Literal, Optional, Tuple

import kuzu
from loguru import logger
from llama_index.core import (
    Document,
    KnowledgeGraphIndex,
    Response,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.prompts import PromptTemplate
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import BaseSynthesizer, CompactAndRefine
from llama_index.core.vector_stores import MetadataFilters, VectorStore
from llama_index.graph_stores.kuzu import KuzuGraphStore
from llama_index.llms.litellm import LiteLLM

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from .llm import LLM_TEMPERATURES, get_llm_messages, get_llm_params, llm_completion
from .models import natural_sort_key
from .vector import get_embed_model
from utils.log import init_logger


def get_kuzu_graph_store(db_path: str) -> KuzuGraphStore:
    logger.info(f"æ­£åœ¨è®¿é—® KÃ¹zu å›¾æ•°æ®åº“: path='{db_path}'")
    os.makedirs(db_path, exist_ok=True)
    db = kuzu.Database(db_path)
    graph_store = KuzuGraphStore(db)
    logger.success(f"KÃ¹zu å›¾æ•°æ®åº“å·²åœ¨è·¯å¾„ '{db_path}' å‡†å¤‡å°±ç»ªã€‚")
    return graph_store


def store(
    storage_context: StorageContext,
    content: str,
    metadata: Dict[str, Any],
    doc_id: str,
    kg_extraction_prompt: str,
    content_format: Literal["markdown", "text", "json"] = "markdown",
    max_triplets_per_chunk: int = 15,
) -> None:
    """
    å°†å†…å®¹å­˜å‚¨åˆ°çŸ¥è¯†å›¾è°±ä¸­ã€‚

    Args:
        storage_context (StorageContext): LlamaIndexçš„å­˜å‚¨ä¸Šä¸‹æ–‡ã€‚
        content (str): è¦å­˜å‚¨çš„æ–‡æœ¬å†…å®¹ã€‚
        metadata (Dict[str, Any]): ä¸æ–‡æ¡£å…³è”çš„å…ƒæ•°æ®ã€‚
        doc_id (str): æ–‡æ¡£çš„å”¯ä¸€IDã€‚
        kg_extraction_prompt (str): ç”¨äºçŸ¥è¯†å›¾è°±æå–çš„æç¤ºæ¨¡æ¿ã€‚
        content_format (Literal["markdown", "text", "json"], optional): å†…å®¹æ ¼å¼ã€‚ Defaults to "markdown".
        max_triplets_per_chunk (int, optional): æ¯ä¸ªå—æœ€å¤šæå–çš„ä¸‰å…ƒç»„æ•°é‡ã€‚ Defaults to 15.
    """
    logger.info(f"å¼€å§‹ä¸ºæ–‡æ¡£ '{doc_id}' (æ ¼å¼: {content_format}) æ„å»ºçŸ¥è¯†å›¾è°±...")

    doc = Document(id_=doc_id, text=content, metadata=metadata)

    transformations = []
    if content_format == "markdown":
        transformations.append(MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True))
    elif content_format == "text":
        transformations.append(SentenceSplitter(chunk_size=512, chunk_overlap=100, include_metadata=True, include_prev_next_rel=True))
    elif content_format == "json":
        logger.info("å†…å®¹æ ¼å¼ä¸º 'json'ï¼Œå°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªèŠ‚ç‚¹å¤„ç†ã€‚")
    else:
        transformations.append(SentenceSplitter(chunk_size=512, chunk_overlap=100, include_metadata=True, include_prev_next_rel=True))

    llm_extract_params = get_llm_params(llm="fast", temperature=LLM_TEMPERATURES["summarization"])
    llm = LiteLLM(**llm_extract_params)

    KnowledgeGraphIndex.from_documents(
        [doc],
        storage_context=storage_context,
        llm=llm,
        embed_model=get_embed_model(),
        kg_extraction_prompt=PromptTemplate(kg_extraction_prompt),
        max_triplets_per_chunk=max_triplets_per_chunk,
        include_embeddings=True,
        transformations=transformations,
    )
    logger.success(f"æ–‡æ¡£ '{doc_id}' çš„çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆã€‚")



def _format_response_with_sorting(response: Response, sort_by: Literal["time", "narrative", "relevance"]) -> str:
    """
    sort_by (Literal): æ’åºç­–ç•¥: 'time' (æ—¶é—´å€’åº), 'narrative' (ç« èŠ‚é¡ºåº), 'relevance' (ç›¸å…³æ€§)ã€‚
    """
    if not response.source_nodes:
        return f"æœªæ‰¾åˆ°ç›¸å…³æ¥æºä¿¡æ¯, ä½†ç»¼åˆå›ç­”æ˜¯: \n{str(response)}"

    if sort_by == "narrative":
        sorted_nodes = sorted(
            list(response.source_nodes),
            key=lambda n: natural_sort_key(n.metadata.get("task_id", "")),
            reverse=False,  # æ­£åºæ’åˆ—
        )
        sort_description = "æŒ‰å°è¯´ç« èŠ‚é¡ºåºæ’åˆ— (ä»å‰åˆ°å)"
    elif sort_by == "time":
        sorted_nodes = sorted(
            list(response.source_nodes),
            key=lambda n: n.metadata.get("created_at", "1970-01-01T00:00:00"),
            reverse=True,  # å€’åºæ’åˆ—, æœ€æ–°çš„åœ¨å‰
        )
        sort_description = "æŒ‰æ—¶é—´å€’åºæ’åˆ— (æœ€æ–°çš„åœ¨å‰)"
    else:  # 'relevance' æˆ–å…¶ä»–é»˜è®¤æƒ…å†µ
        sort_description = "æŒ‰ç›¸å…³æ€§æ’åº"
        sorted_nodes = list(response.source_nodes)

    source_details = []
    for node in sorted_nodes:
        timestamp = node.metadata.get("created_at", "æœªçŸ¥æ—¶é—´")
        task_id = node.metadata.get("task_id", "æœªçŸ¥ç« èŠ‚")
        score = node.get_score()
        score_str = f"{score:.4f}" if score is not None else "N/A"
        content = re.sub(r"\s+", " ", node.get_content()).strip()
        source_details.append(
            f"æ¥æºä¿¡æ¯ (ç« èŠ‚: {task_id}, æ—¶é—´: {timestamp}, ç›¸å…³æ€§: {score_str}):\n---\n{content}\n---"
        )

    formatted_sources = "\n\n".join(source_details)

    final_output = (
        f"ç»¼åˆå›ç­”:\n{str(response)}\n\nè¯¦ç»†æ¥æº ({sort_description}):\n{formatted_sources}"
    )
    return final_output


def hybrid_query(
    vector_store: VectorStore,
    graph_store: KuzuGraphStore,
    retrieval_query_text: str,
    synthesis_query_text: str,
    synthesis_system_prompt: str,
    synthesis_user_prompt: str,
    kg_nl2graphquery_prompt: Optional[PromptTemplate] = None,
    vector_filters: Optional[MetadataFilters] = None,
    vector_similarity_top_k: int = 150,
    vector_rerank_top_n: int = 50,
    kg_similarity_top_k: int = 300,
    kg_rerank_top_n: int = 100,
    vector_sort_by: Literal["time", "narrative", "relevance"] = "relevance",
    kg_sort_by: Literal["time", "narrative", "relevance"] = "relevance",
) -> str:
    """
    æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„æ··åˆæŸ¥è¯¢æµç¨‹ï¼šå‘é‡æ£€ç´¢ -> çŸ¥è¯†å›¾è°±æ£€ç´¢ -> LLMç»¼åˆã€‚
    æ­¤å‡½æ•°å°è£…äº†ä»åˆ›å»ºæŸ¥è¯¢å¼•æ“åˆ°æœ€ç»ˆç­”æ¡ˆåˆæˆçš„æ‰€æœ‰æ­¥éª¤ã€‚

    Args:
        retrieval_query_text (str): ç”¨äºå‘é‡å’Œå›¾è°±æ£€ç´¢çš„æŸ¥è¯¢æ–‡æœ¬ã€‚
        synthesis_query_text (str): ç”¨äºæœ€ç»ˆLLMç»¼åˆçš„ã€æ›´è¯¦ç»†çš„ä»£ç†æŸ¥è¯¢æ–‡æœ¬ã€‚
        synthesis_system_prompt (str): ç»¼åˆé˜¶æ®µçš„ç³»ç»Ÿæç¤ºã€‚
        synthesis_user_prompt (str): ç»¼åˆé˜¶æ®µçš„ç”¨æˆ·æç¤ºæ¨¡æ¿ã€‚
        vector_store (VectorStore): ç”¨äºå‘é‡æ£€ç´¢çš„å‘é‡å­˜å‚¨ã€‚
        graph_store (KuzuGraphStore): ç”¨äºçŸ¥è¯†å›¾è°±æ£€ç´¢çš„å›¾å­˜å‚¨ã€‚
        kg_nl2graphquery_prompt (Optional[PromptTemplate], optional): KGä¸­NL2GraphQueryçš„æç¤ºã€‚ Defaults to None.
        vector_filters (Optional[MetadataFilters]): åº”ç”¨äºå‘é‡æ£€ç´¢çš„å…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚
        vector_similarity_top_k (int): å‘é‡æ£€ç´¢çš„top_kã€‚
        vector_rerank_top_n (int): å‘é‡æ£€ç´¢åLLMé‡æ’çš„top_nã€‚
        kg_similarity_top_k (int): KGæ··åˆæ£€ç´¢ä¸­å‘é‡éƒ¨åˆ†çš„top_kã€‚
        kg_rerank_top_n (int): KGæ£€ç´¢åLLMé‡æ’çš„top_nã€‚
        vector_sort_by (Literal): å‘é‡ç»“æœçš„æ’åºæ–¹å¼ã€‚
        kg_sort_by (Literal): çŸ¥è¯†å›¾è°±ç»“æœçš„æ’åºæ–¹å¼ã€‚

    Returns:
        str: LLMç»¼åˆåçš„æœ€ç»ˆç­”æ¡ˆã€‚
    """
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ··åˆæŸ¥è¯¢ï¼ˆå‘é‡ + çŸ¥è¯†å›¾è°±ï¼‰...")

    embed_model = get_embed_model()

    reasoning_llm_params = get_llm_params(llm="reasoning", temperature=LLM_TEMPERATURES["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)

    synthesis_llm_params = get_llm_params(llm="reasoning", temperature=LLM_TEMPERATURES["synthesis"])
    synthesis_llm = LiteLLM(**synthesis_llm_params)

    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params.get('context_window', 4096),
            num_output=synthesis_llm_params.get('max_tokens', 512),
            chunk_overlap_ratio=0.2
        )
    )

    # --- å‘é‡æŸ¥è¯¢ ---
    logger.info("æ„å»ºå‘é‡æŸ¥è¯¢å¼•æ“...")
    vector_index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store, embed_model=embed_model
    )
    vector_query_engine = vector_index.as_query_engine(
        filters=vector_filters,
        llm=reasoning_llm,
        response_synthesizer=response_synthesizer,
        similarity_top_k=vector_similarity_top_k,
        node_postprocessors=[
            LLMRerank(llm=reasoning_llm, top_n=vector_rerank_top_n)
        ]
    )
    logger.info(f"æ­£åœ¨æ‰§è¡Œå‘é‡æŸ¥è¯¢: '{retrieval_query_text}'")
    vector_response = vector_query_engine.query(retrieval_query_text)
    formatted_vector_str = _format_response_with_sorting(vector_response, vector_sort_by)
    logger.info(f"å‘é‡æŸ¥è¯¢å®Œæˆ, æ£€ç´¢åˆ° {len(vector_response.source_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")

    # --- çŸ¥è¯†å›¾è°±æŸ¥è¯¢ ---
    logger.info("æ„å»ºçŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“...")
    kg_storage_context = StorageContext.from_defaults(graph_store=graph_store)
    kg_index = KnowledgeGraphIndex.from_documents(
        [],
        storage_context=kg_storage_context,
        llm=reasoning_llm,
        include_embeddings=True,
        embed_model=embed_model
    )
    kg_retriever = kg_index.as_retriever(
        retriever_mode="hybrid",
        similarity_top_k=kg_similarity_top_k,
        with_nl2graphquery=True,
        graph_traversal_depth=2,
        nl2graphquery_prompt=kg_nl2graphquery_prompt,
    )
    kg_query_engine = RetrieverQueryEngine(
        retriever=kg_retriever,
        response_synthesizer=response_synthesizer,
        node_postprocessors=[
            LLMRerank(llm=reasoning_llm, top_n=kg_rerank_top_n)
        ]
    )
    logger.info(f"æ­£åœ¨æ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢: '{retrieval_query_text}'")
    kg_response = kg_query_engine.query(retrieval_query_text)
    formatted_kg_str = _format_response_with_sorting(kg_response, kg_sort_by)
    logger.info(f"çŸ¥è¯†å›¾è°±æŸ¥è¯¢å®Œæˆ, æ£€ç´¢åˆ° {len(kg_response.source_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")

    logger.info("æ­£åœ¨æ•´åˆå‘é‡å’ŒçŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢ç»“æœ...")
    context_dict_user = {
        "query_text": synthesis_query_text,
        "formatted_vector_str": formatted_vector_str,
        "formatted_kg_str": formatted_kg_str,
    }
    messages = get_llm_messages(synthesis_system_prompt, synthesis_user_prompt, None, context_dict_user)

    final_llm_params = get_llm_params(llm='reasoning', messages=messages, temperature=LLM_TEMPERATURES["synthesis"])

    final_message = llm_completion(final_llm_params)
    result = final_message.content
    logger.success("âœ… æ··åˆæŸ¥è¯¢åŠç»“æœæ•´åˆå®Œæˆã€‚")

    return result


if __name__ == "__main__":
    from datetime import datetime
    from utils.vector import get_chroma_vector_store

    init_logger(os.path.splitext(os.path.basename(__file__))[0])

    test_db_path = "./.test_chroma_db_graph"
    test_kuzu_path = "./.test_kuzu_db_graph"
    test_collection_name = "test_collection_graph"
    vector_store = get_chroma_vector_store(db_path=test_db_path, collection_name=test_collection_name)
    graph_store = get_kuzu_graph_store(db_path=test_kuzu_path)
    storage_context = StorageContext.from_defaults(
        vector_store=vector_store,
        graph_store=graph_store
    )

    doc_id = "test_story_001"
    metadata = {
        "author": "æµ‹è¯•å‘˜", 
        "task_id": "ç¬¬ä¸€ç« ", 
        "created_at": datetime.now().isoformat()
    }
    content = """
    åœ¨ä¸€ä¸ªé˜³å…‰æ˜åªšçš„ä¸‹åˆï¼Œå°æ˜åœ¨æ‘åº„åé¢çš„å°æºªè¾¹ç©è€ã€‚
    ä»–æ— æ„é—´è¸¢åˆ°äº†ä¸€å—é—ªé—ªå‘å…‰çš„çŸ³å¤´ã€‚è¿™å—çŸ³å¤´ä¸åŒå¯»å¸¸ï¼Œ
    å®ƒé€šä½“å‘ˆæ·±è“è‰²ï¼Œè¡¨é¢åˆ»æœ‰å¤è€çš„ç¬¦æ–‡ï¼Œå¹¶ä¸”æ•£å‘ç€å¾®å¼±çš„æš–æ„ã€‚
    å°æ˜å¥½å¥‡åœ°æ¡èµ·äº†å®ƒï¼Œæ„Ÿè§‰ä¸€è‚¡å¥‡å¦™çš„èƒ½é‡æ¶Œå…¥èº«ä½“ã€‚
    è¿™å—çŸ³å¤´ï¼Œå°±æ˜¯ä¼ è¯´ä¸­çš„â€œè‹ç©¹ä¹‹çŸ³â€ï¼Œæ®è¯´æ‹¥æœ‰è¿æ¥å¤©ç©ºä¸å¤§åœ°çš„åŠ›é‡ã€‚
    æ‘é‡Œçš„é•¿è€æ›¾è¯´è¿‡ï¼Œåªæœ‰å¿ƒçµçº¯æ´çš„äººæ‰èƒ½å”¤é†’å®ƒã€‚
    """
    # ç”¨äºçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æå–çš„æç¤º
    kg_extraction_prompt = """
    ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–çŸ¥è¯†ä¸‰å…ƒç»„ã€‚ä¸‰å…ƒç»„åº”ä¸º (ä¸»è¯­, è°“è¯­, å®¾è¯­) æ ¼å¼ã€‚
    è¯·ä¸“æ³¨äºå®ä½“åŠå…¶ä¹‹é—´çš„å…³ç³»ã€‚
    ä¾‹å¦‚:
    æ–‡æœ¬: "å°æ˜å‘ç°äº†ä¸€å—è“è‰²çš„çŸ³å¤´ã€‚"
    ä¸‰å…ƒç»„: (å°æ˜, å‘ç°, è“è‰²çŸ³å¤´)
    ---
    æ–‡æœ¬:
    {text}
    ---
    æå–çš„ä¸‰å…ƒç»„:
    """

    # 4. è°ƒç”¨ store å‡½æ•°å°†å†…å®¹å­˜å…¥çŸ¥è¯†å›¾è°±å’Œå‘é‡å­˜å‚¨
    logger.info("\n--- æ­¥éª¤1: å¼€å§‹å­˜å‚¨å†…å®¹ ---")
    store(
        storage_context=storage_context,
        content=content,
        metadata=metadata,
        doc_id=doc_id,
        kg_extraction_prompt=kg_extraction_prompt,
        content_format="text",
    )
    logger.success("--- å†…å®¹å­˜å‚¨å®Œæˆ ---")

    # 5. å‡†å¤‡æŸ¥è¯¢
    logger.info("\n--- æ­¥éª¤2: å¼€å§‹æ··åˆæŸ¥è¯¢ ---")
    retrieval_query_text = "å°æ˜å’Œè‹ç©¹ä¹‹çŸ³æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    synthesis_query_text = f"è¯·è¯¦ç»†æ€»ç»“ä¸€ä¸‹å…³äº'{retrieval_query_text}'çš„æ‰€æœ‰ä¿¡æ¯ã€‚"

    synthesis_system_prompt = "ä½ æ˜¯ä¸€ä¸ªå°è¯´åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„â€œå‘é‡æ£€ç´¢ä¿¡æ¯â€å’Œâ€œçŸ¥è¯†å›¾è°±ä¿¡æ¯â€ï¼Œæ•´åˆå¹¶è¯¦ç»†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä¼˜å…ˆä½¿ç”¨æä¾›çš„ä¿¡æ¯ï¼Œå¹¶ä»¥æµç•…ã€è¿è´¯çš„è¯­è¨€ç»„ç»‡ç­”æ¡ˆã€‚"
    synthesis_user_prompt = """
    [ç”¨æˆ·ä¿¡æ¯]
    é—®é¢˜: {query_text}

    [å‘é‡æ£€ç´¢ä¿¡æ¯]
    {formatted_vector_str}

    [çŸ¥è¯†å›¾è°±ä¿¡æ¯]
    {formatted_kg_str}

    [ä½ çš„ä»»åŠ¡]
    è¯·ç»¼åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç»™å‡ºæœ€ç»ˆçš„è¯¦ç»†å›ç­”ã€‚
    """

    final_answer = hybrid_query(
        retrieval_query_text=retrieval_query_text,
        synthesis_query_text=synthesis_query_text,
        synthesis_system_prompt=synthesis_system_prompt,
        synthesis_user_prompt=synthesis_user_prompt,
        vector_store=vector_store,
        graph_store=graph_store,
    )
    logger.success("\n--- æœ€ç»ˆç»¼åˆå›ç­” ---")
    logger.info(f"\n{final_answer}")

    logger.info("\n--- æµ‹è¯•å®Œæˆ ---")
    logger.info(f"ä½ å¯ä»¥æ£€æŸ¥ä»¥ä¸‹ç›®å½•æ¥éªŒè¯ç»“æœ: '{test_db_path}', '{test_kuzu_path}'")