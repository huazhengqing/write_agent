import os
import re
import asyncio
from datetime import datetime
from pathlib import Path
import chromadb
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional, get_args

from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import (
    Document,
    Settings,
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser import MarkdownElementNodeParser
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.schema import BaseNode, TextNode, NodeRelationship, RelatedNodeInfo
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank

from utils.config import llm_temperatures, get_llm_params, get_embedding_params
from utils.vector_prompts import (
    summary_query_str,
    text_qa_prompt,
    refine_prompt,
    mermaid_summary_prompt,
)


###############################################################################


synthesis_llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["synthesis"])

synthesizer = CompactAndRefine(
    llm=LiteLLM(**synthesis_llm_params),
    text_qa_template=PromptTemplate(text_qa_prompt),
    refine_template=PromptTemplate(refine_prompt),
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )
)


###############################################################################


def init_llama_settings():
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


###############################################################################


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    os.makedirs(db_path, exist_ok=True)
    db = chromadb.PersistentClient(path=db_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    return vector_store


###############################################################################


def file_metadata_default(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def _load_and_filter_documents(
    input_dir: str,
    metadata_func: Callable[[str], dict]
) -> List[Document]:
    """ä»ç›®å½•åŠ è½½æ–‡æ¡£å¹¶è¿‡æ»¤æ‰ç©ºæ–‡ä»¶ã€‚"""
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return []

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è¿‡æ»¤å’Œè§£æ...")
    
    valid_docs = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        valid_docs.append(doc)
    
    return valid_docs


class MermaidExtractor:
    """
    ä¸€ä¸ªè¾…åŠ©ç±»ï¼Œä¸“é—¨ç”¨äºè§£æMermaidå›¾è¡¨ã€‚
    å®ƒä¼šä¸ºå›¾è¡¨ç”Ÿæˆä¸€ä¸ªè‡ªç„¶è¯­è¨€æ‘˜è¦ï¼Œå¹¶å°†æ‘˜è¦å’ŒåŸå§‹ä»£ç ä½œä¸ºç‹¬ç«‹çš„èŠ‚ç‚¹è¿”å›ã€‚
    """

    def __init__(self, llm: LiteLLM, summary_prompt_str: str):
        self._llm = llm
        self._summary_prompt = PromptTemplate(summary_prompt_str)

    def get_nodes(self, mermaid_code: str, metadata: dict) -> List[BaseNode]:
        if not mermaid_code.strip():
            return []

        summary_response = self._llm.predict(self._summary_prompt, mermaid_code=mermaid_code)

        summary_node = TextNode(
            text=f"Mermaidå›¾è¡¨æ‘˜è¦:\n{summary_response}",
            metadata=metadata,
        )
        code_node = TextNode(
            text=f"```mermaid\n{mermaid_code}\n```", metadata=metadata
        )

        summary_node.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=code_node.id_)
        code_node.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=summary_node.id_)
        return [summary_node, code_node]


class CustomMarkdownNodeParser(MarkdownElementNodeParser):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„MarkdownèŠ‚ç‚¹è§£æå™¨ã€‚
    å®ƒé¦–å…ˆåˆ†ç¦»å‡ºMermaidå›¾è¡¨è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼ˆç”Ÿæˆæ‘˜è¦ï¼‰ï¼Œ
    ç„¶åå°†å…¶ä½™çš„Markdownå†…å®¹äº¤ç»™å†…ç½®çš„MarkdownElementNodeParserå¤„ç†ã€‚
    """
    def __init__(self, llm: LiteLLM, summary_query_str: str, mermaid_summary_prompt: str, **kwargs: Any):
        # å°†llmå’Œsummary_query_strä¼ é€’ç»™çˆ¶ç±»ï¼Œä»¥ç¡®ä¿è¡¨æ ¼æ‘˜è¦åŠŸèƒ½æ­£å¸¸å·¥ä½œ
        super().__init__(llm=llm, summary_query_str=summary_query_str, **kwargs)
        self.mermaid_extractor = MermaidExtractor(llm=llm, summary_prompt_str=mermaid_summary_prompt)

    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:
        """é‡å†™æ­¤æ–¹æ³•ä»¥å®ç°å¯¹Mermaidå›¾çš„ç‰¹æ®Šå¤„ç†ã€‚"""
        text = node.get_content()
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬ï¼Œä¿ç•™Mermaidä»£ç å—ä½œä¸ºç‹¬ç«‹éƒ¨åˆ†
        parts = re.split(r"(```mermaid\n.*?\n```)", text, flags=re.DOTALL)
        
        final_nodes: List[BaseNode] = []
        for part in parts:
            if not part.strip():
                continue
            
            if part.startswith("```mermaid"):
                # è¿™æ˜¯Mermaidå›¾è¡¨éƒ¨åˆ†ï¼Œä½¿ç”¨è‡ªå®šä¹‰æå–å™¨å¤„ç†
                mermaid_code = part.removeprefix("```mermaid\n").removesuffix("\n```")
                final_nodes.extend(self.mermaid_extractor.get_nodes(mermaid_code, node.metadata))
            else:
                # è¿™æ˜¯æ™®é€šMarkdownéƒ¨åˆ†ï¼Œåˆ›å»ºä¸´æ—¶æ–‡æ¡£å¹¶è°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•å¤„ç†
                # è¿™æ ·å¯ä»¥å®Œç¾å¤ç”¨çˆ¶ç±»å¯¹è¡¨æ ¼ã€æ ‡é¢˜ç­‰çš„åŸç”Ÿè§£æèƒ½åŠ›
                temp_node = Document(text=part, metadata=node.metadata)
                # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„ get_nodes_from_node æ–¹æ³•ï¼Œä»¥é¿å…æ— é™é€’å½’
                final_nodes.extend(super().get_nodes_from_node(temp_node))
                
        return final_nodes


def _get_node_parser(content_format: Literal["md", "txt", "json"], content_length: int = 0) -> NodeParser:
    if content_length > 20000:
        chunk_size = 1024
        chunk_overlap = 200
    elif content_length > 5000:
        chunk_size = 512
        chunk_overlap = 128
    else:
        chunk_size = 256
        chunk_overlap = 64

    if content_format == "json":
        return JSONNodeParser(
            include_metadata=True,
            max_depth=5, 
            levels_to_keep=2
        )
    elif content_format == "txt":
        return SentenceSplitter(
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap,
        )
    # ä½¿ç”¨è‡ªå®šä¹‰çš„Markdownè§£æå™¨ï¼Œä»¥æ”¯æŒMermaidå›¾è¡¨å’Œè‡ªå®šä¹‰ä¸­æ–‡è¡¨æ ¼æ‘˜è¦
    return CustomMarkdownNodeParser(
        llm=Settings.llm,
        summary_query_str=summary_query_str,
        mermaid_summary_prompt=mermaid_summary_prompt
    )


def filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    """è¿‡æ»¤æ‰æ— æ•ˆçš„èŠ‚ç‚¹ï¼ˆå†…å®¹ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºç™½/éè¯æ±‡å­—ç¬¦ï¼‰ã€‚"""
    valid_nodes = []
    for node in nodes:
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    return valid_nodes


def _parse_docs_to_nodes_by_format(documents: List[Document]) -> List[BaseNode]:
    """æ ¹æ®æ–‡ä»¶æ ¼å¼å°†æ–‡æ¡£è§£æä¸ºèŠ‚ç‚¹ã€‚"""
    docs_by_format: Dict[str, List[Document]] = {
        "md": [], 
        "txt": [], 
        "json": []
    }
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        file_extension = file_path.suffix.lstrip('.')
        if file_extension in docs_by_format:
            docs_by_format[file_extension].append(doc)
        else:
            logger.warning(f"æ£€æµ‹åˆ°æœªæ”¯æŒçš„æ–‡ä»¶æ‰©å±•å '{file_extension}'ï¼Œå°†å¿½ç•¥ã€‚")

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨ä¸º {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶åŠ¨æ€è§£æèŠ‚ç‚¹...")
        nodes_for_format = []
        for doc in format_docs:
            node_parser = _get_node_parser(content_format, content_length=len(doc.text))
            parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
            nodes_for_format.extend(filter_invalid_nodes(parsed_nodes))
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)
    
    return all_nodes


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    metadata_func: Callable[[str], dict] = file_metadata_default,
) -> bool:
    documents = _load_and_filter_documents(input_dir, metadata_func)
    if not documents:
        return False

    all_nodes = _parse_docs_to_nodes_by_format(documents)
    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def _is_content_too_similar(
    vector_store: VectorStore,
    content: str,
    threshold: float,
    doc_id: Optional[str] = None
) -> bool:
    """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å‘é‡åº“ä¸­ç°æœ‰æ–‡æ¡£è¿‡äºç›¸ä¼¼ã€‚"""
    query_embedding = Settings.embed_model.get_text_embedding(content)
    logger.trace(f"ä¸º doc_id '{doc_id}' ç”Ÿæˆçš„åµŒå…¥å‘é‡ (å‰10ç»´): {query_embedding[:10]}")
    vector_store_query = VectorStoreQuery(
        query_embedding=query_embedding, similarity_top_k=1, filters=None
    )
    query_result = vector_store.query(vector_store_query)
    if query_result.nodes and query_result.similarities:
        is_updating_itself = doc_id and query_result.nodes[0].ref_doc_id == doc_id
        if not is_updating_itself and query_result.similarities[0] > threshold:
            logger.warning(f"å‘ç°ä¸ doc_id '{doc_id}' å†…å®¹é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {query_result.similarities[0]:.4f}) çš„æ–‡æ¡£ (ID: '{query_result.nodes[0].ref_doc_id}'), è·³è¿‡æ·»åŠ ã€‚")
            return True
    return False


def _parse_content_to_nodes(
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"],
    doc_id: Optional[str] = None,
) -> List[BaseNode]:
    """å°†å•ä¸ªå†…å®¹å­—ç¬¦ä¸²è§£æä¸ºèŠ‚ç‚¹åˆ—è¡¨ã€‚"""
    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")
    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    node_parser = _get_node_parser(content_format, content_length=len(content))
    return filter_invalid_nodes(node_parser.get_nodes_from_documents([doc], show_progress=False))


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"] = "md",
    doc_id: Optional[str] = None,
    check_similarity: bool = False,
    similarity_threshold: float = 0.999,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    if check_similarity and _is_content_too_similar(vector_store, content, similarity_threshold, doc_id):
        return False

    if doc_id:
        logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
        vector_store.delete(ref_doc_id=doc_id)

    nodes_to_insert = _parse_content_to_nodes(content, metadata, content_format, doc_id)
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_vector_store_info_default() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def _create_reranker(rerank_top_n: int) -> Optional[SiliconFlowRerank]:
    if rerank_top_n and rerank_top_n > 0:
        return SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=rerank_top_n,
        )
    return None


def _create_auto_retriever_engine(
    index: VectorStoreIndex,
    vector_store_info: VectorStoreInfo,
    similarity_top_k: int,
    similarity_cutoff: float,
    postprocessors: List,
) -> BaseQueryEngine:
    logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    retriever = VectorIndexAutoRetriever(
        index,
        vector_store_info=vector_store_info,
        similarity_top_k=similarity_top_k,
        llm=reasoning_llm,
        verbose=True,
        similarity_cutoff=similarity_cutoff,
    )
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=synthesizer,
        node_postprocessors=postprocessors,
    )
    logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    return query_engine


def _create_standard_query_engine(
    index: VectorStoreIndex,
    filters: Optional[MetadataFilters],
    similarity_top_k: int,
    similarity_cutoff: float,
    postprocessors: List,
) -> BaseQueryEngine:
    logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
    return index.as_query_engine(
        response_synthesizer=synthesizer, filters=filters, similarity_top_k=similarity_top_k,
        node_postprocessors=postprocessors, similarity_cutoff=similarity_cutoff
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 25,
    rerank_top_n: int = 5,
    similarity_cutoff: float = 0,
    use_auto_retriever: bool = False,
    vector_store_info: VectorStoreInfo = get_vector_store_info_default(),
) -> BaseQueryEngine:
    
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
        f"similarity_cutoff={similarity_cutoff}"
    )

    index = VectorStoreIndex.from_vector_store(vector_store)

    reranker = _create_reranker(rerank_top_n)
    postprocessors = [reranker] if reranker else []

    if use_auto_retriever:
        return _create_auto_retriever_engine(
            index=index,
            vector_store_info=vector_store_info,
            similarity_top_k=similarity_top_k,
            similarity_cutoff=similarity_cutoff,
            postprocessors=postprocessors,
        )
    else:
        return _create_standard_query_engine(
            index=index,
            filters=filters,
            similarity_top_k=similarity_top_k,
            similarity_cutoff=similarity_cutoff,
            postprocessors=postprocessors,
        )


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""

    logger.info(f"å¼€å§‹æ‰§è¡Œç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    if not result or not getattr(result, "source_nodes", []) or not answer or answer == "Empty Response":
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºå›ç­”ã€‚")
        answer = ""

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”: {answer}")

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    # ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘é‡ä¸º3ï¼Œé˜²æ­¢å¯¹LLM APIé€ æˆè¿‡å¤§å‹åŠ›ã€‚
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            try:
                return await index_query(query_engine, question)
            except Exception as e:
                logger.error("æ‰¹é‡æŸ¥è¯¢ä¸­ï¼Œé—®é¢˜ '{}' å¤±è´¥: {}", question, e, exc_info=True)
                return ""

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆã€‚")
    return results
