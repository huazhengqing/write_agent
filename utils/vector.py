import os
import re
import asyncio
from datetime import datetime
from pathlib import Path
import chromadb
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional

from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import Document, Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser, JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.nodes.base import BaseNode
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank

from utils.config import llm_temperatures, get_llm_params, get_embedding_params


###############################################################################


qa_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¿¡æ¯æå–åŠ©æ‰‹ã€‚

# ä»»åŠ¡
ä»ä¸‹æ–¹çš„`ä¸Šä¸‹æ–‡ä¿¡æ¯`ä¸­ï¼Œæå–ä¸`é—®é¢˜`ç›¸å…³çš„æ‰€æœ‰äº‹å®å’Œæè¿°ï¼Œå¹¶ä»¥æ¸…æ™°çš„é™ˆè¿°å¥å½¢å¼å‘ˆç°ã€‚

# æ ¸å¿ƒåŸåˆ™
1.  **å¿ äºåŸæ–‡**: ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäº`ä¸Šä¸‹æ–‡ä¿¡æ¯`ï¼Œç¦æ­¢å¼•å…¥å¤–éƒ¨çŸ¥è¯†ã€‚
2.  **æå–è€Œéå›ç­”**: ä½ çš„ç›®æ ‡æ˜¯æå–ä¿¡æ¯ç‰‡æ®µï¼Œè€Œä¸æ˜¯ç›´æ¥å½¢æˆå¯¹`é—®é¢˜`çš„æœ€ç»ˆç­”æ¡ˆã€‚å¦‚æœ`ä¸Šä¸‹æ–‡ä¿¡æ¯`åªåŒ…å«éƒ¨åˆ†ç›¸å…³ä¿¡æ¯ï¼Œå°±åªè¾“å‡ºé‚£éƒ¨åˆ†ã€‚
3.  **æ— ç›¸å…³åˆ™ä¸ºç©º**: å¦‚æœ`ä¸Šä¸‹æ–‡ä¿¡æ¯`ä¸`é—®é¢˜`å®Œå…¨æ— å…³ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
4.  **ç›´æ¥é™ˆè¿°**: ç›´æ¥åˆ—å‡ºäº‹å®ï¼Œä¸è¦æ·»åŠ å¼•è¿°æ€§çŸ­è¯­ã€‚

# ä¸Šä¸‹æ–‡ä¿¡æ¯
---------------------
{context_str}
---------------------

# é—®é¢˜
{query_str}

# æå–çš„äº‹å®
"""


refine_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é«˜çº§ä¿¡æ¯æ•´åˆå¸ˆã€‚

# ä»»åŠ¡
æ ¹æ®`æ–°çš„ä¸Šä¸‹æ–‡`ï¼Œä¼˜åŒ–`å·²æœ‰çš„å›ç­”`ï¼Œä»¥æ›´å…¨é¢ã€æ›´ç²¾ç¡®åœ°å›ç­”`åŸå§‹é—®é¢˜`ã€‚

# å·¥ä½œæµç¨‹
1.  **åˆ†ææ–°ä¿¡æ¯**: ä»”ç»†é˜…è¯»`æ–°çš„ä¸Šä¸‹æ–‡`ï¼Œè¯†åˆ«å‡ºå…¶ä¸­åŒ…å«çš„ã€ä½†`å·²æœ‰çš„å›ç­”`ä¸­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„æ–°ä¿¡æ¯ç‚¹ã€‚
2.  **æ¯”è¾ƒä¸æ•´åˆ**: å°†æ–°ä¿¡æ¯ç‚¹ä¸`å·²æœ‰çš„å›ç­”`è¿›è¡Œèåˆï¼Œéµå¾ªä¸‹æ–¹çš„æ ¸å¿ƒåŸåˆ™ã€‚
3.  **ç”Ÿæˆæ–°ç­”æ¡ˆ**: æ„å»ºä¸€ä¸ªå•ä¸€ã€è¿è´¯ã€å…¨é¢çš„æ–°ç­”æ¡ˆã€‚

# æ ¸å¿ƒåŸåˆ™
1.  **ä¿¡æ¯å®Œæ•´æ€§**: æœ€ç»ˆç­”æ¡ˆå¿…é¡»æ— ç¼æ•´åˆ`å·²æœ‰çš„å›ç­”`å’Œ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸­çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç¦æ­¢ä¸¢å¤±ä»»ä½•ç»†èŠ‚ã€‚
2.  **å¢é‡ä¼˜åŒ–**: ä½ çš„ç›®æ ‡æ˜¯â€œä¼˜åŒ–â€è€Œéâ€œé‡å†™â€ã€‚åªæœ‰å½“`æ–°çš„ä¸Šä¸‹æ–‡`èƒ½æä¾›è¡¥å……ã€ä¿®æ­£æˆ–æ›´å…·ä½“çš„ç»†èŠ‚æ—¶ï¼Œæ‰è¿›è¡Œä¿®æ”¹ã€‚
3.  **å†²çªå¤„ç†**: å¦‚æœ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸`å·²æœ‰çš„å›ç­”`ä¸­çš„ä¿¡æ¯å‘ç”Ÿå†²çªï¼Œè¯·ç»¼åˆåˆ¤æ–­ï¼Œä¿ç•™æ›´å…·ä½“ã€æ›´å¯ä¿¡çš„ä¿¡æ¯ã€‚å¦‚æœæ— æ³•åˆ¤æ–­ä¼˜åŠ£ï¼Œåˆ™åº”åŒæ—¶æåŠä¸¤ç§è¯´æ³•å¹¶æ˜ç¡®æŒ‡å‡ºå…¶çŸ›ç›¾ä¹‹å¤„ã€‚
4.  **æ— æ•ˆåˆ™è¿”å›åŸæ–‡**: å¦‚æœ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸é—®é¢˜æ— å…³ï¼Œæˆ–æœªèƒ½æä¾›ä»»ä½•æœ‰ä»·å€¼çš„æ–°ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¿”å›`å·²æœ‰çš„å›ç­”`ï¼Œä¸è¦åšä»»ä½•æ”¹åŠ¨ã€‚
5.  **é£æ ¼ä¸€è‡´**: åœ¨ç”Ÿæˆæ–°ç­”æ¡ˆæ—¶ï¼Œå°½é‡ä¿æŒ`å·²æœ‰çš„å›ç­”`çš„è¯­è¨€é£æ ¼å’Œæ ¼å¼ï¼Œä½¿æœ€ç»ˆç­”æ¡ˆæµ‘ç„¶ä¸€ä½“ã€‚

# åŸå§‹é—®é¢˜
{query_str}

# å·²æœ‰çš„å›ç­”
{existing_answer}

# æ–°çš„ä¸Šä¸‹æ–‡
------------
{context_str}
------------

# ä¼˜åŒ–åçš„å›ç­”
"""


synthesis_llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["synthesis"])

synthesizer = CompactAndRefine(
    llm=LiteLLM(**synthesis_llm_params),
    text_qa_template=PromptTemplate(qa_prompt),
    refine_template=PromptTemplate(refine_prompt),
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )
)


###############################################################################


def init_llama_settings():
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


###############################################################################


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    os.makedirs(db_path, exist_ok=True)
    db = chromadb.PersistentClient(path=db_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    return vector_store


###############################################################################


def get_node_parser(content_format: Literal["markdown", "text", "json"]) -> NodeParser:
    if content_format == "json":
        return JSONNodeParser(
            include_metadata=True,
            max_depth=3, 
            levels_to_keep=0
        )
    elif content_format == "text":
        return SentenceSplitter(
            chunk_size=256, 
            chunk_overlap=50,
        )
    return MarkdownElementNodeParser(
        llm=Settings.llm,
        chunk_size = 256, 
        chunk_overlap = 50, 
        # num_workers=3,
        include_metadata=True,
        show_progress=False,
    )


###############################################################################


def _filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    """è¿‡æ»¤æ‰æ— æ•ˆçš„èŠ‚ç‚¹ï¼ˆå†…å®¹ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºç™½/éè¯æ±‡å­—ç¬¦ï¼‰ã€‚"""
    valid_nodes = []
    for node in nodes:
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    return valid_nodes


def default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    metadata_func = file_metadata_func or default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    # æŒ‰å†…å®¹æ ¼å¼å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç»„ï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†
    docs_by_format: Dict[str, List[Document]] = {"markdown": [], "text": [], "json": []}
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        file_extension = file_path.suffix.lstrip('.')
        content_format_map = {"md": "markdown", "txt": "text", "json": "json"}
        content_format = content_format_map.get(file_extension, "text")
        docs_by_format[content_format].append(doc)

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨ä¸º {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶æ‰¹é‡è§£æèŠ‚ç‚¹...")
        node_parser = get_node_parser(content_format)
        parsed_nodes = node_parser.get_nodes_from_documents(format_docs, show_progress=False)
        
        nodes_for_format = _filter_invalid_nodes(parsed_nodes)
        # è¿‡æ»¤æ‰ä»…åŒ…å«åˆ†éš”ç¬¦æˆ–ç©ºç™½ç­‰éæ–‡æœ¬å†…å®¹çš„æ— æ•ˆèŠ‚ç‚¹ (å·²ç§»è‡³ _filter_invalid_nodes)
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    # ç›¸ä¼¼åº¦æœç´¢å»é‡
    query_embedding = Settings.embed_model.get_text_embedding(content)
    logger.trace(f"ä¸º doc_id '{doc_id}' ç”Ÿæˆçš„åµŒå…¥å‘é‡ (å‰10ç»´): {query_embedding[:10]}")
    vector_store_query = VectorStoreQuery(
        query_embedding=query_embedding,
        similarity_top_k=1,
        filters=None,
    )
    query_result = vector_store.query(vector_store_query)
    if query_result.nodes:
        if query_result.similarities[0] > 0.999:
            logger.warning(f"å‘ç°ä¸ doc_id '{doc_id}' å†…å®¹é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {query_result.similarities[0]:.4f}) çš„æ–‡æ¡£ï¼Œè·³è¿‡æ·»åŠ ã€‚")
            return False

    if doc_id:
        logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
        vector_store.delete(ref_doc_id=doc_id)
        logger.info(f"å·²åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹ã€‚")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    node_parser = get_node_parser(content_format)
    parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
    nodes_to_insert = _filter_invalid_nodes(parsed_nodes)
    
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False
    logger.debug(f"ä¸º doc_id '{doc_id}' åˆ›å»ºçš„èŠ‚ç‚¹å†…å®¹: {[n.get_content(metadata_mode='all') for n in nodes_to_insert]}")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_default_vector_store_info() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
    similarity_cutoff: Optional[float] = None,
) -> BaseQueryEngine:
    
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
        f"similarity_cutoff={similarity_cutoff}"
    )

    index = VectorStoreIndex.from_vector_store(vector_store)

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        reranker = SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=rerank_top_n,
        )
        postprocessors.append(reranker)

    if use_auto_retriever:
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        
        reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
        reasoning_llm = LiteLLM(**reasoning_llm_params)
        final_vector_store_info = vector_store_info or get_default_vector_store_info()
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=final_vector_store_info,
            similarity_top_k=similarity_top_k,
            llm=reasoning_llm,
            verbose=True,
            similarity_cutoff=similarity_cutoff,
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=synthesizer,
            node_postprocessors=postprocessors,
        )
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine
    else:
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        retriever_kwargs = {}
        if similarity_cutoff is not None:
            retriever_kwargs["similarity_cutoff"] = similarity_cutoff

        query_engine = index.as_query_engine(
            response_synthesizer=synthesizer,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
            **retriever_kwargs,
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""

    logger.info(f"å¼€å§‹æ‰§è¡Œç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    if not result or not getattr(result, "source_nodes", []) or not answer or answer == "Empty Response":
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºå›ç­”ã€‚")
        answer = ""

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”: {answer}")

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    # ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘é‡ä¸º3ï¼Œé˜²æ­¢å¯¹LLM APIé€ æˆè¿‡å¤§å‹åŠ›ã€‚
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            try:
                return await index_query(query_engine, question)
            except Exception as e:
                logger.error(f"æ‰¹é‡æŸ¥è¯¢ä¸­ï¼Œé—®é¢˜ '{question}' å¤±è´¥: {e}", exc_info=True)
                return ""

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆã€‚")
    return results
