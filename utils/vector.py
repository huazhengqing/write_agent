import os
import re
import asyncio
import hashlib
from datetime import datetime
from pathlib import Path
import chromadb
from loguru import logger
from diskcache import Cache
from typing import Any, Callable, Dict, List, Literal, Optional, get_args

from llama_index.core.bridge.pydantic import PrivateAttr
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import (
    Document,
    Settings,
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser import MarkdownElementNodeParser
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever, VectorIndexRetriever
from llama_index.core.response_synthesizers import TreeSummarize
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.schema import BaseNode, TextNode, NodeRelationship, RelatedNodeInfo
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank

from utils.config import llm_temperatures, get_llm_params, get_embedding_params
from utils.file import cache_dir
from utils.vector_prompts import (
    summary_query_str,
    # text_qa_prompt,
    # refine_prompt,
    tree_summary_prompt,
    mermaid_summary_prompt,
    vector_store_query_prompt
)


ChromaVectorStore.model_config['extra'] = 'allow'
if hasattr(ChromaVectorStore, 'model_rebuild'):
    ChromaVectorStore.model_rebuild(force=True)


def init_llama_settings():
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    db_path_obj = Path(db_path)
    db_path_obj.mkdir(parents=True, exist_ok=True)

    db = chromadb.PersistentClient(path=str(db_path_obj))
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    store_cache_path = db_path_obj / f"{collection_name}.cache.db"
    vector_store.cache = Cache(str(store_cache_path), size_limit=int(32 * (1024**2)))
    
    return vector_store


synthesis_llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["synthesis"])

synthesizer = TreeSummarize(
    llm=LiteLLM(**synthesis_llm_params),
    summary_template=PromptTemplate(tree_summary_prompt),
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    ),
    use_async=True,
)


###############################################################################


def file_metadata_default(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def _load_and_filter_documents(
    input_dir: str,
    metadata_func: Callable[[str], dict]
) -> List[Document]:
    logger.info(f"å¼€å§‹ä»ç›®å½• '{input_dir}' åŠ è½½å’Œè¿‡æ»¤æ–‡æ¡£...")
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return []
    
    logger.debug(f"ä» '{input_dir}' åˆå§‹åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£ã€‚")
    valid_docs = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©º, å·²è·³è¿‡ã€‚")
            continue
        valid_docs.append(doc)
    
    logger.success(f"å®Œæˆæ–‡æ¡£åŠ è½½å’Œè¿‡æ»¤, å…±è·å¾— {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ã€‚")
    return valid_docs


class MermaidExtractor:
    def __init__(self, llm: LiteLLM, summary_prompt_str: str):
        self._llm = llm
        self._summary_prompt = PromptTemplate(summary_prompt_str)

    def get_nodes(self, mermaid_code: str, metadata: dict) -> List[BaseNode]:
        if not mermaid_code.strip():
            return []

        logger.debug("æ­£åœ¨ä¸º Mermaid å›¾è¡¨ç”Ÿæˆæ‘˜è¦...")
        summary_response = self._llm.predict(self._summary_prompt, mermaid_code=mermaid_code)
        logger.debug(f"Mermaid å›¾è¡¨æ‘˜è¦ç”Ÿæˆå®Œæ¯•, é•¿åº¦: {len(summary_response)}")

        summary_node = TextNode(
            text=f"Mermaidå›¾è¡¨æ‘˜è¦:\n{summary_response}",
            metadata=metadata,
        )
        code_node = TextNode(
            text=f"```mermaid\n{mermaid_code}\n```", metadata=metadata
        )
        logger.debug(f"åˆ›å»ºäº† Mermaid æ‘˜è¦èŠ‚ç‚¹ (ID: {summary_node.id_}) å’Œä»£ç èŠ‚ç‚¹ (ID: {code_node.id_})ã€‚")

        summary_node.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=code_node.id_)
        code_node.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=summary_node.id_)
        logger.debug("å·²åœ¨æ‘˜è¦èŠ‚ç‚¹å’Œä»£ç èŠ‚ç‚¹ä¹‹é—´å»ºç«‹åŒå‘å…³ç³»ã€‚")
        return [summary_node, code_node]


class CustomMarkdownNodeParser(MarkdownElementNodeParser):
    _mermaid_extractor: MermaidExtractor = PrivateAttr()

    def __init__(self, llm: LiteLLM, summary_query_str: str, mermaid_summary_prompt: str, **kwargs: Any):
        super().__init__(llm=llm, summary_query_str=summary_query_str, **kwargs)
        self._mermaid_extractor = MermaidExtractor(llm=llm, summary_prompt_str=mermaid_summary_prompt)

    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:
        logger.debug(f"CustomMarkdownNodeParser: å¼€å§‹ä»èŠ‚ç‚¹ (ID: {node.id_}) æå–å­èŠ‚ç‚¹...")
        text = node.get_content()
        parts = re.split(r"(```mermaid\n.*?\n```)", text, flags=re.DOTALL)

        final_nodes: List[BaseNode] = []
        for part in parts:
            if not part.strip():
                continue
            
            if part.startswith("```mermaid"):
                logger.debug("åœ¨ Markdown ä¸­æ£€æµ‹åˆ° Mermaid å›¾è¡¨, æ­£åœ¨æå–...")
                mermaid_code = part.removeprefix("```mermaid\n").removesuffix("\n```")
                mermaid_nodes = self._mermaid_extractor.get_nodes(mermaid_code, node.metadata)
                logger.debug(f"  - Mermaid å›¾è¡¨éƒ¨åˆ†æå–äº† {len(mermaid_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
                final_nodes.extend(mermaid_nodes)
            else:
                logger.debug("åœ¨ Markdown ä¸­æ£€æµ‹åˆ°å¸¸è§„æ–‡æœ¬éƒ¨åˆ†, æ­£åœ¨ä½¿ç”¨çˆ¶è§£æå™¨å¤„ç†...")
                temp_node = Document(text=part, metadata=node.metadata)
                regular_nodes = super().get_nodes_from_node(temp_node)
                logger.debug(f"  - å¸¸è§„æ–‡æœ¬éƒ¨åˆ†è§£æå‡º {len(regular_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
                final_nodes.extend(regular_nodes)

        logger.debug(f"CustomMarkdownNodeParser å®Œæˆå¤„ç†, å…±ç”Ÿæˆ {len(final_nodes)} ä¸ªå­èŠ‚ç‚¹ã€‚")
        return final_nodes


def _get_node_parser(content_format: Literal["md", "txt", "json"], content_length: int = 0) -> NodeParser:
    if content_length > 20000:
        chunk_size = 1024
        chunk_overlap = 200
    elif content_length > 5000:
        chunk_size = 512
        chunk_overlap = 128
    else:
        chunk_size = 256
        chunk_overlap = 64

    logger.debug(f"ä¸º '{content_format}' (é•¿åº¦: {content_length}) é€‰æ‹©èŠ‚ç‚¹è§£æå™¨ã€‚")

    if content_format == "json":
        logger.debug("ä½¿ç”¨ JSONNodeParserã€‚")
        return JSONNodeParser(
            include_metadata=True,
            max_depth=5, 
            levels_to_keep=2
        )
    elif content_format == "txt":
        logger.debug(f"ä½¿ç”¨ SentenceSplitter, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}ã€‚")
        return SentenceSplitter(
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap,
        )
    logger.debug("ä½¿ç”¨ CustomMarkdownNodeParserã€‚")
    return CustomMarkdownNodeParser(
        llm=Settings.llm,
        summary_query_str=summary_query_str,
        mermaid_summary_prompt=mermaid_summary_prompt
    )


def filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    valid_nodes = []
    initial_count = len(nodes)
    for node in nodes:
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    
    removed_count = initial_count - len(valid_nodes)
    if removed_count > 0:
        logger.debug(f"è¿‡æ»¤æ‰ {removed_count} ä¸ªæ— æ•ˆæˆ–ç©ºèŠ‚ç‚¹ã€‚")
    return valid_nodes


def _parse_docs_to_nodes_by_format(documents: List[Document]) -> List[BaseNode]:
    logger.info("å¼€å§‹æŒ‰æ–‡ä»¶æ ¼å¼è§£ææ–‡æ¡£ä¸ºèŠ‚ç‚¹...")
    docs_by_format: Dict[str, List[Document]] = {
        "md": [], 
        "txt": [], 
        "json": []
    }
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        file_extension = file_path.suffix.lstrip('.')
        if file_extension in docs_by_format:
            docs_by_format[file_extension].append(doc)
        else:
            logger.warning(f"æ£€æµ‹åˆ°æœªæ”¯æŒçš„æ–‡ä»¶æ‰©å±•å '{file_extension}', å°†å¿½ç•¥ã€‚")

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨å¤„ç† {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶...")
        nodes_for_format = []
        for doc in format_docs:
            node_parser = _get_node_parser(content_format, content_length=len(doc.text))
            parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
            nodes_for_format.extend(filter_invalid_nodes(parsed_nodes))
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)
    
    logger.success(f"æ–‡æ¡£è§£æå®Œæˆ, æ€»å…±ç”Ÿæˆ {len(all_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    return all_nodes


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    metadata_func: Callable[[str], dict] = file_metadata_default,
) -> bool:
    logger.info(f"å¼€å§‹ä»ç›®å½• '{input_dir}' æ·»åŠ å†…å®¹åˆ°å‘é‡åº“...")
    documents = _load_and_filter_documents(input_dir, metadata_func)
    if not documents:
        return False

    all_nodes = _parse_docs_to_nodes_by_format(documents)
    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    logger.info(f"å‡†å¤‡å°† {len(unique_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹æ³¨å…¥ IngestionPipeline...")
    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def _parse_content_to_nodes(
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"],
    doc_id: Optional[str] = None,
) -> List[BaseNode]:
    logger.info(f"å¼€å§‹ä¸º doc_id '{doc_id}' è§£æå†…å®¹ä¸ºèŠ‚ç‚¹ (æ ¼å¼: {content_format})...")
    doc = Document(text=content, metadata=metadata, id_=doc_id)
    node_parser = _get_node_parser(content_format, content_length=len(content))
    nodes = filter_invalid_nodes(node_parser.get_nodes_from_documents([doc], show_progress=False))
    logger.info(f"ä¸º doc_id '{doc_id}' è§£æå‡º {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    return nodes


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"] = "md",
    doc_id: Optional[str] = None,
) -> bool:
    logger.info(f"å¼€å§‹å‘å‘é‡åº“æ·»åŠ å†…å®¹, doc_id='{doc_id}', format='{content_format}'...")
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯, è·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False

    new_content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
    doc_cache = getattr(vector_store, "cache", None)
    if doc_cache and doc_cache.get(new_content_hash):
        logger.info(f"å†…å®¹ (hash: {new_content_hash[:8]}...) å·²å­˜åœ¨, è·³è¿‡é‡å¤æ·»åŠ ã€‚")
        return True

    effective_doc_id = doc_id or new_content_hash

    nodes_to_insert = _parse_content_to_nodes(content, metadata, content_format, effective_doc_id)
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (id: {effective_doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹, è·³è¿‡æ·»åŠ ã€‚")
        return False

    logger.info(f"å‡†å¤‡å°† {len(nodes_to_insert)} ä¸ªèŠ‚ç‚¹ (id: {effective_doc_id}) æ³¨å…¥ IngestionPipeline...")
    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    if doc_cache:
        doc_cache.set(new_content_hash, True)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (id: {effective_doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_vector_store_info_default() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸ, æ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def _create_auto_retriever_engine(
    index: VectorStoreIndex,
    vector_store_info: VectorStoreInfo,
    similarity_top_k: int,
    node_postprocessors: List,
) -> BaseQueryEngine:
    logger.info("æ­£åœ¨åˆ›å»º Auto-Retriever æŸ¥è¯¢å¼•æ“...")
    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    retriever = VectorIndexAutoRetriever(
        index=index,
        vector_store_info=vector_store_info,
        llm=reasoning_llm,
        prompt_template_str=vector_store_query_prompt, 
        similarity_top_k=similarity_top_k,
    )
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=synthesizer,
        node_postprocessors=node_postprocessors,
    )
    logger.success("Auto-Retriever æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    return query_engine


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 25,
    top_n: int = 5,
    use_auto_retriever: bool = False,
    vector_store_info: VectorStoreInfo = get_vector_store_info_default(),
) -> BaseQueryEngine:
    logger.info("å¼€å§‹æ„å»ºå‘é‡æŸ¥è¯¢å¼•æ“...")
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, top_n={top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
    )

    index = VectorStoreIndex.from_vector_store(vector_store)
    logger.debug("ä» VectorStore åˆ›å»º VectorStoreIndex å®Œæˆã€‚")

    reranker = None
    if top_n and top_n > 0:
        logger.debug(f"æ­£åœ¨åˆ›å»º Reranker, top_n={top_n}")
        reranker = SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=top_n,
        )
    node_postprocessors = [reranker] if reranker else []

    if use_auto_retriever:
        query_engine = _create_auto_retriever_engine(
            index=index,
            vector_store_info=vector_store_info,
            similarity_top_k=similarity_top_k,
            node_postprocessors=node_postprocessors,
        )
    else:
        logger.info("æ­£åœ¨åˆ›å»ºæ ‡å‡†æŸ¥è¯¢å¼•æ“...")
        query_engine = index.as_query_engine(
            response_synthesizer=synthesizer, 
            filters=filters, 
            similarity_top_k=similarity_top_k,
            node_postprocessors=node_postprocessors, 
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    
    logger.success("å‘é‡æŸ¥è¯¢å¼•æ“æ„å»ºæˆåŠŸã€‚")
    return query_engine


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""
    
    logger.info(f"å¼€å§‹æ‰§è¡Œå‘é‡ç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    source_nodes = getattr(result, "source_nodes", [])

    if not source_nodes or not answer or answer == "Empty Response":
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”, è¿”å›ç©ºå›ç­”ã€‚")
        answer = ""
    else:
        logger.info(f"æŸ¥è¯¢ '{question}' æ£€ç´¢åˆ° {len(source_nodes)} ä¸ªæºèŠ‚ç‚¹ã€‚")
        for i, node in enumerate(source_nodes):
            logger.debug(
                f"  - æºèŠ‚ç‚¹ {i+1} (ID: {node.node_id}, åˆ†æ•°: {node.score:.4f}):\n"
                f"{node.get_content()[:200]}..."
            )
        logger.success(f"æˆåŠŸå®Œæˆå¯¹ '{question}' çš„æŸ¥è¯¢, ç”Ÿæˆå›ç­”é•¿åº¦: {len(answer)}")

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”:\n{answer}")

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"å¼€å§‹æ‰§è¡Œ {len(questions)} ä¸ªé—®é¢˜çš„æ‰¹é‡å‘é‡æŸ¥è¯¢...")
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            try:
                return await index_query(query_engine, question)
            except Exception as e:
                logger.error("æ‰¹é‡æŸ¥è¯¢ä¸­, é—®é¢˜ '{}' å¤±è´¥: {}", question, e, exc_info=True)
                return ""

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.success(f"æ‰¹é‡å‘é‡æŸ¥è¯¢å®Œæˆ, æˆåŠŸå¤„ç† {len(results)} ä¸ªé—®é¢˜ã€‚")
    return results
