import os
import sys
import re
import threading
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
from pydantic import Field
import chromadb
from loguru import logger
from llama_index.core import Settings
from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.tools import QueryEngineTool
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from litellm import arerank, rerank
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.llm import llm_temperatures, get_embedding_params, get_llm_params, get_rerank_params
from utils.agent import call_react_agent


def setup_global_settings():
    if getattr(Settings, '_llm', None) is None:
        default_llm_params = get_llm_params(llm_group="fast", temperature=llm_temperatures["summarization"])
        Settings.llm = LiteLLM(**default_llm_params)

setup_global_settings()


_embed_model: Optional[LiteLLMEmbedding] = None
_embed_model_lock = threading.Lock()
def get_embed_model() -> LiteLLMEmbedding:
    global _embed_model
    if _embed_model is None:
        with _embed_model_lock:
            if _embed_model is None:
                embedding_params = get_embedding_params()
                embed_model_name = embedding_params.pop('model')
                _embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)
                if getattr(Settings, '_embed_model', None) is None:
                    Settings.embed_model = _embed_model
    return _embed_model


_vector_stores: Dict[Tuple[str, str], ChromaVectorStore] = {}
_vector_store_lock = threading.Lock()
def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    with _vector_store_lock:
        if (db_path, collection_name) in _vector_stores:
            return _vector_stores[(db_path, collection_name)]
        os.makedirs(db_path, exist_ok=True)
        db = chromadb.PersistentClient(path=db_path)
        chroma_collection = db.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        _vector_stores[(db_path, collection_name)] = vector_store
        return vector_store


_vector_indices: Dict[int, VectorStoreIndex] = {}
_vector_index_lock = threading.Lock()


def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def get_nodes_from_document(doc: Document) -> List[Document]:
    summary_llm_params = get_llm_params(llm_group="fast", temperature=llm_temperatures["summarization"])
    summary_llm = LiteLLM(**summary_llm_params)
    parser = MarkdownElementNodeParser(
        llm=summary_llm,
        chunk_size=256,
        chunk_overlap=50
    )
    nodes = parser.get_nodes_from_documents([doc])
    return nodes


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"å‘é‡åº“å†…å®¹å˜æ›´, ä½¿ç¼“å­˜çš„ VectorStoreIndex å¤±æ•ˆ (key: {cache_key})ã€‚")
            del _vector_indices[cache_key]

    metadata_func = file_metadata_func or _default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    all_nodes = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        nodes = []
        if file_path.suffix == ".json":
            nodes = [doc]
        else:
            # å¯¹äº .md å’Œ .txt, ä½¿ç”¨è¾…åŠ©å‡½æ•°è¿›è¡Œåˆ†å—
            nodes = get_nodes_from_document(doc)

        logger.info(f"  - æ–‡ä»¶ '{file_path.name}' è¢«è§£ææˆ {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())
    index.insert_nodes(all_nodes, show_progress=True)
    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(all_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"å‘é‡åº“å†…å®¹å˜æ›´, ä½¿ç¼“å­˜çš„ VectorStoreIndex å¤±æ•ˆ (key: {cache_key})ã€‚")
            del _vector_indices[cache_key]
    
    index = VectorStoreIndex.from_vector_store(
        vector_store, 
        embed_model=get_embed_model()
    )

    if doc_id:
        try:
            logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
            index.delete_ref_doc(doc_id, delete_from_docstore=True)
            logger.info(f"å·²åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹ã€‚")
        except Exception as e:
            logger.warning(f"åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹æ—¶å‡ºé”™ (å¯èƒ½æ˜¯é¦–æ¬¡æ·»åŠ ): {e}")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    nodes = []
    if content_format == "json":
        nodes = [doc]
    else:
        nodes = get_nodes_from_document(doc)

    if not nodes:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•èŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False

    index.insert_nodes(nodes)
    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


class LiteLLMReranker(BaseNodePostprocessor):
    top_n: int = 3
    rerank_params: Dict[str, Any] = Field(default_factory=dict)
    
    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError("å¿…é¡»æä¾›æŸ¥è¯¢ä¿¡æ¯ (QueryBundle) æ‰èƒ½è¿›è¡Œé‡æ’ã€‚")
        if not nodes:
            return []

        query_str = query_bundle.query_str
        documents = [node.get_content() for node in nodes]

        rerank_request_params = self.rerank_params.copy()
        rerank_request_params.update({
            "query": query_str,
            "documents": documents,
            "top_n": self.top_n,
        })
        
        logger.debug(f"å‘ LiteLLM Reranker å‘é€åŒæ­¥è¯·æ±‚: model={rerank_request_params.get('model')}, top_n={self.top_n}, num_docs={len(documents)}")
        
        response = rerank(**rerank_request_params)

        new_nodes_with_scores = []
        for result in response.results:
            original_node = nodes[result.index]
            original_node.score = result.relevance_score
            new_nodes_with_scores.append(original_node)
        
        logger.debug(f"é‡æ’åè¿”å› {len(new_nodes_with_scores)} ä¸ªèŠ‚ç‚¹ã€‚")
        return new_nodes_with_scores


    async def _aprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError("å¿…é¡»æä¾›æŸ¥è¯¢ä¿¡æ¯ (QueryBundle) æ‰èƒ½è¿›è¡Œé‡æ’ã€‚")
        if not nodes:
            return []

        query_str = query_bundle.query_str
        documents = [node.get_content() for node in nodes]

        rerank_request_params = self.rerank_params.copy()
        rerank_request_params.update({
            "query": query_str,
            "documents": documents,
            "top_n": self.top_n,
        })
        
        logger.debug(f"å‘ LiteLLM Reranker å‘é€å¼‚æ­¥è¯·æ±‚: model={rerank_request_params.get('model')}, top_n={self.top_n}, num_docs={len(documents)}")
        
        response = await arerank(**rerank_request_params)

        new_nodes_with_scores = []
        for result in response.results:
            original_node = nodes[result.index]
            original_node.score = result.relevance_score
            new_nodes_with_scores.append(original_node)
        
        logger.debug(f"é‡æ’åè¿”å› {len(new_nodes_with_scores)} ä¸ªèŠ‚ç‚¹ã€‚")
        return new_nodes_with_scores


def get_default_vector_store_info() -> VectorStoreInfo:
    """
    ä¸ºé¡¹ç›®åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ VectorStoreInfo, å®šä¹‰äº†å¸¸è§çš„å…ƒæ•°æ®å­—æ®µã€‚
    è¿™ä½¿å¾—è‡ªåŠ¨æ£€ç´¢å™¨ (AutoRetriever) èƒ½å¤Ÿç†è§£å…ƒæ•°æ®ç»“æ„å¹¶ç”Ÿæˆè¿‡æ»¤æŸ¥è¯¢ã€‚
    """
    metadata_field_info = [

        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
) -> BaseQueryEngine:
    logger.info("æ­£åœ¨åˆ›å»ºå‘é‡æŸ¥è¯¢å¼•æ“...")
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}"
    )
    
    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)

    synthesis_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["synthesis"])
    synthesis_llm = LiteLLM(**synthesis_llm_params)

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        logger.info(f"é…ç½® LiteLLM Reranker åå¤„ç†å™¨, top_n={rerank_top_n}")
        rerank_params = get_rerank_params()
        reranker = LiteLLMReranker(top_n=rerank_top_n, rerank_params=rerank_params)
        postprocessors.append(reranker)

    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params.get('context_window', 4096),
            num_output=synthesis_llm_params.get('max_tokens', 512),
            chunk_overlap_ratio=0.2
        )
    )

    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"ä»ç¼“å­˜ä¸­è·å– VectorStoreIndex (key: {cache_key})ã€‚")
            index = _vector_indices[cache_key]
        else:
            logger.info(f"ç¼“å­˜ä¸­æœªæ‰¾åˆ° VectorStoreIndex, æ­£åœ¨åˆ›å»ºå¹¶ç¼“å­˜ (key: {cache_key})ã€‚")
            index = VectorStoreIndex.from_vector_store(
                vector_store, 
                embed_model=get_embed_model()
            )
            _vector_indices[cache_key] = index

    if use_auto_retriever:
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼ã€‚")
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾› vector_store_info, åˆ™ä½¿ç”¨é»˜è®¤çš„ã€‚
        # è¿™ä½¿å¾—è‡ªåŠ¨å…ƒæ•°æ®è¿‡æ»¤åŠŸèƒ½å¼€ç®±å³ç”¨ã€‚
        final_vector_store_info = vector_store_info or get_default_vector_store_info()
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=final_vector_store_info,
            similarity_top_k=similarity_top_k,
            llm=reasoning_llm,
            verbose=True
        )
        
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
            node_postprocessors=postprocessors,
        )
    else:
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼ã€‚")
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return index.as_query_engine(
            llm=reasoning_llm,
            response_synthesizer=response_synthesizer,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
        )


async def index_query(
    query_engine: BaseQueryEngine,
    questions: List[str],
) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    all_nodes: Dict[str, NodeWithScore] = {}

    tasks = []
    for q in questions:
        query_text = f"{q}\n# è¯·ä½¿ç”¨ä¸­æ–‡å›å¤"
        tasks.append(query_engine.aquery(query_text))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    for question, result in zip(questions, results):
        if isinstance(result, Exception):
            logger.warning(f"æŸ¥è¯¢ '{question}' æ—¶å‡ºé”™: {result}")
            continue
        if result and result.source_nodes:
            for node in result.source_nodes:
                all_nodes[node.node.id_] = node

    if not all_nodes:
        logger.info("æ‰€æœ‰æŸ¥è¯¢å‡æœªæ‰¾åˆ°ç›¸å…³çš„æºèŠ‚ç‚¹ã€‚")
        return []

    nodes_in_order = list(all_nodes.values())
    final_content = [re.sub(r"\s+", " ", node.get_content()).strip() for node in nodes_in_order]
    logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œå…±èšåˆäº† {len(final_content)} ä¸ªç‹¬ç‰¹çš„çŸ¥è¯†ç‰‡æ®µã€‚")
    logger.debug(f"è¿”å›çš„çŸ¥è¯†ç‰‡æ®µå†…å®¹: \n{final_content}")
    return final_content


async def index_query_react(
    query_engine: BaseQueryEngine,
    query_str: str,
    agent_system_prompt: Optional[str] = None,
) -> str:
    vector_tool = QueryEngineTool.from_defaults(
        query_engine=query_engine,
        name="vector_search",
        description="ç”¨äºæŸ¥æ‰¾è®¾å®šã€æ‘˜è¦ç­‰è¯­ä¹‰ç›¸ä¼¼çš„å†…å®¹ (ä¾‹å¦‚: è§’è‰²èƒŒæ™¯, ä¸–ç•Œè§‚è®¾å®š, ç‰©å“æè¿°)ã€‚å½“é—®é¢˜æ¯”è¾ƒå¤æ‚æ—¶, ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨æ­¤å·¥å…·æ¥å›ç­”é—®é¢˜çš„ä¸åŒéƒ¨åˆ†, ç„¶åç»¼åˆç­”æ¡ˆã€‚"
    )
    result = await call_react_agent(
        system_prompt=agent_system_prompt,
        user_prompt=query_str,
        tools=[vector_tool],
        llm_group="reasoning",
        temperature=llm_temperatures["reasoning"]
    )
    if not isinstance(result, str):
        logger.warning(f"Agent è¿”å›äº†éå­—ç¬¦ä¸²ç±»å‹, å°†å…¶å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²: {type(result)}")
        result = str(result)
    return result


###############################################################################


if __name__ == '__main__':
    import asyncio
    import tempfile
    import shutil
    from pathlib import Path
    from utils.log import init_logger

    # 1. åˆå§‹åŒ–æ—¥å¿—å’Œä¸´æ—¶ç›®å½•
    init_logger("vector_test")
    test_dir = tempfile.mkdtemp()
    db_path = os.path.join(test_dir, "chroma_db")
    input_dir = os.path.join(test_dir, "input_data")
    os.makedirs(input_dir, exist_ok=True)
    logger.info(f"æµ‹è¯•ç›®å½•å·²åˆ›å»º: {test_dir}")

    # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
    (Path(input_dir) / "doc1.md").write_text("# è§’è‰²ï¼šé¾™å‚²å¤©\né¾™å‚²å¤©æ˜¯ä¸€åæ¥è‡ªå¼‚ä¸–ç•Œçš„ç©¿è¶Šè€…ã€‚", encoding='utf-8')
    (Path(input_dir) / "doc2.txt").write_text("ä¸–ç•Œæ ‘æ˜¯å®‡å®™çš„ä¸­å¿ƒï¼Œè¿æ¥ç€ä¹å¤§ç‹å›½ã€‚", encoding='utf-8')
    logger.info(f"æµ‹è¯•æ–‡ä»¶å·²å†™å…¥: {input_dir}")

    # 3. æµ‹è¯• get_vector_store
    logger.info("--- æµ‹è¯• get_vector_store ---")
    vector_store = get_vector_store(db_path=db_path, collection_name="test_collection")
    logger.info(f"æˆåŠŸè·å– VectorStore: {vector_store}")

    # 4. æµ‹è¯• vector_add_from_dir
    logger.info("--- æµ‹è¯• vector_add_from_dir ---")
    vector_add_from_dir(vector_store, input_dir)

    # 5. æµ‹è¯• vector_add
    logger.info("--- æµ‹è¯• vector_add ---")
    vector_add(vector_store, "è™šç©ºä¹‹çŸ³æ˜¯ä¸€ä¸ªç¥ç§˜ç‰©å“ã€‚", {"category": "item"}, doc_id="item_void_stone")

    # 6. æµ‹è¯• get_vector_query_engine
    logger.info("--- æµ‹è¯• get_vector_query_engine ---")
    query_engine = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=2)
    logger.info(f"æˆåŠŸåˆ›å»ºæŸ¥è¯¢å¼•æ“: {type(query_engine)}")

    async def run_queries():
        # 7. æµ‹è¯• index_query
        logger.info("--- æµ‹è¯• index_query ---")
        questions = ["é¾™å‚²å¤©æ˜¯è°ï¼Ÿ", "ä¸–ç•Œæ ‘æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"]
        results = await index_query(query_engine, questions)
        logger.info(f"index_query æŸ¥è¯¢ç»“æœ:\n{results}")

        # 8. æµ‹è¯• index_query_react
        logger.info("--- æµ‹è¯• index_query_react ---")
        react_question = "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹é¾™å‚²å¤©ã€‚"
        react_result = await index_query_react(query_engine, react_question, "ä½ æ˜¯ä¸€ä¸ªå°è¯´è®¾å®šåŠ©æ‰‹ã€‚")
        logger.info(f"index_query_react æŸ¥è¯¢ç»“æœ:\n{react_result}")

    try:
        asyncio.run(run_queries())
    finally:
        # 9. æ¸…ç†
        shutil.rmtree(test_dir)
        logger.info(f"æµ‹è¯•ç›®å½•å·²åˆ é™¤: {test_dir}")
