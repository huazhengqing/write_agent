import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import chromadb
from litellm.exceptions import RateLimitError
from loguru import logger
from llama_index.core import (
    Document,
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core import PromptTemplate
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore
from llama_index.core.vector_stores import MetadataFilters
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.llm import get_embedding_params, get_llm_params


_embed_model: Optional[LiteLLMEmbedding] = None
def get_embed_model() -> LiteLLMEmbedding:
    global _embed_model
    if _embed_model is None:
        logger.info("é¦–æ¬¡è®¿é—®ï¼Œæ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embedding_params = get_embedding_params()
        embed_model_name = embedding_params.pop('model')
        _embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)
        logger.success("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚")
    return _embed_model

def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    logger.info(f"æ­£åœ¨è®¿é—®ChromaDB: path='{db_path}', collection='{collection_name}'")
    os.makedirs(db_path, exist_ok=True)
    db = chromadb.PersistentClient(path=db_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    logger.success(f"ChromaDBå‘é‡å­˜å‚¨ '{collection_name}' å·²å‡†å¤‡å°±ç»ªã€‚")
    return vector_store

def vector_query(
    vector_store: VectorStore,
    query_text: str,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
) -> Tuple[Optional[str], Optional[List[NodeWithScore]]]:
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå‘é‡æŸ¥è¯¢ä¸åˆæˆ: '{query_text}'")

    logger.info("ä»å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å‘é‡ç´¢å¼•...")
    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        logger.info(f"æ­£åœ¨é…ç½®LLMé‡æ’åºå™¨ (top_n={rerank_top_n})...")
        rerank_llm_params = get_llm_params(llm="fast")
        reranker = LLMRerank(choice_batch_size=5, top_n=rerank_top_n, llm=LiteLLM(**rerank_llm_params))
        postprocessors.append(reranker)
        log_message = f"æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢ï¼šåˆæ­¥æ£€ç´¢ {similarity_top_k} ä¸ªæ–‡æ¡£ï¼Œé‡æ’å¹¶é€‰å‡ºå‰ {rerank_top_n} ä¸ªç”¨äºåˆæˆç­”æ¡ˆ..."
    else:
        log_message = f"æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢ï¼šæ£€ç´¢ {similarity_top_k} ä¸ªæ–‡æ¡£ç”¨äºåˆæˆç­”æ¡ˆ (æ— é‡æ’)..."

    synthesis_llm_params = get_llm_params(llm="reasoning")
    synthesis_llm = LiteLLM(**synthesis_llm_params)

    # å®šä¹‰ä¸­æ–‡æç¤ºè¯
    TEXT_QA_TEMPLATE_CN = PromptTemplate(
        """ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
        ä½ å°†æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
        ---------------------
        {context_str}
        ---------------------
        åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{query_str}
        """
    )

    REFINE_TEMPLATE_CN = PromptTemplate(
        """ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººï¼Œä½ æ­£åœ¨æ”¹è¿›ä¸€ä¸ªå·²æœ‰çš„ç­”æ¡ˆã€‚
        ä½ å·²ç»æä¾›äº†ä¸€ä¸ªç­”æ¡ˆï¼š{existing_answer}
        ä½ ç°åœ¨æœ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        ---------------------
        {context_msg}
        ---------------------
        è¯·æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ”¹è¿›ä½ çš„ç­”æ¡ˆã€‚
        å¦‚æœä½ ä¸èƒ½æ”¹è¿›ä½ çš„ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¿”å›å·²æœ‰çš„ç­”æ¡ˆã€‚
        """
    )

    # response_synthesizer = get_response_synthesizer(
    #     llm=synthesis_llm,
    #     text_qa_template=TEXT_QA_TEMPLATE_CN,
    #     refine_template=REFINE_TEMPLATE_CN
    # )
    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params['context_window'],
            num_output=synthesis_llm_params['max_tokens'],
            chunk_overlap_ratio=0.2
        )
        ,text_qa_template=TEXT_QA_TEMPLATE_CN,
        refine_template=REFINE_TEMPLATE_CN
    )

    query_engine = index.as_query_engine(
        llm=synthesis_llm,
        response_synthesizer=response_synthesizer,
        filters=filters,
        similarity_top_k=similarity_top_k,
        node_postprocessors=postprocessors
    )

    logger.info(log_message)
    response = query_engine.query(query_text)
    if not response.response or not response.source_nodes:
        logger.warning("ğŸ¤· æœªèƒ½ç”Ÿæˆç­”æ¡ˆæˆ–æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
        return None, None
    else:
        logger.success("âœ… æŸ¥è¯¢æˆåŠŸå®Œæˆã€‚")
        return response.response, response.source_nodes

def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }

def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    """
    ä»æŒ‡å®šç›®å½•åŠ è½½ã€è§£ææ–‡ä»¶ï¼Œå¹¶å°†å†…å®¹å­˜å…¥å‘é‡æ•°æ®åº“ã€‚
    Args:
        vector_store (VectorStore): ç›®æ ‡å‘é‡å­˜å‚¨ã€‚
        input_dir (str): è¾“å…¥ç›®å½•çš„è·¯å¾„ã€‚
        file_metadata_func (Optional[Callable[[str], dict]]): ç”¨äºä»æ–‡ä»¶åç”Ÿæˆå…ƒæ•°æ®çš„å‡½æ•°ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‡½æ•°æå–æ–‡ä»¶åå’Œæ—¶é—´æˆ³ã€‚
    """
    logger.info(f"ğŸ“‚ å¼€å§‹ä»ç›®å½• '{input_dir}' æ‘„å–æ–‡ä»¶...")
    metadata_func = file_metadata_func or _default_file_metadata
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")
    md_parser = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True)
    txt_parser = SentenceSplitter(chunk_size=600, chunk_overlap=120, include_metadata=True, include_prev_next_rel=True)
    all_nodes = []

    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        if file_path.suffix == ".md":
            nodes = md_parser.get_nodes_from_documents([doc])
        elif file_path.suffix == ".json":
            # JSONæ–‡ä»¶ä½œä¸ºä¸€ä¸ªæ•´ä½“èŠ‚ç‚¹ï¼Œä¸è¿›è¡Œåˆ†å‰²
            nodes = [doc]
        else:
            nodes = txt_parser.get_nodes_from_documents([doc])
        logger.info(f"  - æ–‡ä»¶ '{file_path.name}' è¢«è§£ææˆ {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes)

    if all_nodes:
        logger.info(f"âš™ï¸ èŠ‚ç‚¹æ„å»ºå®Œæˆ ({len(all_nodes)}ä¸ªèŠ‚ç‚¹)ï¼Œå‡†å¤‡å°†æ•°æ®å­˜å…¥å‘é‡æ•°æ®åº“...")
        index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())
        index.insert_nodes(all_nodes, show_progress=True)
        logger.success(f"âœ… æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡ä»¶å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ã€‚")
        return True
    else:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: str = "text",
    doc_id: Optional[str] = None,
) -> bool:
    """
    Args:
        vector_store (VectorStore): ç›®æ ‡å‘é‡å­˜å‚¨ã€‚
        content (str): è¦å­˜å‚¨çš„æ–‡æœ¬å†…å®¹ã€‚
        metadata (Dict[str, Any]): ä¸å†…å®¹å…³è”çš„å…ƒæ•°æ®å­—å…¸ã€‚
        content_format (str): å†…å®¹æ ¼å¼ï¼Œæ”¯æŒ "text", "markdown", "json"ã€‚
        doc_id (Optional[str]): æ–‡æ¡£çš„å”¯ä¸€IDã€‚å¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆã€‚
    """
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False

    logger.info(f"âœï¸ æ­£åœ¨å°†ç±»å‹ä¸º '{metadata.get('type', 'N/A')}' (æ ¼å¼: {content_format}) çš„å†…å®¹å­˜å…¥å‘é‡åº“...")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())

    if content_format == "markdown":
        parser = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True)
        nodes = parser.get_nodes_from_documents([doc])
        index.insert_nodes(nodes)
    elif content_format == "json":
        # å¯¹äºJSONå­—ç¬¦ä¸²ï¼Œé€šå¸¸ä½œä¸ºå•ä¸ªæ–‡æ¡£ç›´æ¥æ’å…¥
        index.insert(doc)
    else:  # "text"
        parser = SentenceSplitter(chunk_size=600, chunk_overlap=120, include_metadata=True, include_prev_next_rel=True)
        nodes = parser.get_nodes_from_documents([doc])
        index.insert_nodes(nodes)

    logger.success(f"âœ… å†…å®¹å·²æˆåŠŸå­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {final_metadata}")
    return True


if __name__ == "__main__":
    from utils.log import init_logger
    init_logger(os.path.splitext(os.path.basename(__file__))[0])

    test_db_path = "./.test_chroma_db_vector"
    test_collection_name = "test_collection_vector"
    vector_store = get_vector_store(db_path=test_db_path, collection_name=test_collection_name)
    
    doc_id_1 = "single_doc_001"
    metadata_1 = {"type": "test_doc", "author": "tester1"}
    content_1 = "è¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½å¦‚ä½•æ”¹å˜è½¯ä»¶å·¥ç¨‹çš„æµ‹è¯•æ–‡æ¡£ã€‚"
    vector_store(
        vector_store=vector_store,
        content=content_1,
        metadata=metadata_1,
        content_format="text",
        doc_id=doc_id_1
    )

    test_input_dir = "./.test_input_dir_vector"
    os.makedirs(test_input_dir, exist_ok=True)
    
    with open(os.path.join(test_input_dir, "test1.txt"), "w", encoding="utf-8") as f:
        f.write("è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå†…å®¹æ˜¯å…³äºæœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ã€‚")
        
    with open(os.path.join(test_input_dir, "test2.md"), "w", encoding="utf-8") as f:
        f.write("# Markdown æµ‹è¯•\n\nè¿™æ˜¯ä¸€ä¸ª Markdown æ–‡ä»¶ï¼Œè®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ã€‚")

    vector_add_from_dir(
        vector_store=vector_store,
        input_dir=test_input_dir,
        required_exts=[".txt", ".md"]
    )

    query_text = "å¤§å‹è¯­è¨€æ¨¡å‹æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"
    answer, source_nodes = vector_query(
        vector_store=vector_store,
        query_text=query_text,
        similarity_top_k=5,
        rerank_top_n=2
    )

    if answer and source_nodes:
        logger.success("\n--- âœ… æœ€ç»ˆç­”æ¡ˆ ---")
        logger.info(answer)
        
        logger.info("\n--- ç­”æ¡ˆæ¥æº (ç»é‡æ’åº) ---")
        for i, node in enumerate(source_nodes):
            score = node.score if node.score is not None else 'N/A'
            score_str = f"{score:.4f}" if isinstance(score, float) else score
            logger.info(f"\nğŸ“„ æ–‡æ¡£ {i+1}: (ç›¸å…³æ€§å¾—åˆ†: {score_str})")
            logger.info(f"  - å…ƒæ•°æ®: {node.metadata}")
            logger.info(f"  - å†…å®¹:\n{node.get_content()}")
    else:
        logger.error("æŸ¥è¯¢å¤±è´¥ï¼Œæœªè¿”å›ä»»ä½•ç»“æœã€‚")

    logger.info("\n--- æµ‹è¯•å®Œæˆ ---")
    logger.info(f"ä½ å¯ä»¥æ£€æŸ¥ä»¥ä¸‹ç›®å½•æ¥éªŒè¯ç»“æœ: '{test_db_path}', '{test_input_dir}'")
