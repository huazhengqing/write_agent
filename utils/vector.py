import os
import re
import sys
import numpy as np
import threading
import asyncio
from datetime import datetime
from pathlib import Path
import json
import chromadb
from diskcache import Cache
from typing import cast
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
from pydantic import Field

from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import Document, Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser, JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import BaseNode, NodeWithScore, QueryBundle
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo, MetadataFilter
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.config import llm_temperatures, get_llm_params, get_embedding_params
from utils.file import cache_dir


cache_query_dir = cache_dir / "query"
cache_query_dir.mkdir(parents=True, exist_ok=True)
cache_query = Cache(str(cache_query_dir), size_limit=int(32 * (1024**2)))


###############################################################################


default_text_qa_prompt_tmpl_cn = """
ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ã€‚
---------------------
{context_str}
---------------------
è¯·ä¸¥æ ¼æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œä¸æ˜¯ä½ çš„å…ˆéªŒçŸ¥è¯†ï¼Œå›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ä¸è¦ç¼–é€ ç­”æ¡ˆï¼Œä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸”åªèƒ½æ˜¯ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
é—®é¢˜: {query_str}
å›ç­”: 
"""

default_text_qa_prompt_cn = PromptTemplate(default_text_qa_prompt_tmpl_cn)


default_refine_prompt_tmpl_cn = """
åŸå§‹é—®é¢˜å¦‚ä¸‹: {query_str}
æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªå›ç­”: {existing_answer}
æˆ‘ä»¬æœ‰æœºä¼šé€šè¿‡ä¸‹é¢çš„æ›´å¤šä¸Šä¸‹æ–‡æ¥ä¼˜åŒ–å·²æœ‰çš„å›ç­”(ä»…åœ¨éœ€è¦æ—¶)ã€‚
------------
{context_str}
------------
æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡ï¼Œä¼˜åŒ–åŸå§‹å›ç­”ä»¥æ›´å¥½åœ°å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç”¨ï¼Œè¯·è¿”å›åŸå§‹å›ç­”ã€‚
ä¼˜åŒ–åçš„å›ç­”: 
"""

default_refine_prompt_cn = PromptTemplate(default_refine_prompt_tmpl_cn)


synthesis_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["synthesis"])

response_synthesizer_default = CompactAndRefine(
    llm=LiteLLM(**synthesis_llm_params),
    text_qa_template=default_text_qa_prompt_cn,
    refine_template=default_refine_prompt_cn,
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )
)


###############################################################################


def init_llama_settings():
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


###############################################################################


_vector_stores: Dict[Tuple[str, str], ChromaVectorStore] = {}
_vector_store_lock = threading.Lock()

def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    with _vector_store_lock:
        cache_key = (db_path, collection_name)
        if cache_key in _vector_stores:
            return _vector_stores[cache_key]
        os.makedirs(db_path, exist_ok=True)
        db = chromadb.PersistentClient(path=db_path)
        chroma_collection = db.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        _vector_stores[cache_key] = vector_store
        return vector_store


_vector_indices: Dict[int, VectorStoreIndex] = {}
_vector_index_lock = threading.Lock()

def clear_vector_index_cache(vector_store: Optional[VectorStore] = None):
    with _vector_index_lock:
        if vector_store:
            cache_key = id(vector_store)
            if cache_key in _vector_indices:
                del _vector_indices[cache_key]
        else:
            _vector_indices.clear()


###############################################################################


def get_node_parser(content_format: Literal["markdown", "text", "json"]) -> NodeParser:
    if content_format == "json":
        return JSONNodeParser(
            include_metadata=True,
            max_depth=3, 
            levels_to_keep=0
        )
    elif content_format == "text":
        return SentenceSplitter(
            chunk_size=256, 
            chunk_overlap=50,
        )
    return MarkdownElementNodeParser(
        llm=Settings.llm,
        chunk_size = 256, 
        chunk_overlap = 50, 
        # num_workers=3,
        include_metadata=True,
        show_progress=False,
    )


###############################################################################


def _filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    """è¿‡æ»¤æ‰æ— æ•ˆçš„èŠ‚ç‚¹ï¼ˆå†…å®¹ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºç™½/éè¯æ±‡å­—ç¬¦ï¼‰ã€‚"""
    valid_nodes = []
    for node in nodes:
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    return valid_nodes


def default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]

    metadata_func = file_metadata_func or default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    # æŒ‰å†…å®¹æ ¼å¼å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç»„ï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†
    docs_by_format: Dict[str, List[Document]] = {"markdown": [], "text": [], "json": []}
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        file_extension = file_path.suffix.lstrip('.')
        content_format_map = {"md": "markdown", "txt": "text", "json": "json"}
        content_format = content_format_map.get(file_extension, "text")
        docs_by_format[content_format].append(doc)

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨ä¸º {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶æ‰¹é‡è§£æèŠ‚ç‚¹...")
        node_parser = get_node_parser(content_format)
        parsed_nodes = node_parser.get_nodes_from_documents(format_docs, show_progress=False)
        
        nodes_for_format = _filter_invalid_nodes(parsed_nodes)
        # è¿‡æ»¤æ‰ä»…åŒ…å«åˆ†éš”ç¬¦æˆ–ç©ºç™½ç­‰éæ–‡æœ¬å†…å®¹çš„æ— æ•ˆèŠ‚ç‚¹ (å·²ç§»è‡³ _filter_invalid_nodes)
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]
    
    # ç›¸ä¼¼åº¦æœç´¢å»é‡
    query_embedding = Settings.embed_model.get_text_embedding(content)
    logger.trace(f"ä¸º doc_id '{doc_id}' ç”Ÿæˆçš„åµŒå…¥å‘é‡ (å‰10ç»´): {query_embedding[:10]}")
    vector_store_query = VectorStoreQuery(
        query_embedding=query_embedding,
        similarity_top_k=1,
        filters=None,
    )
    query_result = vector_store.query(vector_store_query)
    if query_result.nodes:
        # å¦‚æœæ‰¾åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦æé«˜çš„èŠ‚ç‚¹ï¼Œæˆ‘ä»¬æœ‰ç†ç”±ç›¸ä¿¡è¿™æ˜¯é‡å¤å†…å®¹ã€‚
        # ä¹‹å‰çš„å®ç°è¯•å›¾æ¯”è¾ƒå®Œæ•´å†…å®¹å’ŒèŠ‚ç‚¹å†…å®¹ï¼Œè¿™æ˜¯ä¸å‡†ç¡®çš„ï¼Œå› ä¸ºèŠ‚ç‚¹åªæ˜¯æ–‡æ¡£çš„ä¸€éƒ¨åˆ†ã€‚
        # ä»…åŸºäºé«˜ç›¸ä¼¼åº¦åˆ†æ•°è¿›è¡Œåˆ¤æ–­æ˜¯æ›´ç®€å•ä¸”é²æ£’çš„åšæ³•ã€‚
        if query_result.similarities[0] > 0.995:
            logger.warning(f"å‘ç°ä¸ doc_id '{doc_id}' å†…å®¹é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {query_result.similarities[0]:.4f}) çš„æ–‡æ¡£ï¼Œè·³è¿‡æ·»åŠ ã€‚")
            return False

    if doc_id:
        logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
        vector_store.delete(ref_doc_id=doc_id)
        logger.info(f"å·²åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹ã€‚")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    node_parser = get_node_parser(content_format)
    parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
    nodes_to_insert = _filter_invalid_nodes(parsed_nodes)
    
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False
    logger.debug(f"ä¸º doc_id '{doc_id}' åˆ›å»ºçš„èŠ‚ç‚¹å†…å®¹: {[n.get_content(metadata_mode='all') for n in nodes_to_insert]}")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_default_vector_store_info() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
    similarity_cutoff: Optional[float] = None,
) -> BaseQueryEngine:
    
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
        f"similarity_cutoff={similarity_cutoff}"
    )

    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            index = _vector_indices[cache_key]
        else:
            index = VectorStoreIndex.from_vector_store(vector_store)
            _vector_indices[cache_key] = index

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        reranker = SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=rerank_top_n,
        )
        postprocessors.append(reranker)

    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    
    if use_auto_retriever:
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        
        final_vector_store_info = vector_store_info or get_default_vector_store_info()
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=final_vector_store_info,
            similarity_top_k=similarity_top_k,
            llm=reasoning_llm,
            verbose=True,
            similarity_cutoff=similarity_cutoff,
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer_default,
            node_postprocessors=postprocessors,
        )
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine
    else:
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        retriever_kwargs = {}
        if similarity_cutoff is not None:
            retriever_kwargs["similarity_cutoff"] = similarity_cutoff

        query_engine = index.as_query_engine(
            llm=reasoning_llm,
            response_synthesizer=response_synthesizer_default,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
            **retriever_kwargs,
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""

    cache_key = None
    retriever = getattr(query_engine, "retriever", getattr(query_engine, "_retriever", None))
    # æ³¨æ„ï¼šä¸‹é¢çš„ç¼“å­˜é”®ç”Ÿæˆæ–¹å¼ä¾èµ–äº llama-index å’Œ chromadb çš„å†…éƒ¨å®ç°ç»†èŠ‚ï¼ˆå¦‚ `_vector_store`, `_path`ï¼‰ã€‚
    # è¿™åœ¨åº“ç‰ˆæœ¬æ›´æ–°æ—¶å¯èƒ½ä¼šå¤±æ•ˆã€‚æ›´ç¨³å¦¥çš„æ–¹æ¡ˆæ˜¯æ˜¾å¼ä¼ é€’æ•°æ®åº“è·¯å¾„å’Œé›†åˆåç§°æ¥æ„å»ºç¼“å­˜é”®ã€‚
    # æ­¤å¤„ä½¿ç”¨ getattr è¿›è¡Œå®‰å…¨è®¿é—®ä»¥å¢åŠ ä»£ç éŸ§æ€§ã€‚
    vector_store = getattr(retriever, '_vector_store', None)
    if isinstance(vector_store, ChromaVectorStore):
        collection = getattr(vector_store, 'collection', None)
        client = getattr(vector_store, 'client', None)
        collection_name = getattr(collection, 'name', None)
        db_path = getattr(client, '_path', None)
        
        if db_path and collection_name:
            cache_key = f"index_query:{db_path}:{collection_name}:{question}"

    if cache_key:
        cached_result = cache_query.get(cache_key)
        if cached_result is not None:
            logger.info(f"ä»ç¼“å­˜ä¸­è·å–æŸ¥è¯¢ '{question}' çš„ç»“æœã€‚")
            return cached_result

    logger.info(f"å¼€å§‹æ‰§è¡Œç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    if not result or not getattr(result, "source_nodes", []) or not answer or answer == "Empty Response":
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºå›ç­”ã€‚")
        answer = ""

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”: {answer}")

    if cache_key:
        cache_query.set(cache_key, answer)

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    # ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘é‡ä¸º3ï¼Œé˜²æ­¢å¯¹LLM APIé€ æˆè¿‡å¤§å‹åŠ›ã€‚
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            return await index_query(query_engine, question)

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆã€‚")
    return results


###############################################################################
