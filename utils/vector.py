import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
import chromadb
from loguru import logger
from llama_index.core import (
    Document,
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.core import PromptTemplate
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore
from llama_index.core.vector_stores import MetadataFilters
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.llm import get_embedding_params, get_llm_params


_embed_model: Optional[LiteLLMEmbedding] = None
def get_embed_model() -> LiteLLMEmbedding:
    global _embed_model
    if _embed_model is None:
        embedding_params = get_embedding_params()
        embed_model_name = embedding_params.pop('model')
        _embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)
    return _embed_model


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    os.makedirs(db_path, exist_ok=True)
    db = chromadb.PersistentClient(path=db_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    return vector_store


def vector_query(
    vector_store: VectorStore,
    query_text: str,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
) -> Tuple[Optional[str], Optional[List[NodeWithScore]]]:
    
    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        rerank_llm_params = get_llm_params(llm="fast")
        reranker = LLMRerank(choice_batch_size=5, top_n=rerank_top_n, llm=LiteLLM(**rerank_llm_params))
        postprocessors.append(reranker)

    synthesis_llm_params = get_llm_params(llm="reasoning")
    synthesis_llm = LiteLLM(**synthesis_llm_params)

    # å®šä¹‰ä¸­æ–‡æç¤ºè¯
    TEXT_QA_TEMPLATE_CN = PromptTemplate(
        """ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
        ä½ å°†æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
        ---------------------
        {context_str}
        ---------------------
        åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{query_str}
        """
    )

    REFINE_TEMPLATE_CN = PromptTemplate(
        """ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººï¼Œä½ æ­£åœ¨æ”¹è¿›ä¸€ä¸ªå·²æœ‰çš„ç­”æ¡ˆã€‚
        ä½ å·²ç»æä¾›äº†ä¸€ä¸ªç­”æ¡ˆï¼š{existing_answer}
        ä½ ç°åœ¨æœ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        ---------------------
        {context_msg}
        ---------------------
        è¯·æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ”¹è¿›ä½ çš„ç­”æ¡ˆã€‚
        å¦‚æœä½ ä¸èƒ½æ”¹è¿›ä½ çš„ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¿”å›å·²æœ‰çš„ç­”æ¡ˆã€‚
        """
    )

    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params['context_window'],
            num_output=synthesis_llm_params['max_tokens'],
            chunk_overlap_ratio=0.2
        ),
        text_qa_template=TEXT_QA_TEMPLATE_CN,
        refine_template=REFINE_TEMPLATE_CN
    )

    query_engine = index.as_query_engine(
        llm=synthesis_llm,
        response_synthesizer=response_synthesizer,
        filters=filters,
        similarity_top_k=similarity_top_k,
        node_postprocessors=postprocessors
    )

    response = query_engine.query(query_text)
    if not response.response or not response.source_nodes:
        logger.warning("ğŸ¤· æœªèƒ½ç”Ÿæˆç­”æ¡ˆæˆ–æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
        return None, None

    return response.response, response.source_nodes


def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    metadata_func = file_metadata_func or _default_file_metadata
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    md_parser = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True)
    txt_parser = SentenceSplitter(chunk_size=600, chunk_overlap=120, include_metadata=True, include_prev_next_rel=True)
    all_nodes = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        if file_path.suffix == ".md":
            nodes = md_parser.get_nodes_from_documents([doc])
        elif file_path.suffix == ".json":
            nodes = [doc]
        else:
            nodes = txt_parser.get_nodes_from_documents([doc])
        logger.info(f"  - æ–‡ä»¶ '{file_path.name}' è¢«è§£ææˆ {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())
    index.insert_nodes(all_nodes, show_progress=True)
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())

    if content_format == "markdown":
        parser = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True)
        nodes = parser.get_nodes_from_documents([doc])
        index.insert_nodes(nodes)
    elif content_format == "json":
        index.insert(doc)
    else:
        parser = SentenceSplitter(chunk_size=600, chunk_overlap=120, include_metadata=True, include_prev_next_rel=True)
        nodes = parser.get_nodes_from_documents([doc])
        index.insert_nodes(nodes)

    return True


