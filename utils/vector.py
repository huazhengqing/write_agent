import os
from pathlib import Path
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional

from llama_index.core import Document
from llama_index.core import Settings, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.schema import BaseNode
from llama_index.llms.litellm import LiteLLM


###############################################################################


from llama_index.vector_stores.chroma import ChromaVectorStore
ChromaVectorStore.model_config['extra'] = 'allow'
if hasattr(ChromaVectorStore, 'model_rebuild'):
    ChromaVectorStore.model_rebuild(force=True)


def init_llama_settings():
    from utils.llm_api import llm_temperatures, get_llm_params, get_embedding_params
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    from llama_index.embeddings.litellm import LiteLLMEmbedding
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    db_path_obj = Path(db_path)
    db_path_obj.mkdir(parents=True, exist_ok=True)

    import chromadb
    db = chromadb.PersistentClient(path=str(db_path_obj))
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    store_cache_path = db_path_obj / f"{collection_name}.cache.db"
    from diskcache import Cache  
    vector_store.cache = Cache(str(store_cache_path), size_limit=int(32 * (1024**2)))
    
    return vector_store


def get_synthesizer():
    from utils.llm_api import llm_temperatures, get_llm_params
    synthesis_llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["synthesis"])
    from llama_index.core.response_synthesizers import TreeSummarize
    from utils.vector_prompts import tree_summary_prompt
    synthesizer = TreeSummarize(
        llm=LiteLLM(**synthesis_llm_params),
        summary_template=PromptTemplate(tree_summary_prompt),
        prompt_helper = PromptHelper(
            context_window=synthesis_llm_params.get('context_window', 8192),
            num_output=synthesis_llm_params.get('max_tokens', 2048),
            chunk_overlap_ratio=0.2,
        ),
        use_async=True,
    )
    return synthesizer


###############################################################################


def filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    valid_nodes = []
    initial_count = len(nodes)
    for node in nodes:
        import re
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    
    removed_count = initial_count - len(valid_nodes)
    if removed_count > 0:
        logger.debug(f"è¿‡æ»¤æ‰ {removed_count} ä¸ªæ— æ•ˆæˆ–ç©ºèŠ‚ç‚¹ã€‚")
    return valid_nodes


def _parse_content_to_nodes(
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"],
    doc_id: Optional[str] = None,
) -> List[BaseNode]:
    logger.info(f"å¼€å§‹ä¸º doc_id '{doc_id}' è§£æå†…å®¹ä¸ºèŠ‚ç‚¹ (æ ¼å¼: {content_format})...")
    doc = Document(text=content, metadata=metadata, id_=doc_id)
    from utils.vector_splitter import get_vector_node_parser
    node_parser = get_vector_node_parser(content_format, content_length=len(content))
    nodes = filter_invalid_nodes(node_parser.get_nodes_from_documents([doc], show_progress=False))
    logger.info(f"ä¸º doc_id '{doc_id}' è§£æå‡º {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    return nodes


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"] = "md",
    doc_id: Optional[str] = None,
) -> bool:
    logger.info(f"å¼€å§‹å‘å‘é‡åº“æ·»åŠ å†…å®¹, doc_id='{doc_id}', format='{content_format}'...")
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯, è·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False

    import hashlib
    new_content_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
    doc_cache = getattr(vector_store, "cache", None)
    if doc_cache and doc_cache.get(new_content_hash):
        return True

    effective_doc_id = doc_id or new_content_hash

    nodes_to_insert = _parse_content_to_nodes(content, metadata, content_format, effective_doc_id)
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (id: {effective_doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹, è·³è¿‡æ·»åŠ ã€‚")
        return False

    if doc_id:
        logger.info(f"ä¸º doc_id '{doc_id}' æ‰§è¡Œæ›´æ–°æ“ä½œ, å°†é¦–å…ˆåˆ é™¤æ—§èŠ‚ç‚¹ã€‚")
        vector_store.delete(doc_id)

    from llama_index.core.ingestion import IngestionPipeline
    pipeline = IngestionPipeline(vector_store=vector_store, transformations=[Settings.embed_model])
    pipeline.run(nodes=nodes_to_insert)

    if doc_cache:
        doc_cache.set(new_content_hash, True)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (id: {effective_doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_vector_store_info_default() -> 'VectorStoreInfo':
    from llama_index.core.vector_stores import VectorStoreInfo
    from llama_index.core.vector_stores import MetadataInfo
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸ, æ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def _create_auto_retriever_engine(
    index: VectorStoreIndex,
    vector_store_info: 'VectorStoreInfo',
    similarity_top_k: int,
    node_postprocessors: List,
) -> BaseQueryEngine:
    from utils.llm_api import llm_temperatures, get_llm_params
    from utils.vector_prompts import vector_store_query_prompt
    logger.info("æ­£åœ¨åˆ›å»º Auto-Retriever æŸ¥è¯¢å¼•æ“...")
    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    from llama_index.core.retrievers import VectorIndexAutoRetriever
    retriever = VectorIndexAutoRetriever(
        index=index,
        vector_store_info=vector_store_info,
        llm=reasoning_llm,
        prompt_template_str=vector_store_query_prompt, 
        similarity_top_k=similarity_top_k,
    )
    from llama_index.core.query_engine import RetrieverQueryEngine
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=get_synthesizer(),
        node_postprocessors=node_postprocessors,
    )
    logger.success("Auto-Retriever æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    return query_engine


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional['MetadataFilters'] = None,
    similarity_top_k: int = 50,
    top_n: int = 10,
    use_auto_retriever: bool = False,
    vector_store_info: Optional['VectorStoreInfo'] = None,
) -> BaseQueryEngine:
    from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo

    logger.info("å¼€å§‹æ„å»ºå‘é‡æŸ¥è¯¢å¼•æ“...")

    index = VectorStoreIndex.from_vector_store(vector_store)
    logger.debug("ä» VectorStore åˆ›å»º VectorStoreIndex å®Œæˆã€‚")

    reranker = None
    if top_n and top_n > 0:
        from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank
        reranker = SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=top_n,
        )
    node_postprocessors = [reranker] if reranker else []

    if use_auto_retriever:
        effective_vector_store_info = vector_store_info or get_vector_store_info_default()
        query_engine = _create_auto_retriever_engine(
            index=index,
            vector_store_info=effective_vector_store_info,
            similarity_top_k=similarity_top_k,
            node_postprocessors=node_postprocessors,
        )
    else:
        logger.info("æ­£åœ¨åˆ›å»ºæ ‡å‡†æŸ¥è¯¢å¼•æ“...")
        query_engine = index.as_query_engine(
            response_synthesizer=get_synthesizer(), 
            filters=filters, 
            similarity_top_k=similarity_top_k,
            node_postprocessors=node_postprocessors, 
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    
    logger.success("å‘é‡æŸ¥è¯¢å¼•æ“æ„å»ºæˆåŠŸã€‚")
    return query_engine


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""
    
    logger.info(f"å¼€å§‹æ‰§è¡Œå‘é‡ç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    source_nodes = getattr(result, "source_nodes", [])

    if not source_nodes or not answer or answer == "Empty Response" or "æ— æ³•å›ç­”" in answer:
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”, è¿”å›ç©ºå›ç­”ã€‚")
        answer = ""
    else:
        logger.info(f"æŸ¥è¯¢ '{question}' æ£€ç´¢åˆ° {len(source_nodes)} ä¸ªæºèŠ‚ç‚¹ã€‚")
        for i, node in enumerate(source_nodes):
            logger.debug(
                f"  - æºèŠ‚ç‚¹ {i+1} (ID: {node.node_id}, åˆ†æ•°: {node.score:.4f}):\n"
                f"{node.get_content()[:200]}..."
            )
        logger.success(f"æˆåŠŸå®Œæˆå¯¹ '{question}' çš„æŸ¥è¯¢, ç”Ÿæˆå›ç­”é•¿åº¦: {len(answer)}")

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”:\n{answer}")

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"å¼€å§‹æ‰§è¡Œ {len(questions)} ä¸ªé—®é¢˜çš„æ‰¹é‡å‘é‡æŸ¥è¯¢...")
    
    import asyncio
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            try:
                return await index_query(query_engine, question)
            except Exception as e:
                logger.error("æ‰¹é‡æŸ¥è¯¢ä¸­, é—®é¢˜ '{}' å¤±è´¥: {}", question, e, exc_info=True)
                return ""

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.info(f"æ”¶åˆ°æ‰¹é‡å›ç­”: \n{results}")
    return results
