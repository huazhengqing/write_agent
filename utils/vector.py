import os
import re
import asyncio
from datetime import datetime
from pathlib import Path
import chromadb
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional, get_args

from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import Document, Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser, JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.core.schema import BaseNode
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessor.siliconflow_rerank import SiliconFlowRerank

from utils.config import llm_temperatures, get_llm_params, get_embedding_params


###############################################################################


qa_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¿¡æ¯æå–åŠ©æ‰‹ã€‚

# ä»»åŠ¡
ä»ä¸‹æ–¹çš„`ä¸Šä¸‹æ–‡ä¿¡æ¯`ä¸­ï¼Œæå–ä¸`é—®é¢˜`ç›¸å…³çš„æ‰€æœ‰äº‹å®å’Œæè¿°ï¼Œå¹¶ä»¥æ¸…æ™°çš„é™ˆè¿°å¥å½¢å¼å‘ˆç°ã€‚

# æ ¸å¿ƒåŸåˆ™
1.  **å¿ äºåŸæ–‡**: ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäº`ä¸Šä¸‹æ–‡ä¿¡æ¯`ï¼Œç¦æ­¢å¼•å…¥å¤–éƒ¨çŸ¥è¯†ã€‚
2.  **æå–è€Œéå›ç­”**: ä½ çš„ç›®æ ‡æ˜¯æå–ä¿¡æ¯ç‰‡æ®µï¼Œè€Œä¸æ˜¯ç›´æ¥å½¢æˆå¯¹`é—®é¢˜`çš„æœ€ç»ˆç­”æ¡ˆã€‚å¦‚æœ`ä¸Šä¸‹æ–‡ä¿¡æ¯`åªåŒ…å«éƒ¨åˆ†ç›¸å…³ä¿¡æ¯ï¼Œå°±åªè¾“å‡ºé‚£éƒ¨åˆ†ã€‚
3.  **æ— ç›¸å…³åˆ™ä¸ºç©º**: å¦‚æœ`ä¸Šä¸‹æ–‡ä¿¡æ¯`ä¸`é—®é¢˜`å®Œå…¨æ— å…³ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
4.  **ç›´æ¥é™ˆè¿°**: ç›´æ¥åˆ—å‡ºäº‹å®ï¼Œä¸è¦æ·»åŠ å¼•è¿°æ€§çŸ­è¯­ã€‚

# ä¸Šä¸‹æ–‡ä¿¡æ¯
---------------------
{context_str}
---------------------

# é—®é¢˜
{query_str}

# æå–çš„äº‹å®
"""


refine_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é«˜çº§ä¿¡æ¯æ•´åˆå¸ˆã€‚

# ä»»åŠ¡
æ ¹æ®`æ–°çš„ä¸Šä¸‹æ–‡`ï¼Œä¼˜åŒ–`å·²æœ‰çš„å›ç­”`ï¼Œä»¥æ›´å…¨é¢ã€æ›´ç²¾ç¡®åœ°å›ç­”`åŸå§‹é—®é¢˜`ã€‚

# å·¥ä½œæµç¨‹
1.  **åˆ†ææ–°ä¿¡æ¯**: ä»”ç»†é˜…è¯»`æ–°çš„ä¸Šä¸‹æ–‡`ï¼Œè¯†åˆ«å‡ºå…¶ä¸­åŒ…å«çš„ã€ä½†`å·²æœ‰çš„å›ç­”`ä¸­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„æ–°ä¿¡æ¯ç‚¹ã€‚
2.  **æ¯”è¾ƒä¸æ•´åˆ**: å°†æ–°ä¿¡æ¯ç‚¹ä¸`å·²æœ‰çš„å›ç­”`è¿›è¡Œèåˆï¼Œéµå¾ªä¸‹æ–¹çš„æ ¸å¿ƒåŸåˆ™ã€‚
3.  **ç”Ÿæˆæ–°ç­”æ¡ˆ**: æ„å»ºä¸€ä¸ªå•ä¸€ã€è¿è´¯ã€å…¨é¢çš„æ–°ç­”æ¡ˆã€‚

# æ ¸å¿ƒåŸåˆ™
1.  **ä¿¡æ¯å®Œæ•´æ€§**: æœ€ç»ˆç­”æ¡ˆå¿…é¡»æ— ç¼æ•´åˆ`å·²æœ‰çš„å›ç­”`å’Œ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸­çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç¦æ­¢ä¸¢å¤±ä»»ä½•ç»†èŠ‚ã€‚
2.  **å¢é‡ä¼˜åŒ–**: ä½ çš„ç›®æ ‡æ˜¯â€œä¼˜åŒ–â€è€Œéâ€œé‡å†™â€ã€‚åªæœ‰å½“`æ–°çš„ä¸Šä¸‹æ–‡`èƒ½æä¾›è¡¥å……ã€ä¿®æ­£æˆ–æ›´å…·ä½“çš„ç»†èŠ‚æ—¶ï¼Œæ‰è¿›è¡Œä¿®æ”¹ã€‚
3.  **å†²çªå¤„ç†**: å¦‚æœ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸`å·²æœ‰çš„å›ç­”`ä¸­çš„ä¿¡æ¯å‘ç”Ÿå†²çªï¼Œè¯·ç»¼åˆåˆ¤æ–­ï¼Œä¿ç•™æ›´å…·ä½“ã€æ›´å¯ä¿¡çš„ä¿¡æ¯ã€‚å¦‚æœæ— æ³•åˆ¤æ–­ä¼˜åŠ£ï¼Œåˆ™åº”åŒæ—¶æåŠä¸¤ç§è¯´æ³•å¹¶æ˜ç¡®æŒ‡å‡ºå…¶çŸ›ç›¾ä¹‹å¤„ã€‚
4.  **æ— æ•ˆåˆ™è¿”å›åŸæ–‡**: å¦‚æœ`æ–°çš„ä¸Šä¸‹æ–‡`ä¸é—®é¢˜æ— å…³ï¼Œæˆ–æœªèƒ½æä¾›ä»»ä½•æœ‰ä»·å€¼çš„æ–°ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¿”å›`å·²æœ‰çš„å›ç­”`ï¼Œä¸è¦åšä»»ä½•æ”¹åŠ¨ã€‚
5.  **é£æ ¼ä¸€è‡´**: åœ¨ç”Ÿæˆæ–°ç­”æ¡ˆæ—¶ï¼Œå°½é‡ä¿æŒ`å·²æœ‰çš„å›ç­”`çš„è¯­è¨€é£æ ¼å’Œæ ¼å¼ï¼Œä½¿æœ€ç»ˆç­”æ¡ˆæµ‘ç„¶ä¸€ä½“ã€‚

# åŸå§‹é—®é¢˜
{query_str}

# å·²æœ‰çš„å›ç­”
{existing_answer}

# æ–°çš„ä¸Šä¸‹æ–‡
------------
{context_str}
------------

# ä¼˜åŒ–åçš„å›ç­”
"""


synthesis_llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["synthesis"])

synthesizer = CompactAndRefine(
    llm=LiteLLM(**synthesis_llm_params),
    text_qa_template=PromptTemplate(qa_prompt),
    refine_template=PromptTemplate(refine_prompt),
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )
)


###############################################################################


def init_llama_settings():
    llm_params = get_llm_params(llm_group="summary", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


###############################################################################


def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    os.makedirs(db_path, exist_ok=True)
    db = chromadb.PersistentClient(path=db_path)
    chroma_collection = db.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    return vector_store


###############################################################################


def file_metadata_default(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def _load_and_filter_documents(
    input_dir: str,
    metadata_func: Callable[[str], dict]
) -> List[Document]:
    """ä»ç›®å½•åŠ è½½æ–‡æ¡£å¹¶è¿‡æ»¤æ‰ç©ºæ–‡ä»¶ã€‚"""
    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )
    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return []

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è¿‡æ»¤å’Œè§£æ...")
    
    valid_docs = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        valid_docs.append(doc)
    
    return valid_docs


def _get_node_parser(content_format: Literal["md", "txt", "json"], content_length: int = 0) -> NodeParser:
    if content_length > 20000:
        chunk_size = 1024
        chunk_overlap = 200
    elif content_length > 5000:
        chunk_size = 512
        chunk_overlap = 128
    else:
        chunk_size = 256
        chunk_overlap = 64

    if content_format == "json":
        return JSONNodeParser(
            include_metadata=True,
            max_depth=5, 
            levels_to_keep=2
        )
    elif content_format == "txt":
        return SentenceSplitter(
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap,
        )
    return MarkdownElementNodeParser(
        llm=Settings.llm,
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap, 
        # num_workers=3,
        include_metadata=True,
        show_progress=False,
    )


def filter_invalid_nodes(nodes: List[BaseNode]) -> List[BaseNode]:
    """è¿‡æ»¤æ‰æ— æ•ˆçš„èŠ‚ç‚¹ï¼ˆå†…å®¹ä¸ºç©ºæˆ–ä»…åŒ…å«ç©ºç™½/éè¯æ±‡å­—ç¬¦ï¼‰ã€‚"""
    valid_nodes = []
    for node in nodes:
        if node.text.strip() and re.search(r'\w', node.text):
            valid_nodes.append(node)
    return valid_nodes


def _parse_docs_to_nodes_by_format(documents: List[Document]) -> List[BaseNode]:
    """æ ¹æ®æ–‡ä»¶æ ¼å¼å°†æ–‡æ¡£è§£æä¸ºèŠ‚ç‚¹ã€‚"""
    docs_by_format: Dict[str, List[Document]] = {
        "md": [], 
        "txt": [], 
        "json": []
    }
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        file_extension = file_path.suffix.lstrip('.')
        if file_extension in docs_by_format:
            docs_by_format[file_extension].append(doc)
        else:
            logger.warning(f"æ£€æµ‹åˆ°æœªæ”¯æŒçš„æ–‡ä»¶æ‰©å±•å '{file_extension}'ï¼Œå°†å¿½ç•¥ã€‚")

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨ä¸º {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶åŠ¨æ€è§£æèŠ‚ç‚¹...")
        nodes_for_format = []
        for doc in format_docs:
            node_parser = _get_node_parser(content_format, content_length=len(doc.text))
            parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
            nodes_for_format.extend(filter_invalid_nodes(parsed_nodes))
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)
    
    return all_nodes


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    metadata_func: Callable[[str], dict] = file_metadata_default,
) -> bool:
    documents = _load_and_filter_documents(input_dir, metadata_func)
    if not documents:
        return False

    all_nodes = _parse_docs_to_nodes_by_format(documents)
    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def _is_content_too_similar(
    vector_store: VectorStore,
    content: str,
    threshold: float,
    doc_id: Optional[str] = None
) -> bool:
    """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å‘é‡åº“ä¸­ç°æœ‰æ–‡æ¡£è¿‡äºç›¸ä¼¼ã€‚"""
    query_embedding = Settings.embed_model.get_text_embedding(content)
    logger.trace(f"ä¸º doc_id '{doc_id}' ç”Ÿæˆçš„åµŒå…¥å‘é‡ (å‰10ç»´): {query_embedding[:10]}")
    vector_store_query = VectorStoreQuery(
        query_embedding=query_embedding, similarity_top_k=1, filters=None
    )
    query_result = vector_store.query(vector_store_query)
    if query_result.nodes and query_result.similarities:
        is_updating_itself = doc_id and query_result.nodes[0].ref_doc_id == doc_id
        if not is_updating_itself and query_result.similarities[0] > threshold:
            logger.warning(f"å‘ç°ä¸ doc_id '{doc_id}' å†…å®¹é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {query_result.similarities[0]:.4f}) çš„æ–‡æ¡£ (ID: '{query_result.nodes[0].ref_doc_id}'), è·³è¿‡æ·»åŠ ã€‚")
            return True
    return False


def _parse_content_to_nodes(
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"],
    doc_id: Optional[str] = None,
) -> List[BaseNode]:
    """å°†å•ä¸ªå†…å®¹å­—ç¬¦ä¸²è§£æä¸ºèŠ‚ç‚¹åˆ—è¡¨ã€‚"""
    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")
    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    node_parser = _get_node_parser(content_format, content_length=len(content))
    return filter_invalid_nodes(node_parser.get_nodes_from_documents([doc], show_progress=False))


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["md", "txt", "json"] = "md",
    doc_id: Optional[str] = None,
    check_similarity: bool = False,
    similarity_threshold: float = 0.999,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    if check_similarity and _is_content_too_similar(vector_store, content, similarity_threshold, doc_id):
        return False

    if doc_id:
        logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
        vector_store.delete(ref_doc_id=doc_id)

    nodes_to_insert = _parse_content_to_nodes(content, metadata, content_format, doc_id)
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_vector_store_info_default() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def _create_reranker(rerank_top_n: int) -> Optional[SiliconFlowRerank]:
    if rerank_top_n and rerank_top_n > 0:
        return SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=rerank_top_n,
        )
    return None


def _create_auto_retriever_engine(
    index: VectorStoreIndex,
    vector_store_info: VectorStoreInfo,
    similarity_top_k: int,
    similarity_cutoff: float,
    postprocessors: List,
) -> BaseQueryEngine:
    logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    retriever = VectorIndexAutoRetriever(
        index,
        vector_store_info=vector_store_info,
        similarity_top_k=similarity_top_k,
        llm=reasoning_llm,
        verbose=True,
        similarity_cutoff=similarity_cutoff,
    )
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=synthesizer,
        node_postprocessors=postprocessors,
    )
    logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
    return query_engine


def _create_standard_query_engine(
    index: VectorStoreIndex,
    filters: Optional[MetadataFilters],
    similarity_top_k: int,
    similarity_cutoff: float,
    postprocessors: List,
) -> BaseQueryEngine:
    logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
    return index.as_query_engine(
        response_synthesizer=synthesizer, filters=filters, similarity_top_k=similarity_top_k,
        node_postprocessors=postprocessors, similarity_cutoff=similarity_cutoff
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 25,
    rerank_top_n: int = 5,
    similarity_cutoff: float = 0,
    use_auto_retriever: bool = False,
    vector_store_info: VectorStoreInfo = get_vector_store_info_default(),
) -> BaseQueryEngine:
    
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
        f"similarity_cutoff={similarity_cutoff}"
    )

    index = VectorStoreIndex.from_vector_store(vector_store)

    reranker = _create_reranker(rerank_top_n)
    postprocessors = [reranker] if reranker else []

    if use_auto_retriever:
        return _create_auto_retriever_engine(
            index=index,
            vector_store_info=vector_store_info,
            similarity_top_k=similarity_top_k,
            similarity_cutoff=similarity_cutoff,
            postprocessors=postprocessors,
        )
    else:
        return _create_standard_query_engine(
            index=index,
            filters=filters,
            similarity_top_k=similarity_top_k,
            similarity_cutoff=similarity_cutoff,
            postprocessors=postprocessors,
        )


###############################################################################


async def index_query(query_engine: BaseQueryEngine, question: str) -> str:
    if not question:
        return ""

    logger.info(f"å¼€å§‹æ‰§è¡Œç´¢å¼•æŸ¥è¯¢: '{question}'")
    result = await query_engine.aquery(question)

    answer = str(getattr(result, "response", "")).strip()
    if not result or not getattr(result, "source_nodes", []) or not answer or answer == "Empty Response":
        logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºå›ç­”ã€‚")
        answer = ""

    logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”: {answer}")

    return answer


async def index_query_batch(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    # ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘é‡ä¸º3ï¼Œé˜²æ­¢å¯¹LLM APIé€ æˆè¿‡å¤§å‹åŠ›ã€‚
    sem = asyncio.Semaphore(3)

    async def safe_query(question: str) -> str:
        async with sem:
            try:
                return await index_query(query_engine, question)
            except Exception as e:
                logger.error("æ‰¹é‡æŸ¥è¯¢ä¸­ï¼Œé—®é¢˜ '{}' å¤±è´¥: {}", question, e, exc_info=True)
                return ""

    tasks = [safe_query(q) for q in questions]
    results = await asyncio.gather(*tasks)

    logger.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆã€‚")
    return results
