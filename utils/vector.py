import os
import re
import sys
import numpy as np
import threading
import asyncio
from datetime import datetime
from pathlib import Path
import json
import chromadb
from typing import cast
from loguru import logger
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
from pydantic import Field

from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import Document, Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores import VectorStoreQuery
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser, JSONNodeParser, SentenceSplitter
from llama_index.core.node_parser.interface import NodeParser
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
from llama_index.postprocessors.siliconflow_rerank import SiliconFlowRerank

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.config import llm_temperatures, get_llm_params, get_embedding_params


###############################################################################


default_text_qa_prompt_tmpl_cn = """
ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ã€‚
---------------------
{context_str}
---------------------
è¯·ä¸¥æ ¼æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œä¸æ˜¯ä½ çš„å…ˆéªŒçŸ¥è¯†ï¼Œå›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ä¸è¦ç¼–é€ ç­”æ¡ˆï¼Œä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸”åªèƒ½æ˜¯ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
é—®é¢˜: {query_str}
å›ç­”: 
"""

default_text_qa_prompt_cn = PromptTemplate(default_text_qa_prompt_tmpl_cn)


default_refine_prompt_tmpl_cn = """
åŸå§‹é—®é¢˜å¦‚ä¸‹: {query_str}
æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªå›ç­”: {existing_answer}
æˆ‘ä»¬æœ‰æœºä¼šé€šè¿‡ä¸‹é¢çš„æ›´å¤šä¸Šä¸‹æ–‡æ¥ä¼˜åŒ–å·²æœ‰çš„å›ç­”(ä»…åœ¨éœ€è¦æ—¶)ã€‚
------------
{context_str}
------------
æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡ï¼Œä¼˜åŒ–åŸå§‹å›ç­”ä»¥æ›´å¥½åœ°å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç”¨ï¼Œè¯·è¿”å›åŸå§‹å›ç­”ã€‚
ä¼˜åŒ–åçš„å›ç­”: 
"""

default_refine_prompt_cn = PromptTemplate(default_refine_prompt_tmpl_cn)


synthesis_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["synthesis"])

response_synthesizer_default = CompactAndRefine(
    llm=LiteLLM(**synthesis_llm_params),
    text_qa_template=default_text_qa_prompt_cn,
    refine_template=default_refine_prompt_cn,
    prompt_helper = PromptHelper(
        context_window=synthesis_llm_params.get('context_window', 8192),
        num_output=synthesis_llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )
)


###############################################################################


def init_llama_settings():
    llm_params = get_llm_params(llm_group="fast", temperature=llm_temperatures["summarization"])
    Settings.llm = LiteLLM(**llm_params)
    
    Settings.prompt_helper = PromptHelper(
        context_window=llm_params.get('context_window', 8192),
        num_output=llm_params.get('max_tokens', 2048),
        chunk_overlap_ratio=0.2,
    )

    embedding_params = get_embedding_params()
    embed_model_name = embedding_params.pop('model')
    Settings.embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)

init_llama_settings()


###############################################################################


_vector_stores: Dict[Tuple[str, str], ChromaVectorStore] = {}
_vector_store_lock = threading.Lock()

def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    with _vector_store_lock:
        cache_key = (db_path, collection_name)
        if cache_key in _vector_stores:
            return _vector_stores[cache_key]
        os.makedirs(db_path, exist_ok=True)
        db = chromadb.PersistentClient(path=db_path)
        chroma_collection = db.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        _vector_stores[cache_key] = vector_store
        return vector_store


_vector_indices: Dict[int, VectorStoreIndex] = {}
_vector_index_lock = threading.Lock()


###############################################################################


def get_node_parser(content_format: Literal["markdown", "text", "json"]) -> NodeParser:
    if content_format == "json":
        return JSONNodeParser(
            include_metadata=True,
            max_depth=3, 
            levels_to_keep=0
        )
    elif content_format == "text":
        return SentenceSplitter(
            chunk_size=256, 
            chunk_overlap=50,
        )
    return MarkdownElementNodeParser(
        llm=Settings.llm,
        chunk_size = 256, 
        chunk_overlap = 50, 
        # num_workers=3,
        include_metadata=True,
        show_progress=False,
    )


###############################################################################


def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]

    metadata_func = file_metadata_func or _default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    # æŒ‰å†…å®¹æ ¼å¼å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç»„ï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†
    docs_by_format: Dict[str, List[Document]] = {"markdown": [], "text": [], "json": []}
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text or not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        file_extension = file_path.suffix.lstrip('.')
        content_format_map = {"md": "markdown", "txt": "text", "json": "json"}
        content_format = content_format_map.get(file_extension, "text")
        docs_by_format[content_format].append(doc)

    all_nodes = []
    for content_format, format_docs in docs_by_format.items():
        if not format_docs:
            continue
        
        logger.info(f"æ­£åœ¨ä¸º {len(format_docs)} ä¸ª '{content_format}' æ–‡ä»¶æ‰¹é‡è§£æèŠ‚ç‚¹...")
        node_parser = get_node_parser(content_format)
        parsed_nodes = node_parser.get_nodes_from_documents(format_docs, show_progress=True)
        
        # è¿‡æ»¤æ‰ä»…åŒ…å«åˆ†éš”ç¬¦æˆ–ç©ºç™½ç­‰éæ–‡æœ¬å†…å®¹çš„æ— æ•ˆèŠ‚ç‚¹
        nodes_for_format = [node for node in parsed_nodes if node.text.strip() and re.search(r'\w', node.text)]
        logger.info(f"  - ä» '{content_format}' æ–‡ä»¶ä¸­æˆåŠŸè§£æå‡º {len(nodes_for_format)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes_for_format)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    unique_nodes = []
    seen_ids = set()
    for node in all_nodes:
        if node.id_ not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node.id_)
        else:
            logger.warning(f"å‘ç°å¹¶ç§»é™¤äº†é‡å¤çš„èŠ‚ç‚¹ID: {node.id_}ã€‚è¿™å¯èƒ½ç”±åŒ…å«å¤šä¸ªè¡¨æ ¼çš„Markdownæ–‡ä»¶å¼•èµ·ã€‚")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=unique_nodes)

    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]
    
    # ç›¸ä¼¼åº¦æœç´¢å»é‡
    query_embedding = Settings.embed_model.get_text_embedding(content)
    logger.trace(f"ä¸º doc_id '{doc_id}' ç”Ÿæˆçš„åµŒå…¥å‘é‡ (å‰10ç»´): {query_embedding[:10]}")
    vector_store_query = VectorStoreQuery(
        query_embedding=query_embedding,
        similarity_top_k=1,
        filters=None,
    )
    query_result = vector_store.query(vector_store_query)
    if query_result.nodes:
        similarity_score = query_result.similarities[0]
        if similarity_score > 0.99:
            most_similar_node_content = query_result.nodes[0].get_content()
            if most_similar_node_content == content:
                logger.warning(f"å‘ç°ä¸ doc_id '{doc_id}' å†…å®¹å®Œå…¨ç›¸åŒ (ç›¸ä¼¼åº¦: {similarity_score:.4f}) çš„æ–‡æ¡£ï¼Œè·³è¿‡æ·»åŠ ã€‚")
                return False
            else:
                logger.critical(
                    f"æ£€æµ‹åˆ°å‘é‡ç¢°æ’ (ç›¸ä¼¼åº¦: {similarity_score:.4f})ï¼"
                    f"ä¸åŒçš„å†…å®¹äº§ç”Ÿäº†ç›¸åŒçš„å‘é‡ï¼Œè¿™é€šå¸¸æ˜¯åµŒå…¥æ¨¡å‹å­˜åœ¨ä¸¥é‡é—®é¢˜çš„è¿¹è±¡ã€‚"
                    f"Doc ID: '{doc_id}', æ–°å†…å®¹: '{content[:500]}...', "
                    f"å·²å­˜åœ¨å†…å®¹: '{most_similar_node_content[:500]}...'"
                )

    if doc_id:
        logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
        vector_store.delete(ref_doc_id=doc_id)
        logger.info(f"å·²åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹ã€‚")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    node_parser = get_node_parser(content_format)
    parsed_nodes = node_parser.get_nodes_from_documents([doc], show_progress=False)
    nodes_to_insert = [node for node in parsed_nodes if node.text.strip() and re.search(r'\w', node.text)]
    
    if not nodes_to_insert:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False
    logger.debug(f"ä¸º doc_id '{doc_id}' åˆ›å»ºçš„èŠ‚ç‚¹å†…å®¹: {[n.get_content(metadata_mode='all') for n in nodes_to_insert]}")

    pipeline = IngestionPipeline(vector_store=vector_store)
    pipeline.run(nodes=nodes_to_insert)

    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes_to_insert)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


###############################################################################


def get_default_vector_store_info() -> VectorStoreInfo:
    metadata_field_info = [
        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
    similarity_cutoff: Optional[float] = None,
) -> BaseQueryEngine:
    
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}, "
        f"similarity_cutoff={similarity_cutoff}"
    )

    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            index = _vector_indices[cache_key]
        else:
            index = VectorStoreIndex.from_vector_store(vector_store)
            _vector_indices[cache_key] = index

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        reranker = SiliconFlowRerank(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            top_n=rerank_top_n,
        )
        postprocessors.append(reranker)

    reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)
    
    if use_auto_retriever:
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        
        final_vector_store_info = vector_store_info or get_default_vector_store_info()
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=final_vector_store_info,
            similarity_top_k=similarity_top_k,
            llm=reasoning_llm,
            verbose=True,
            similarity_cutoff=similarity_cutoff,
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer_default,
            node_postprocessors=postprocessors,
        )
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine
    else:
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        retriever_kwargs = {}
        if similarity_cutoff is not None:
            retriever_kwargs["similarity_cutoff"] = similarity_cutoff

        query_engine = index.as_query_engine(
            llm=reasoning_llm,
            response_synthesizer=response_synthesizer_default,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
            **retriever_kwargs,
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine


###############################################################################


async def index_query(query_engine: BaseQueryEngine, questions: List[str]) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    tasks = []
    for q in questions:
        query_text = f"{q}\n# è¯·ä½¿ç”¨ä¸­æ–‡å›å¤"
        tasks.append(query_engine.aquery(query_text))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    final_answers = []
    for question, result in zip(questions, results):
        if isinstance(result, Exception):
            logger.warning(f"æŸ¥è¯¢ '{question}' æ—¶å‡ºé”™: {result}")
            final_answers.append("")
            continue

        answer = str(getattr(result, "response", "")).strip()
        if (
            not result
            or not getattr(result, "source_nodes", [])
            or not answer
            or answer == "Empty Response"
        ):
            logger.warning(f"æŸ¥è¯¢ '{question}' æœªæ£€ç´¢åˆ°ä»»ä½•æºèŠ‚ç‚¹æˆ–æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºå›ç­”ã€‚")
            final_answers.append("")
            continue

        final_answers.append(answer)
        logger.debug(f"é—®é¢˜ '{question}' çš„å›ç­”: {answer}")

    logger.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œå…±è¿”å› {len(final_answers)} ä¸ªå›ç­”ã€‚")
    return final_answers


###############################################################################


async def _test_embedding_model():
    """ä¸“é—¨æµ‹è¯•åµŒå…¥æ¨¡å‹çš„åŠŸèƒ½å’Œæ­£ç¡®æ€§ã€‚"""
    logger.info("--- 3. æµ‹è¯•åµŒå…¥æ¨¡å‹ (Embedding Model) ---")
    embed_model = Settings.embed_model

    # 1. æµ‹è¯•ä¸åŒæ–‡æœ¬æ˜¯å¦äº§ç”Ÿä¸åŒå‘é‡
    logger.info("--- 3.1. æµ‹è¯•ä¸åŒæ–‡æœ¬çš„å‘é‡å·®å¼‚æ€§ ---")
    text1 = "è¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„å¥å­ã€‚"
    text2 = "è¿™æ˜¯ä¸€ä¸ªå…³äºè‡ªç„¶è¯­è¨€å¤„ç†çš„å¥å­ã€‚"
    
    try:
        embedding1_list = await embed_model.aget_text_embedding(text1)
        embedding2_list = await embed_model.aget_text_embedding(text2)
        embedding1 = np.array(embedding1_list)
        embedding2 = np.array(embedding2_list)

        logger.debug(f"æ–‡æœ¬1çš„å‘é‡ (å‰5ç»´): {embedding1[:5]}")
        logger.debug(f"æ–‡æœ¬2çš„å‘é‡ (å‰5ç»´): {embedding2[:5]}")

        # æ£€æŸ¥å‘é‡æ˜¯å¦å…¨ä¸ºé›¶
        assert np.any(embedding1 != 0), "åµŒå…¥å‘é‡1ä¸åº”ä¸ºå…¨é›¶å‘é‡ï¼Œè¿™è¡¨æ˜åµŒå…¥æ¨¡å‹å¯èƒ½æœªæ­£ç¡®å·¥ä½œã€‚"
        assert np.any(embedding2 != 0), "åµŒå…¥å‘é‡2ä¸åº”ä¸ºå…¨é›¶å‘é‡ï¼Œè¿™è¡¨æ˜åµŒå…¥æ¨¡å‹å¯èƒ½æœªæ­£ç¡®å·¥ä½œã€‚"
        logger.info("å‘é‡éé›¶æ£€æŸ¥é€šè¿‡ã€‚")

        # æ£€æŸ¥å‘é‡æ˜¯å¦ç›¸åŒ
        are_equal = np.array_equal(embedding1, embedding2)
        assert not are_equal, "ä¸åŒæ–‡æœ¬ä¸åº”äº§ç”Ÿå®Œå…¨ç›¸åŒçš„åµŒå…¥å‘é‡ã€‚å¦‚æœç›¸åŒï¼Œè¯´æ˜åµŒå…¥æ¨¡å‹å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼ˆå‘é‡ç¢°æ’ï¼‰ã€‚"
        logger.info("ä¸åŒæ–‡æœ¬çš„å‘é‡ä¸åŒï¼Œæ£€æŸ¥é€šè¿‡ã€‚")

        # æ£€æŸ¥å‘é‡ç›¸ä¼¼åº¦
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        assert norm1 > 0 and norm2 > 0, "å‘é‡æ¨¡é•¿ä¸èƒ½ä¸ºé›¶ã€‚"
        
        similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
        logger.info(f"ä¸¤ä¸ªä¸åŒä½†ç›¸å…³å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
        assert 0.5 < similarity < 0.999, "ç›¸å…³å¥å­çš„ç›¸ä¼¼åº¦åº”åœ¨åˆç†èŒƒå›´å†… (å¤§äº0.5ï¼Œå°äº1)ã€‚"
        logger.info("ç›¸å…³å¥å­ç›¸ä¼¼åº¦æ£€æŸ¥é€šè¿‡ã€‚")

    except Exception as e:
        logger.error(f"è·å–åµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}", exc_info=True)
        assert False, "åµŒå…¥æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥ã€ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹é…ç½®ã€‚"

    # 2. æµ‹è¯•ç›¸åŒæ–‡æœ¬æ˜¯å¦äº§ç”Ÿç›¸åŒå‘é‡
    logger.info("--- 3.2. æµ‹è¯•ç›¸åŒæ–‡æœ¬çš„å‘é‡ä¸€è‡´æ€§ ---")
    try:
        embedding1_again_list = await embed_model.aget_text_embedding(text1)
        embedding1_again = np.array(embedding1_again_list)
        np.testing.assert_allclose(embedding1, embedding1_again, rtol=1e-5)
        logger.info("ç›¸åŒæ–‡æœ¬çš„å‘é‡ç›¸åŒï¼Œæ£€æŸ¥é€šè¿‡ã€‚")
    except Exception as e:
        logger.error(f"æµ‹è¯•ç›¸åŒæ–‡æœ¬å‘é‡æ—¶å‡ºé”™: {e}", exc_info=True)
        assert False, "ç›¸åŒæ–‡æœ¬å‘é‡ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥ã€‚"

    # 3. æµ‹è¯•æ‰¹é‡åµŒå…¥
    logger.info("--- 3.3. æµ‹è¯•æ‰¹é‡åµŒå…¥ ---")
    try:
        texts_batch = [text1, text2, "ç¬¬ä¸‰ä¸ªå®Œå…¨ä¸åŒçš„å¥å­ã€‚"]
        embeddings_batch = await embed_model.aget_text_embedding_batch(texts_batch)
        assert len(embeddings_batch) == 3, f"æ‰¹é‡åµŒå…¥åº”è¿”å›3ä¸ªå‘é‡ï¼Œä½†è¿”å›äº†{len(embeddings_batch)}ä¸ªã€‚"
        logger.info("æ‰¹é‡åµŒå…¥è¿”å›äº†æ­£ç¡®æ•°é‡çš„å‘é‡ã€‚")
        np.testing.assert_allclose(np.array(embeddings_batch[0]), embedding1, rtol=1e-5)
        logger.info("æ‰¹é‡åµŒå…¥çš„ç¬¬ä¸€ä¸ªç»“æœä¸å•ä¸ªåµŒå…¥ç»“æœä¸€è‡´ï¼Œæ£€æŸ¥é€šè¿‡ã€‚")
    except Exception as e:
        logger.error(f"æ‰¹é‡åµŒå…¥æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        assert False, "æ‰¹é‡åµŒå…¥æµ‹è¯•å¤±è´¥ã€‚"
    logger.success("--- åµŒå…¥æ¨¡å‹æµ‹è¯•é€šè¿‡ ---")

async def _test_reranker():
    """ä¸“é—¨æµ‹è¯•é‡æ’æœåŠ¡çš„åŠŸèƒ½å’Œæ­£ç¡®æ€§ã€‚"""
    logger.info("--- æµ‹è¯•é‡æ’æœåŠ¡ (Reranker) ---")
    
    query = "å“ªéƒ¨ä½œå“æ˜¯å…³äºä¸€ä¸ªç”·å­©å‘ç°è‡ªå·±æ˜¯å·«å¸ˆçš„æ•…äº‹ï¼Ÿ"
    documents = [
        "ã€Šæ²™ä¸˜ã€‹æ˜¯ä¸€éƒ¨å…³äºæ˜Ÿé™…æ”¿æ²»å’Œå·¨å‹æ²™è™«çš„å²è¯—ç§‘å¹»å°è¯´ã€‚", # low relevance
        "ã€Šå“ˆåˆ©Â·æ³¢ç‰¹ä¸é­”æ³•çŸ³ã€‹è®²è¿°äº†ä¸€ä¸ªåå«å“ˆåˆ©Â·æ³¢ç‰¹çš„å¹´è½»ç”·å­©ï¼Œä»–å‘ç°è‡ªå·±æ˜¯ä¸€ä¸ªå·«å¸ˆï¼Œå¹¶è¢«éœæ ¼æ²ƒèŒ¨é­”æ³•å­¦æ ¡å½•å–ã€‚", # high relevance
        "ã€Šé­”æˆ’ã€‹è®²è¿°äº†éœæ¯”ç‰¹äººä½›ç½—å¤šÂ·å·´é‡‘æ–¯æ‘§æ¯è‡³å°Šé­”æˆ’çš„æ—…ç¨‹ã€‚", # medium relevance
        "ã€Šç¥ç»æ¼«æ¸¸è€…ã€‹æ˜¯ä¸€éƒ¨èµ›åšæœ‹å…‹å°è¯´ï¼Œæ¢è®¨äº†äººå·¥æ™ºèƒ½å’Œè™šæ‹Ÿç°å®ã€‚", # low relevance
        "ä¸€ä¸ªç”·å­©åœ¨é­”æ³•å­¦æ ¡å­¦ä¹ çš„æ•…äº‹ï¼Œä»–æœ€å¥½çš„æœ‹å‹æ˜¯ä¸€ä¸ªçº¢å‘ç”·å­©å’Œä¸€ä¸ªèªæ˜çš„å¥³å­©ã€‚", # high relevance, but less specific
    ]
    
    reranker = SiliconFlowRerank(
        api_key=os.getenv("SILICONFLOW_API_KEY"),
        top_n=3,
    )
    
    nodes = [NodeWithScore(node=Document(text=d), score=1.0) for d in documents]
    query_bundle = QueryBundle(query_str=query)
    
    try:
        reranked_nodes = await reranker.aprocess_nodes(nodes, query_bundle=query_bundle)
        
        assert len(reranked_nodes) <= 3, f"é‡æ’ååº”è¿”å›æœ€å¤š 3 ä¸ªèŠ‚ç‚¹, ä½†è¿”å›äº† {len(reranked_nodes)} ä¸ªã€‚"
        logger.info(f"é‡æ’åè¿”å› {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œæ•°é‡æ­£ç¡®ã€‚")
        
        assert len(reranked_nodes) > 0, "Reranker è¿”å›äº†ç©ºåˆ—è¡¨ï¼ŒæœåŠ¡å¯èƒ½æœªæ­£å¸¸å·¥ä½œã€‚"

        reranked_texts = [node.get_content() for node in reranked_nodes]
        reranked_scores = [node.score for node in reranked_nodes]
        logger.info(f"é‡æ’åçš„æ–‡æ¡£é¡ºåºåŠåˆ†æ•°: {list(zip(reranked_texts, reranked_scores))}")
        
        assert "å“ˆåˆ©Â·æ³¢ç‰¹" in reranked_texts[0], "æœ€ç›¸å…³çš„æ–‡æ¡£æ²¡æœ‰æ’åœ¨ç¬¬ä¸€ä½ã€‚"
        logger.info("æœ€ç›¸å…³çš„æ–‡æ¡£æ’åºæ­£ç¡®ã€‚")
        
        for i in range(len(reranked_scores) - 1):
            assert reranked_scores[i] >= reranked_scores[i+1], f"é‡æ’ååˆ†æ•°æ²¡æœ‰é€’å‡: {reranked_scores}"
        logger.info("é‡æ’ååˆ†æ•°é€’å‡ï¼Œæ£€æŸ¥é€šè¿‡ã€‚")

    except Exception as e:
        logger.error(f"é‡æ’æœåŠ¡æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        assert False, "é‡æ’æœåŠ¡æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIæˆ–é…ç½®ã€‚"
        
    logger.success("--- é‡æ’æœåŠ¡æµ‹è¯•é€šè¿‡ ---")

def _prepare_test_data(input_dir: str):
    """å‡†å¤‡æ‰€æœ‰ç”¨äºæµ‹è¯•çš„è¾“å…¥æ–‡ä»¶ã€‚"""
    logger.info(f"--- 2. å‡†å¤‡å¤šæ ·åŒ–çš„æµ‹è¯•æ–‡ä»¶ ---")
    # çŸ­æ–‡æœ¬
    (Path(input_dir) / "doc1.md").write_text("# è§’è‰²ï¼šé¾™å‚²å¤©\né¾™å‚²å¤©æ˜¯ä¸€åæ¥è‡ªå¼‚ä¸–ç•Œçš„ç©¿è¶Šè€…ã€‚", encoding='utf-8')
    (Path(input_dir) / "doc2.txt").write_text("ä¸–ç•Œæ ‘æ˜¯å®‡å®™çš„ä¸­å¿ƒï¼Œè¿æ¥ç€ä¹å¤§ç‹å›½ã€‚", encoding='utf-8')
    # è¡¨æ ¼å’Œç®€å•åˆ—è¡¨
    (Path(input_dir) / "doc3.md").write_text(
        "# åŠ¿åŠ›æˆå‘˜è¡¨\n\n| å§“å | é—¨æ´¾ | èŒä½ |\n|---|---|---|\n| è§ç‚ | ç‚ç›Ÿ | ç›Ÿä¸» |\n| æ—åŠ¨ | æ­¦å¢ƒ | æ­¦ç¥– |\n\n## åŠŸæ³•æ¸…å•\n- ç„šå†³\n- å¤§è’èŠœç»",
        encoding='utf-8'
    )
    # JSON
    (Path(input_dir) / "doc4.json").write_text(
        json.dumps({"character": "è¯å°˜", "alias": "è¯è€", "occupation": "ç‚¼è¯å¸ˆ", "specialty": "å¼‚ç«"}, ensure_ascii=False),
        encoding='utf-8'
    )
    # ç©ºæ–‡ä»¶
    (Path(input_dir) / "empty.txt").write_text("", encoding='utf-8')
    # é•¿æ–‡æœ¬æ®µè½
    (Path(input_dir) / "long_text.md").write_text(
        "# è®¾å®šï¼šä¹å¤©ä¸–ç•Œ\n\nä¹å¤©ä¸–ç•Œæ˜¯ä¸€ä¸ªå¹¿é˜”æ— å çš„ä¿®ç‚¼å®‡å®™ï¼Œç”±ä¹é‡å¤©ç•Œå±‚å æ„æˆã€‚æ¯ä¸€é‡å¤©ç•Œéƒ½æ‹¥æœ‰ç‹¬ç‰¹çš„æ³•åˆ™å’Œèƒ½é‡ä½“ç³»ï¼Œå±…ä½ç€å½¢æ€å„å¼‚çš„ç”Ÿçµã€‚ä»æœ€ä½çš„ç¬¬ä¸€é‡å¤©åˆ°è‡³é«˜çš„ç¬¬ä¹é‡å¤©ï¼Œçµæ°”æµ“åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä¿®ç‚¼ç¯å¢ƒä¹Ÿæ„ˆå‘ä¸¥è‹›ã€‚ä¼ è¯´ä¸­ï¼Œç¬¬ä¹é‡å¤©ä¹‹ä¸Šï¼Œæ˜¯è§¦åŠæ°¸æ’çš„å½¼å²¸ã€‚ä¸–ç•Œçš„ä¸­å¿ƒæ˜¯â€œå»ºæœ¨â€ï¼Œä¸€æ£µè´¯ç©¿ä¹å¤©ã€è¿æ¥ä¸‡ç•Œçš„é€šå¤©ç¥æ ‘ï¼Œå…¶æå¶å»¶ä¼¸è‡³æ— æ•°ä¸ªä¸‹ä½é¢ï¼Œæ˜¯å®‡å®™èƒ½é‡æµè½¬çš„æ¢çº½ã€‚æ­¦é“ã€ä»™é“ã€é­”é“ã€å¦–é“ç­‰åƒç™¾ç§ä¿®ç‚¼ä½“ç³»åœ¨æ­¤å¹¶å­˜ï¼Œå…±åŒè°±å†™ç€ä¸€æ›²æ³¢æ¾œå£®é˜”çš„å²è¯—ã€‚æ— æ•°å¤©éª„äººæ°ä¸ºäº†äº‰å¤ºæœ‰é™çš„èµ„æºã€è¿½æ±‚æ›´é«˜çš„å¢ƒç•Œï¼Œå±•å¼€äº†æ°¸æ— ä¼‘æ­¢çš„äº‰æ–—ä¸æ¢ç´¢ã€‚",
        encoding='utf-8'
    )
    # åŒ…å«Mermaidå›¾
    (Path(input_dir) / "diagram.md").write_text(
        '# å…³ç³»å›¾ï¼šä¸»è§’å›¢\n\n```mermaid\ngraph TD\n    A[é¾™å‚²å¤©] -->|å¸ˆå¾’| B(é£æ¸…æ‰¬)\n    A -->|å®¿æ•Œ| C(å¶è‰¯è¾°)\n    A -->|æŒšå‹| D(èµµæ—¥å¤©)\n    C -->|åŒé—¨| E(é­”å°Šé‡æ¥¼)\n    B -->|æ›¾å±äº| F(åå±±å‰‘æ´¾)\n```\n\nä¸Šå›¾å±•ç¤ºäº†ä¸»è§’é¾™å‚²å¤©ä¸ä¸»è¦è§’è‰²çš„å…³ç³»ç½‘ç»œã€‚',
        encoding='utf-8'
    )
    # å¤æ‚åµŒå¥—åˆ—è¡¨
    (Path(input_dir) / "complex_list.md").write_text(
        "# ç‰©å“æ¸…å•\n\n- **ç¥å…µåˆ©å™¨**\n  1. èµ¤éœ„å‰‘: é¾™å‚²å¤©çš„ä½©å‰‘ï¼Œå‰Šé“å¦‚æ³¥ã€‚\n  2. è¯›ä»™å››å‰‘: ä¸Šå¤é—ç•™çš„æ€ä¼è‡³å®ï¼Œåˆ†ä¸ºå››æŸ„ã€‚\n     - è¯›ä»™å‰‘\n     - æˆ®ä»™å‰‘\n     - é™·ä»™å‰‘\n     - ç»ä»™å‰‘\n- **çµä¸¹å¦™è¯**\n  - ä¹è½¬è¿˜é­‚ä¸¹: å¯æ´»æ­»äººï¼Œè‚‰ç™½éª¨ã€‚\n  - è©æå­: è¾…åŠ©æ‚Ÿé“ï¼Œæå‡å¿ƒå¢ƒã€‚",
        encoding='utf-8'
    )
    # å¤åˆè®¾è®¡æ–‡æ¡£ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
    (Path(input_dir) / "composite_design_doc.md").write_text(
        """# å·ä¸€ï¼šä¸œæµ·é£äº‘ - ç« èŠ‚è®¾è®¡

æœ¬å·ä¸»è¦å›´ç»•ä¸»è§’é¾™å‚²å¤©åˆå…¥æ±Ÿæ¹–ï¼Œåœ¨ä¸œæµ·åŒºåŸŸç»“è¯†ç›Ÿå‹ã€é­é‡å®¿æ•Œï¼Œå¹¶æœ€ç»ˆæ­å¼€â€œè‹é¾™ä¸ƒå®¿â€ç§˜å¯†ä¸€è§’çš„åºå¹•ã€‚

> **åˆ›ä½œç¬”è®°**: æœ¬å·çš„é‡ç‚¹æ˜¯å¿«èŠ‚å¥çš„å¥‡é‡å’Œäººç‰©å…³ç³»çš„å»ºç«‹ï¼Œä¸ºåç»­æ›´å®å¤§çš„ä¸–ç•Œè§‚é“ºå«ã€‚

![ä¸œæµ·åœ°å›¾](./images/donghai_map.png)

## ç« èŠ‚å¤§çº²

### æµç¨‹å›¾ï¼šé¾™å‚²å¤©æˆé•¿è·¯å¾„
```mermaid
graph LR
    A[åˆå…¥æ±Ÿæ¹–] --> B{é­é‡å±æœº}
    B --> C{è·å¾—å¥‡é‡}
    C --> D[å®åŠ›æå‡]
    D --> A
```

| ç« èŠ‚ | æ ‡é¢˜ | æ ¸å¿ƒäº‹ä»¶ | å‡ºåœºè§’è‰² | å…³é”®åœºæ™¯/ç‰©å“ | å¤‡æ³¨ |
|---|---|---|---|---|---|
| 1.1 | å­¤èˆŸå°‘å¹´ | é¾™å‚²å¤©ä¹˜å­¤èˆŸæŠµè¾¾ä¸´æµ·é•‡ï¼Œåˆé‡èµµæ—¥å¤©ã€‚ | - **é¾™å‚²å¤©** (ä¸»è§’)<br>- èµµæ—¥å¤© (æŒšå‹) | ä¸´æµ·é•‡ç å¤´ã€æµ·é²œé…’æ¥¼ | å¥ å®šæœ¬å·è½»æ¾è¯™è°çš„åŸºè°ƒã€‚ |
| 1.2 | ä¸æ‰“ä¸ç›¸è¯† | é¾™å‚²å¤©ä¸èµµæ—¥å¤©å› è¯¯ä¼šå¤§æ‰“å‡ºæ‰‹ï¼Œç»“ä¸ºå…„å¼Ÿã€‚ | - é¾™å‚²å¤©<br>- èµµæ—¥å¤© | é•‡å¤–ä¹±çŸ³å²— | å±•ç¤ºé¾™å‚²å¤©çš„å‰‘æ³•å’Œèµµæ—¥å¤©çš„æ‹³æ³•ã€‚ |
| 1.3 | é»‘é£å¯¨ä¹‹å± | é»‘é£å¯¨å±±è´¼è¢­æ‰°ä¸´æµ·é•‡ï¼Œæ³èµ°é•‡é•¿ä¹‹å¥³ã€‚ | - é¾™å‚²å¤©<br>- èµµæ—¥å¤©<br>- é»‘é£å¯¨ä¸» (åæ´¾) | ä¸´æµ·é•‡ã€é»‘é£å¯¨ | å¼•å…¥ç¬¬ä¸€ä¸ªå°å†²çªï¼Œä¸»è§’å›¢é¦–æ¬¡åˆä½œã€‚ |
| 1.4 | å¤œæ¢é»‘é£å¯¨ | é¾™å‚²å¤©ä¸èµµæ—¥å¤©æ½œå…¥é»‘é£å¯¨ï¼Œå‘ç°å…¶ä¸åŒ—å†¥é­”æ®¿æœ‰å…³ã€‚ | - é¾™å‚²å¤©<br>- èµµæ—¥å¤© | é»‘é£å¯¨åœ°ç‰¢ | è·å¾—å…³é”®ç‰©å“ï¼š**åŒ—å†¥ä»¤ç‰Œ**ã€‚ |
| 1.5 | å†³æˆ˜é»‘é£å¯¨ | ä¸»è§’å›¢ä¸é»‘é£å¯¨å†³æˆ˜ï¼Œæ•‘å‡ºäººè´¨ï¼Œå¶è‰¯è¾°é¦–æ¬¡ç™»åœºã€‚ | - é¾™å‚²å¤©<br>- èµµæ—¥å¤©<br>- **å¶è‰¯è¾°** (å®¿æ•Œ) | é»‘é£å¯¨èšä¹‰å… | å¶è‰¯è¾°ä»¥å‹å€’æ€§å®åŠ›å‡»è´¥é»‘é£å¯¨ä¸»ï¼Œå¸¦èµ°ä»¤ç‰Œï¼Œä¸é¾™å‚²å¤©ç»“ä¸‹æ¢å­ã€‚ |

## æ ¸å¿ƒè®¾å®šï¼šè‹é¾™ä¸ƒå®¿

â€œè‹é¾™ä¸ƒå®¿â€æ˜¯æµä¼ äºä¸œæµ·ä¹‹ä¸Šçš„å¤è€ä¼ è¯´ï¼Œä¸ä¸ƒä»¶ä¸Šå¤ç¥å™¨åŠæ˜Ÿè¾°ä¹‹åŠ›æœ‰å…³ã€‚

- **è®¾å®šç»†èŠ‚**:
  - **ä¸œæ–¹ä¸ƒå®¿**: è§’ã€äº¢ã€æ°ã€æˆ¿ã€å¿ƒã€å°¾ã€ç®•ã€‚
  - **å¯¹åº”ç¥å™¨**: æ¯å®¿å¯¹åº”ä¸€ä»¶ç¥å™¨ï¼Œå¦‚â€œè§’å®¿â€å¯¹åº”â€œè‹é¾™è§’â€ã€‚
  - **åŠ›é‡ä½“ç³»**:
    ```json
    {
      "system_name": "æ˜Ÿå®¿ä¹‹åŠ›",
      "activation": "é›†é½ä¸ƒä»¶ç¥å™¨ï¼Œäºç‰¹å®šæ—¶è¾°åœ¨ç‰¹å®šåœ°ç‚¹ï¼ˆä¸œæµ·ä¹‹çœ¼ï¼‰ä¸¾è¡Œä»ªå¼ã€‚",
      "effect": "å¯å·ä»¤å››æµ·ï¼Œå¼•åŠ¨æ˜Ÿè¾°ä¹‹åŠ›ï¼Œæ‹¥æœ‰æ¯å¤©ç­åœ°çš„å¨èƒ½ã€‚"
    }
    ```
- **å‰§æƒ…å…³è”**: åŒ—å†¥é­”æ®¿å’Œä¸»è§’å›¢éƒ½åœ¨å¯»æ‰¾è¿™ä¸ƒä»¶ç¥å™¨ã€‚

### å…³é”®æƒ…èŠ‚çº¿ç´¢
- **åŒ—å†¥ä»¤ç‰Œ**: å¶è‰¯è¾°ä»é»‘é£å¯¨å¤ºèµ°çš„ä»¤ç‰Œï¼Œæ˜¯å¯»æ‰¾åŒ—å†¥é­”æ®¿åˆ†èˆµçš„å…³é”®ã€‚
- **é¾™å‚²å¤©çš„èº«ä¸–**: ä¸»è§’çš„èº«ä¸–ä¹‹è°œï¼Œå¯èƒ½ä¸æŸä¸ªéšä¸–å®¶æ—æœ‰å…³ã€‚
- **èµµæ—¥å¤©çš„èƒŒæ™¯**: æŒšå‹èµµæ—¥å¤©çœ‹ä¼¼æ†¨åšï¼Œä½†å…¶æ‹³æ³•è·¯æ•°ä¸å‡¡ï¼ŒèƒŒåæˆ–æœ‰æ•…äº‹ã€‚
""",
        encoding='utf-8'
    )
    # åŒ…å«ç‰¹æ®Šå­—ç¬¦å’Œä¸åŒè¯­è¨€ä»£ç å—çš„æ–‡æ¡£
    (Path(input_dir) / "special_chars_and_code.md").write_text(
        """# ç‰¹æ®Šå†…å®¹æµ‹è¯•

è¿™æ˜¯ä¸€æ®µåŒ…å«å„ç§ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬ï¼š `!@#$%^&*()_+-=[]{};':"\\|,.<>/?~`

## Python ä»£ç ç¤ºä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ª Python å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚

```python
def fibonacci(n):
    a, b = 0, 1
    while a < n:
        print(a, end=' ')
        a, b = b, a+b
```""",
        encoding='utf-8'
    )
    logger.info(f"æµ‹è¯•æ–‡ä»¶å·²å†™å…¥ç›®å½•: {input_dir}")


async def _test_data_ingestion(vector_store: VectorStore, input_dir: str, test_dir: str):
    """æµ‹è¯•ä»ç›®å½•å’Œå•ä¸ªå†…å®¹æ·»åŠ å‘é‡ï¼ŒåŒ…æ‹¬å„ç§è¾¹ç¼˜æƒ…å†µã€‚"""
    # 4. æµ‹è¯•ä»ç›®å½•æ·»åŠ å…¥åº“
    logger.info("--- 4. æµ‹è¯• vector_add_from_dir (å¸¸è§„) ---")
    vector_add_from_dir(vector_store, input_dir, _default_file_metadata)

    # 5. æµ‹è¯• vector_add (å„ç§åœºæ™¯)
    logger.info("--- 5. æµ‹è¯• vector_add (å„ç§åœºæ™¯) ---")
    logger.info("--- 5.1. é¦–æ¬¡æ·»åŠ  ---")
    vector_add(
        vector_store,
        "è™šç©ºä¹‹çŸ³æ˜¯ä¸€ä¸ªç¥ç§˜ç‰©å“ã€‚",
        {"type": "item", "source": "manual_add_1"},
        doc_id="item_void_stone"
    )

    logger.info("--- 5.2. æ›´æ–°æ–‡æ¡£ ---")
    vector_add(
        vector_store,
        "è™šç©ºä¹‹çŸ³æ˜¯ä¸€ä¸ªæå…¶ç¨€æœ‰çš„ç¥ç§˜ç‰©å“ï¼Œæ®è¯´è•´å«ç€å®‡å®™åˆå¼€çš„åŠ›é‡ã€‚",
        {"type": "item", "source": "manual_add_2"},
        doc_id="item_void_stone"
    )

    logger.info("--- 5.3. æ·»åŠ  JSON å†…å®¹ ---")
    json_content = json.dumps({"event": "åŒå¸ä¹‹æˆ˜", "protagonist": ["è§ç‚", "é­‚å¤©å¸"]}, ensure_ascii=False)
    vector_add(
        vector_store,
        content=json_content,
        metadata={"type": "event", "source": "manual_json"},
        content_format="json",
        doc_id="event_doudi"
    )

    logger.info("--- 5.4. æ·»åŠ ç©ºå†…å®¹ (åº”è·³è¿‡) ---")
    added_empty = vector_add(
        vector_store,
        content="  ",
        metadata={"type": "empty"},
        doc_id="empty_content"
    )
    assert not added_empty

    logger.info("--- 5.5. æ·»åŠ åŒ…å«é”™è¯¯ä¿¡æ¯çš„å†…å®¹ (åº”è·³è¿‡) ---")
    added_error = vector_add(
        vector_store,
        content="è¿™æ˜¯ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„æŠ¥å‘Š: ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™ã€‚",
        metadata={"type": "error"},
        doc_id="error_content"
    )
    assert not added_error
    logger.info("åŒ…å«é”™è¯¯ä¿¡æ¯çš„å†…å®¹æœªè¢«æ·»åŠ ï¼ŒéªŒè¯é€šè¿‡ã€‚")

    logger.info("--- 5.6. æ·»åŠ æ— æ³•è§£æå‡ºèŠ‚ç‚¹çš„å†…å®¹ (åº”è·³è¿‡) ---")
    added_no_nodes = vector_add(
        vector_store,
        content="---\n\n---\n",  # ä»…åŒ…å« Markdown åˆ†å‰²çº¿
        metadata={"type": "no_nodes"},
        doc_id="no_nodes_content"
    )
    assert not added_no_nodes
    logger.info("æ— æ³•è§£æå‡ºèŠ‚ç‚¹çš„å†…å®¹æœªè¢«æ·»åŠ ï¼ŒéªŒè¯é€šè¿‡ã€‚")

    # 6. æµ‹è¯•ä»æ— æ•ˆç›®å½•æ·»åŠ 
    logger.info("--- 6. æµ‹è¯• vector_add_from_dir (ç©ºç›®å½•æˆ–ä»…å«æ— æ•ˆæ–‡ä»¶) ---")
    empty_input_dir = os.path.join(test_dir, "empty_input_data")
    os.makedirs(empty_input_dir, exist_ok=True)
    (Path(empty_input_dir) / "unsupported.log").write_text("some log data", encoding='utf-8')
    (Path(empty_input_dir) / "another_empty.txt").write_text("   ", encoding='utf-8')
    added_from_empty = vector_add_from_dir(vector_store, empty_input_dir)
    assert not added_from_empty
    logger.info("ä»ä»…åŒ…å«æ— æ•ˆæ–‡ä»¶çš„ç›®å½•æ·»åŠ ï¼Œè¿”å›Falseï¼ŒéªŒè¯é€šè¿‡ã€‚")


async def _test_node_deletion(vector_store: VectorStore):
    """æµ‹è¯•èŠ‚ç‚¹çš„æ˜¾å¼åˆ é™¤åŠŸèƒ½ã€‚"""
    logger.info("--- 7. æµ‹è¯•æ˜¾å¼åˆ é™¤ ---")
    doc_id_to_delete = "to_be_deleted"
    content_to_delete = "è¿™æ˜¯ä¸€ä¸ªå”¯ä¸€çš„ã€å³å°†è¢«åˆ é™¤çš„èŠ‚ç‚¹XYZ123ã€‚"
    vector_add(
        vector_store,
        content_to_delete,
        {"type": "disposable", "source": "delete_test"},
        doc_id=doc_id_to_delete
    )
    await asyncio.sleep(2)

    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]
    
    filters = MetadataFilters(filters=[ExactMatchFilter(key="ref_doc_id", value=doc_id_to_delete)])
    query_engine_for_check = get_vector_query_engine(vector_store, filters=filters, similarity_top_k=1, rerank_top_n=0)
    response_before = await query_engine_for_check.aquery("any")
    retrieved_nodes_before = response_before.source_nodes

    assert retrieved_nodes_before and content_to_delete in retrieved_nodes_before[0].get_content()
    logger.info("åˆ é™¤å‰èŠ‚ç‚¹å­˜åœ¨ï¼ŒéªŒè¯é€šè¿‡ã€‚")

    vector_store.delete(ref_doc_id=doc_id_to_delete)
    logger.info("å·²è°ƒç”¨åˆ é™¤æ–¹æ³•ã€‚")

    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            del _vector_indices[cache_key]
    logger.info("å·²ä½¿å‘é‡ç´¢å¼•ç¼“å­˜å¤±æ•ˆä»¥åæ˜ åˆ é™¤æ“ä½œã€‚")

    query_engine_after_delete = get_vector_query_engine(vector_store, filters=filters, similarity_top_k=1, rerank_top_n=0)
    response_after = await query_engine_after_delete.aquery("any")
    retrieved_nodes_after = response_after.source_nodes
    assert not retrieved_nodes_after
    logger.success("--- èŠ‚ç‚¹åˆ é™¤æµ‹è¯•é€šè¿‡ ---")


async def _test_node_update(vector_store: VectorStore):
    """æµ‹è¯•èŠ‚ç‚¹çš„æ›´æ–°æ“ä½œï¼ˆé€šè¿‡è¦†ç›–doc_idï¼‰ã€‚"""
    logger.info("--- 8. æµ‹è¯•æ›´æ–°æ“ä½œ ---")
    doc_id_to_update = "to_be_updated"
    content_v1 = "è¿™æ˜¯æ–‡æ¡£çš„åˆå§‹ç‰ˆæœ¬ V1ï¼Œç”¨äºæµ‹è¯•æ›´æ–°åŠŸèƒ½ã€‚"
    content_v2 = "è¿™æ˜¯æ–‡æ¡£æ›´æ–°åçš„ç‰ˆæœ¬ V2ï¼Œæ—§å†…å®¹åº”è¢«è¦†ç›–ã€‚"

    vector_add(
        vector_store,
        content_v1,
        {"type": "update_test", "version": 1},
        doc_id=doc_id_to_update
    )
    await asyncio.sleep(2)

    filters_update = MetadataFilters(filters=[ExactMatchFilter(key="ref_doc_id", value=doc_id_to_update)])
    query_engine_v1 = get_vector_query_engine(vector_store, filters=filters_update, similarity_top_k=1)
    response_v1 = await query_engine_v1.aquery("any")
    retrieved_v1 = response_v1.source_nodes
    assert retrieved_v1 and "V1" in retrieved_v1[0].get_content()
    logger.info("æ›´æ–°å‰ï¼Œç‰ˆæœ¬ V1 å­˜åœ¨ï¼ŒéªŒè¯é€šè¿‡ã€‚")

    vector_add(
        vector_store,
        content_v2,
        {"type": "update_test", "version": 2},
        doc_id=doc_id_to_update
    )
    await asyncio.sleep(2)

    query_engine_v2 = get_vector_query_engine(vector_store, filters=filters_update, similarity_top_k=1)
    response_v2 = await query_engine_v2.aquery("any")
    retrieved_v2 = response_v2.source_nodes
    assert retrieved_v2 and "V2" in retrieved_v2[0].get_content() and "V1" not in retrieved_v2[0].get_content()
    logger.success("--- èŠ‚ç‚¹æ›´æ–°æµ‹è¯•é€šè¿‡ ---")


async def _test_standard_query(vector_store: VectorStore):
    """æµ‹è¯•æ ‡å‡†æŸ¥è¯¢æ¨¡å¼ã€‚"""
    logger.info("--- 9. æµ‹è¯• get_vector_query_engine (æ ‡å‡†æ¨¡å¼) ---")
    query_engine = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=2)
    logger.info(f"æˆåŠŸåˆ›å»ºæ ‡å‡†æŸ¥è¯¢å¼•æ“: {type(query_engine)}")

    questions = [
        "é¾™å‚²å¤©æ˜¯è°ï¼Ÿ",
        "è™šç©ºä¹‹çŸ³æœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
        "è§ç‚æ˜¯ä»€ä¹ˆé—¨æ´¾çš„ï¼Ÿ",
        "è¯è€æ˜¯è°ï¼Ÿ",
        "åŒå¸ä¹‹æˆ˜çš„ä¸»è§’æ˜¯è°ï¼Ÿ",
        "ä¹å¤©ä¸–ç•Œçš„ä¸­å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿ",
        "é¾™å‚²å¤©å’Œå¶è‰¯è¾°æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "è¯›ä»™å››å‰‘åŒ…æ‹¬å“ªäº›ï¼Ÿ",
        "é»‘é£å¯¨å‘ç”Ÿäº†ä»€ä¹ˆäº‹ï¼Ÿ",
        "è‹é¾™ä¸ƒå®¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "é¾™å‚²å¤©çš„æˆé•¿è·¯å¾„æ˜¯æ€æ ·çš„ï¼Ÿ",
        "åŒ—å†¥ä»¤ç‰Œæœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
        "å¦‚ä½•ç”¨pythonè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Ÿ"
    ]
    results = await index_query(query_engine, questions)
    logger.info(f"æ ‡å‡†æŸ¥è¯¢ç»“æœ:\n{results}")
    assert any("é¾™å‚²å¤©" in r for r in results)
    assert any("è™šç©ºä¹‹çŸ³" in r for r in results)
    assert any("è§ç‚" in r and "ç‚ç›Ÿ" in r for r in results)
    assert any("è¯å°˜" in r for r in results)
    assert any("è§ç‚" in r and "é­‚å¤©å¸" in r for r in results)
    assert any("å»ºæœ¨" in r for r in results)
    assert any("å®¿æ•Œ" in r for r in results)
    assert any("æˆ®ä»™å‰‘" in r and "ç»ä»™å‰‘" in r for r in results)
    assert any("é»‘é£å¯¨" in r and "åŒ—å†¥é­”æ®¿" in r for r in results)
    assert any("è‹é¾™ä¸ƒå®¿" in r and "æ˜Ÿå®¿ä¹‹åŠ›" in r for r in results)
    assert any("åˆå…¥æ±Ÿæ¹–" in r and "å®åŠ›æå‡" in r for r in results)
    assert any("åŒ—å†¥é­”æ®¿åˆ†èˆµ" in r for r in results)
    assert any("fibonacci" in r and "def" in r for r in results)
    assert not any("é”™è¯¯ä¿¡æ¯" in r for r in results)
    assert not any("å³å°†è¢«åˆ é™¤" in r for r in results)
    logger.success("--- æ ‡å‡†æŸ¥è¯¢æµ‹è¯•é€šè¿‡ ---")


async def _test_filtered_query(vector_store: VectorStore):
    """æµ‹è¯•å¸¦å›ºå®šå…ƒæ•°æ®è¿‡æ»¤å™¨çš„æŸ¥è¯¢ã€‚"""
    logger.info("--- 10. æµ‹è¯• get_vector_query_engine (å¸¦å›ºå®šè¿‡æ»¤å™¨) ---")
    filters = MetadataFilters(filters=[ExactMatchFilter(key="type", value="item")])
    query_engine_filtered = get_vector_query_engine(vector_store, filters=filters)
    
    results_hit = await index_query(query_engine_filtered, ["ä»‹ç»ä¸€ä¸‹é‚£ä¸ªçŸ³å¤´ã€‚"])
    logger.info(f"å¸¦è¿‡æ»¤å™¨çš„æŸ¥è¯¢ç»“æœ (åº”å‘½ä¸­):\n{results_hit}")
    assert len(results_hit) > 0 and "è™šç©ºä¹‹çŸ³" in results_hit[0]

    results_miss = await index_query(query_engine_filtered, ["é¾™å‚²å¤©æ˜¯è°ï¼Ÿ"])
    logger.info(f"è¢«è¿‡æ»¤å™¨é˜»æŒ¡çš„æŸ¥è¯¢ç»“æœ (åº”æœªå‘½ä¸­):\n{results_miss}")
    assert not results_miss[0]
    logger.success("--- å¸¦å›ºå®šè¿‡æ»¤å™¨çš„æŸ¥è¯¢æµ‹è¯•é€šè¿‡ ---")


async def _test_no_reranker_sync_query(vector_store: VectorStore):
    """æµ‹è¯•æ— é‡æ’å™¨å’ŒåŒæ­¥æŸ¥è¯¢æ¨¡å¼ã€‚"""
    logger.info("--- 11. æµ‹è¯•æ— é‡æ’å™¨å’ŒåŒæ­¥æŸ¥è¯¢ ---")
    query_engine_no_rerank = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=0)
    sync_question = "æ—åŠ¨çš„åŠŸæ³•æ˜¯ä»€ä¹ˆï¼Ÿ"
    sync_response = query_engine_no_rerank.query(sync_question)
    logger.info(f"åŒæ­¥æŸ¥è¯¢ (æ— é‡æ’å™¨) ç»“æœ:\n{sync_response}")
    assert "å¤§è’èŠœç»" in str(sync_response)
    logger.success("--- æ— é‡æ’å™¨å’ŒåŒæ­¥æŸ¥è¯¢æµ‹è¯•é€šè¿‡ ---")


async def _test_auto_retriever_query(vector_store: VectorStore):
    """æµ‹è¯•è‡ªåŠ¨æ£€ç´¢ï¼ˆAutoRetrieverï¼‰æ¨¡å¼ã€‚"""
    logger.info("--- 12. æµ‹è¯• get_vector_query_engine (è‡ªåŠ¨æ£€ç´¢æ¨¡å¼) ---")
    query_engine_auto = get_vector_query_engine(vector_store, use_auto_retriever=True, similarity_top_k=5, rerank_top_n=2)
    logger.info(f"æˆåŠŸåˆ›å»ºè‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“: {type(query_engine_auto)}")

    auto_question = "è¯·æ ¹æ®ç±»å‹ä¸º 'item' çš„æ–‡æ¡£ï¼Œä»‹ç»ä¸€ä¸‹é‚£ä¸ªç‰©å“ã€‚"
    auto_results = await index_query(query_engine_auto, [auto_question])
    logger.info(f"è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢ç»“æœ:\n{auto_results}")
    assert len(auto_results) > 0 and "è™šç©ºä¹‹çŸ³" in auto_results[0]
    logger.success("--- è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢æµ‹è¯•é€šè¿‡ ---")


async def _test_empty_query(vector_store: VectorStore):
    """æµ‹è¯•å¯¹æ— ç»“æœæŸ¥è¯¢çš„å¤„ç†ã€‚"""
    logger.info("--- 13. æµ‹è¯•ç©ºæŸ¥è¯¢ ---")
    query_engine = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=2)
    empty_results = await index_query(query_engine, ["ä¸€ä¸ªä¸å­˜åœ¨çš„æ¦‚å¿µxyz"])
    logger.info(f"ç©ºæŸ¥è¯¢ç»“æœ: {empty_results}")
    assert not empty_results[0]
    logger.success("--- ç©ºæŸ¥è¯¢æµ‹è¯•é€šè¿‡ ---")


if __name__ == '__main__':
    import asyncio
    import shutil
    from pathlib import Path
    import json
    from utils.log import init_logger
    from utils.file import project_root
    from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter
    import nest_asyncio

    init_logger("vector_test")

    nest_asyncio.apply()

    import logging
    logging.getLogger("litellm").setLevel(logging.WARNING)

    test_dir = project_root / ".test" / "vector_test"
    if test_dir.exists():
        shutil.rmtree(test_dir)

    db_path = os.path.join(test_dir, "chroma_db")
    input_dir = os.path.join(test_dir, "input_data")
    os.makedirs(input_dir, exist_ok=True)

    async def main():
        collection_name = "test_collection"
        _prepare_test_data(input_dir)
        
        await _test_embedding_model()
        await _test_reranker()

        vector_store = get_vector_store(db_path=db_path, collection_name=collection_name)

        await _test_data_ingestion(vector_store, input_dir, str(test_dir))

        # åŸ _test_query_and_delete å·²æ‹†åˆ†ä¸ºä»¥ä¸‹ç‹¬ç«‹æµ‹è¯•
        await _test_node_deletion(vector_store)
        await _test_node_update(vector_store)
        await _test_standard_query(vector_store)
        await _test_filtered_query(vector_store)
        await _test_no_reranker_sync_query(vector_store)
        await _test_auto_retriever_query(vector_store)
        await _test_empty_query(vector_store)

    try:
        asyncio.run(main())
        logger.success("æ‰€æœ‰ vector.py æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼")
    finally:
        logger.info(f"æµ‹è¯•å®Œæˆã€‚æµ‹è¯•æ•°æ®ä¿ç•™åœ¨: {test_dir}")
