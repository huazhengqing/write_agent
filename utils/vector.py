import os
import sys
import re
import threading
import asyncio
from datetime import datetime
from pathlib import Path
import json
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
from pydantic import Field
import chromadb
from loguru import logger
from llama_index.core import Settings
from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownElementNodeParser
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.tools import QueryEngineTool
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo, MetadataInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from litellm import arerank, rerank
from llama_index.embeddings.litellm import LiteLLMEmbedding
from llama_index.llms.litellm import LiteLLM
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.llm import llm_temperatures, get_embedding_params, get_llm_params, get_rerank_params
from utils.agent import call_react_agent


def setup_global_settings():
    if getattr(Settings, '_llm', None) is None:
        default_llm_params = get_llm_params(llm_group="fast", temperature=llm_temperatures["summarization"])
        Settings.llm = LiteLLM(**default_llm_params)

setup_global_settings()


_embed_model: Optional[LiteLLMEmbedding] = None
_embed_model_lock = threading.Lock()
def get_embed_model() -> LiteLLMEmbedding:
    global _embed_model
    if _embed_model is None:
        with _embed_model_lock:
            if _embed_model is None:
                logger.info("æ­£åœ¨åˆ›å»ºå¹¶ç¼“å­˜ LiteLLMEmbedding æ¨¡å‹...")
                embedding_params = get_embedding_params()
                embed_model_name = embedding_params.pop('model')
                _embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)
                if getattr(Settings, '_embed_model', None) is None:
                    Settings.embed_model = _embed_model
                logger.success("LiteLLMEmbedding æ¨¡å‹åˆ›å»ºæˆåŠŸã€‚")
    return _embed_model


_vector_stores: Dict[Tuple[str, str], ChromaVectorStore] = {}
_vector_store_lock = threading.Lock()
def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    with _vector_store_lock:
        cache_key = (db_path, collection_name)
        if cache_key in _vector_stores:
            return _vector_stores[cache_key]
        logger.info(f"åˆ›å»ºå¹¶ç¼“å­˜ ChromaDB å‘é‡åº“: path='{db_path}', collection='{collection_name}'")
        os.makedirs(db_path, exist_ok=True)
        db = chromadb.PersistentClient(path=db_path)
        chroma_collection = db.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        _vector_stores[cache_key] = vector_store
        logger.success("ChromaDB å‘é‡åº“åˆ›å»ºæˆåŠŸã€‚")
        return vector_store


_vector_indices: Dict[int, VectorStoreIndex] = {}
_vector_index_lock = threading.Lock()


def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def get_nodes_from_document(doc: Document) -> List[Document]:
    summary_llm_params = get_llm_params(llm_group="fast", temperature=llm_temperatures["summarization"])
    summary_llm = LiteLLM(**summary_llm_params)
    parser = MarkdownElementNodeParser(
        llm=summary_llm,
        chunk_size=256,
        chunk_overlap=50
    )
    nodes = parser.get_nodes_from_documents([doc])
    return nodes


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"å‘é‡åº“å†…å®¹å˜æ›´, ä½¿ç¼“å­˜çš„ VectorStoreIndex å¤±æ•ˆ (key: {cache_key})ã€‚")
            del _vector_indices[cache_key]

    metadata_func = file_metadata_func or _default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    all_nodes = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        nodes = []
        if file_path.suffix == ".json":
            nodes = [doc]
        else:
            # å¯¹äº .md å’Œ .txt, ä½¿ç”¨è¾…åŠ©å‡½æ•°è¿›è¡Œåˆ†å—
            nodes = get_nodes_from_document(doc)

        logger.info(f"  - æ–‡ä»¶ '{file_path.name}' è¢«è§£ææˆ {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())
    index.insert_nodes(all_nodes, show_progress=True)
    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(all_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"å‘é‡åº“å†…å®¹å˜æ›´, ä½¿ç¼“å­˜çš„ VectorStoreIndex å¤±æ•ˆ (key: {cache_key})ã€‚")
            del _vector_indices[cache_key]
    
    index = VectorStoreIndex.from_vector_store(
        vector_store, 
        embed_model=get_embed_model()
    )

    if doc_id:
        try:
            logger.info(f"æ­£åœ¨ä»å‘é‡åº“ä¸­åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹...")
            index.delete_ref_doc(doc_id, delete_from_docstore=True)
            logger.info(f"å·²åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹ã€‚")
        except Exception as e:
            logger.warning(f"åˆ é™¤ doc_id '{doc_id}' çš„æ—§èŠ‚ç‚¹æ—¶å‡ºé”™ (å¯èƒ½æ˜¯é¦–æ¬¡æ·»åŠ ): {e}")

    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    nodes = []
    if content_format == "json":
        nodes = [doc]
    else:
        nodes = get_nodes_from_document(doc)

    if not nodes:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•èŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False

    index.insert_nodes(nodes)
    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


class LiteLLMReranker(BaseNodePostprocessor):
    top_n: int = 3
    rerank_params: Dict[str, Any] = Field(default_factory=dict)
    
    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError("å¿…é¡»æä¾›æŸ¥è¯¢ä¿¡æ¯ (QueryBundle) æ‰èƒ½è¿›è¡Œé‡æ’ã€‚")
        if not nodes:
            return []

        query_str = query_bundle.query_str
        documents = [node.get_content() for node in nodes]

        rerank_request_params = self.rerank_params.copy()
        rerank_request_params.update({
            "query": query_str,
            "documents": documents,
            "top_n": self.top_n,
        })
        
        logger.debug(f"å‘ LiteLLM Reranker å‘é€åŒæ­¥è¯·æ±‚: model={rerank_request_params.get('model')}, top_n={self.top_n}, num_docs={len(documents)}")
        
        response = rerank(**rerank_request_params)

        new_nodes_with_scores = []
        for result in response.results:
            original_node = nodes[result.index]
            original_node.score = result.relevance_score
            new_nodes_with_scores.append(original_node)
        
        logger.debug(f"é‡æ’åè¿”å› {len(new_nodes_with_scores)} ä¸ªèŠ‚ç‚¹ã€‚")
        return new_nodes_with_scores


    async def _aprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError("å¿…é¡»æä¾›æŸ¥è¯¢ä¿¡æ¯ (QueryBundle) æ‰èƒ½è¿›è¡Œé‡æ’ã€‚")
        if not nodes:
            return []

        query_str = query_bundle.query_str
        documents = [node.get_content() for node in nodes]

        rerank_request_params = self.rerank_params.copy()
        rerank_request_params.update({
            "query": query_str,
            "documents": documents,
            "top_n": self.top_n,
        })
        
        logger.debug(f"å‘ LiteLLM Reranker å‘é€å¼‚æ­¥è¯·æ±‚: model={rerank_request_params.get('model')}, top_n={self.top_n}, num_docs={len(documents)}")
        
        response = await arerank(**rerank_request_params)

        new_nodes_with_scores = []
        for result in response.results:
            original_node = nodes[result.index]
            original_node.score = result.relevance_score
            new_nodes_with_scores.append(original_node)
        
        logger.debug(f"é‡æ’åè¿”å› {len(new_nodes_with_scores)} ä¸ªèŠ‚ç‚¹ã€‚")
        return new_nodes_with_scores


def get_default_vector_store_info() -> VectorStoreInfo:
    """
    ä¸ºé¡¹ç›®åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ VectorStoreInfo, å®šä¹‰äº†å¸¸è§çš„å…ƒæ•°æ®å­—æ®µã€‚
    è¿™ä½¿å¾—è‡ªåŠ¨æ£€ç´¢å™¨ (AutoRetriever) èƒ½å¤Ÿç†è§£å…ƒæ•°æ®ç»“æ„å¹¶ç”Ÿæˆè¿‡æ»¤æŸ¥è¯¢ã€‚
    """
    metadata_field_info = [

        MetadataInfo(
            name="source",
            type="str",
            description="æ–‡æ¡£æ¥æºçš„æ ‡è¯†ç¬¦, ä¾‹å¦‚ 'test_doc_1' æˆ–æ–‡ä»¶åã€‚",
        ),
        MetadataInfo(
            name="type",
            type="str",
            description="æ–‡æ¡£çš„ç±»å‹, ä¾‹å¦‚ 'platform_profile', 'character_relation'ã€‚ç”¨äºåŒºåˆ†ä¸åŒç§ç±»çš„å†…å®¹ã€‚",
        ),
        MetadataInfo(
            name="platform",
            type="str",
            description="å†…å®¹ç›¸å…³çš„å¹³å°åç§°, ä¾‹å¦‚ 'çŸ¥ä¹', 'Bç«™', 'èµ·ç‚¹ä¸­æ–‡ç½‘'ã€‚",
        ),
        MetadataInfo(
            name="date",
            type="str",
            description="å†…å®¹çš„åˆ›å»ºæˆ–å…³è”æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ã€‚",
        ),
        MetadataInfo(
            name="word_count",
            type="int",
            description="æ–‡æ¡£çš„å­—æ•°ç»Ÿè®¡",

        ),
    ]
    return VectorStoreInfo(
        content_info="å…³äºæ•…äº‹ã€ä¹¦ç±ã€æŠ¥å‘Šã€å¸‚åœºåˆ†æç­‰çš„æ–‡æœ¬ç‰‡æ®µã€‚",
        metadata_info=metadata_field_info,
    )


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
) -> BaseQueryEngine:
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}"
    )

    # æ­¥éª¤ 1: è·å–æˆ–åˆ›å»º VectorStoreIndex
    # è¿™æ˜¯æ‰€æœ‰æŸ¥è¯¢æ¨¡å¼å…±äº«çš„åŸºç¡€ã€‚
    with _vector_index_lock:
        cache_key = id(vector_store)
        if cache_key in _vector_indices:
            logger.info(f"ä»ç¼“å­˜ä¸­è·å– VectorStoreIndex (key: {cache_key})ã€‚")
            index = _vector_indices[cache_key]
        else:
            logger.info(f"ç¼“å­˜ä¸­æœªæ‰¾åˆ° VectorStoreIndex, æ­£åœ¨åˆ›å»ºå¹¶ç¼“å­˜ (key: {cache_key})ã€‚")
            index = VectorStoreIndex.from_vector_store(
                vector_store,
                embed_model=get_embed_model()
            )
            _vector_indices[cache_key] = index

    # æ­¥éª¤ 2: é…ç½®åå¤„ç†å™¨ (Reranker)
    # Reranker å¯¹ä¸¤ç§æŸ¥è¯¢æ¨¡å¼éƒ½é€‚ç”¨ã€‚
    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        logger.info(f"é…ç½® LiteLLM Reranker åå¤„ç†å™¨, top_n={rerank_top_n}")
        rerank_params = get_rerank_params()
        reranker = LiteLLMReranker(top_n=rerank_top_n, rerank_params=rerank_params)
        postprocessors.append(reranker)

    # æ­¥éª¤ 3: é…ç½®å“åº”åˆæˆå™¨
    # å“åº”åˆæˆå™¨ä¹Ÿå¯¹ä¸¤ç§æ¨¡å¼éƒ½é€‚ç”¨ï¼Œå®ƒè´Ÿè´£å°†æ£€ç´¢åˆ°çš„èŠ‚ç‚¹æ•´åˆæˆæœ€ç»ˆç­”æ¡ˆã€‚
    synthesis_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["synthesis"])
    synthesis_llm = LiteLLM(**synthesis_llm_params)
    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params.get('context_window', 4096),
            num_output=synthesis_llm_params.get('max_tokens', 512),
            chunk_overlap_ratio=0.2
        )
    )

    # æ­¥éª¤ 4: æ ¹æ®æ¨¡å¼åˆ›å»ºå¹¶è¿”å›å…·ä½“çš„æŸ¥è¯¢å¼•æ“
    if use_auto_retriever:
        # è‡ªåŠ¨æ£€ç´¢æ¨¡å¼: ä½¿ç”¨ LLM åŠ¨æ€ç”Ÿæˆå…ƒæ•°æ®è¿‡æ»¤å™¨ã€‚
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        
        # æ­¤æ¨¡å¼éœ€è¦ä¸€ä¸ª "reasoning" LLM æ¥è§£æè‡ªç„¶è¯­è¨€å¹¶ç”Ÿæˆè¿‡æ»¤å™¨ã€‚
        reasoning_llm_params = get_llm_params(llm_group="reasoning", temperature=llm_temperatures["reasoning"])
        reasoning_llm = LiteLLM(**reasoning_llm_params)
        
        final_vector_store_info = vector_store_info or get_default_vector_store_info()
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=final_vector_store_info,
            similarity_top_k=similarity_top_k,
            llm=reasoning_llm,
            verbose=True
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
            node_postprocessors=postprocessors,
        )
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine
    else:
        # æ ‡å‡†æ¨¡å¼: ä½¿ç”¨å›ºå®šçš„è¿‡æ»¤å™¨è¿›è¡Œæ£€ç´¢ã€‚
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼åˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚")
        query_engine = index.as_query_engine(
            # åœ¨æ ‡å‡†æ¨¡å¼ä¸‹, as_query_engine å†…éƒ¨åˆ›å»ºçš„ RetrieverQueryEngine
            # ä¼šä½¿ç”¨æ­¤ LLM è¿›è¡Œå“åº”åˆæˆã€‚æˆ‘ä»¬ä¼ å…¥ä¸“ç”¨çš„ synthesis_llmã€‚
            llm=synthesis_llm,
            response_synthesizer=response_synthesizer,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
        )
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return query_engine


async def index_query(
    query_engine: BaseQueryEngine,
    questions: List[str],
) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    all_nodes: Dict[str, NodeWithScore] = {}

    tasks = []
    for q in questions:
        query_text = f"{q}\n# è¯·ä½¿ç”¨ä¸­æ–‡å›å¤"
        tasks.append(query_engine.aquery(query_text))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    for question, result in zip(questions, results):
        if isinstance(result, Exception):
            logger.warning(f"æŸ¥è¯¢ '{question}' æ—¶å‡ºé”™: {result}")
            continue
        if result and result.source_nodes:
            for node in result.source_nodes:
                all_nodes[node.node.id_] = node

    if not all_nodes:
        logger.info("æ‰€æœ‰æŸ¥è¯¢å‡æœªæ‰¾åˆ°ç›¸å…³çš„æºèŠ‚ç‚¹ã€‚")
        return []

    nodes_in_order = list(all_nodes.values())
    final_content = [re.sub(r"\s+", " ", node.get_content()).strip() for node in nodes_in_order]
    logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œå…±èšåˆäº† {len(final_content)} ä¸ªç‹¬ç‰¹çš„çŸ¥è¯†ç‰‡æ®µã€‚")
    logger.debug(f"è¿”å›çš„çŸ¥è¯†ç‰‡æ®µå†…å®¹: \n{final_content}")
    return final_content


###############################################################################


if __name__ == '__main__':
    import asyncio
    import tempfile
    import shutil
    from pathlib import Path
    import json
    from utils.log import init_logger
    from llama_index.core.vector_stores import MetadataFilters

    init_logger("vector_test")

    # 1. åˆå§‹åŒ–ä¸´æ—¶ç›®å½•
    test_dir = tempfile.mkdtemp()
    db_path = os.path.join(test_dir, "chroma_db")
    input_dir = os.path.join(test_dir, "input_data")
    os.makedirs(input_dir, exist_ok=True)
    logger.info(f"æµ‹è¯•ç›®å½•å·²åˆ›å»º: {test_dir}")

    async def main():
        # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
        (Path(input_dir) / "doc1.md").write_text("# è§’è‰²ï¼šé¾™å‚²å¤©\né¾™å‚²å¤©æ˜¯ä¸€åæ¥è‡ªå¼‚ä¸–ç•Œçš„ç©¿è¶Šè€…ã€‚", encoding='utf-8')
        (Path(input_dir) / "doc2.txt").write_text("ä¸–ç•Œæ ‘æ˜¯å®‡å®™çš„ä¸­å¿ƒï¼Œè¿æ¥ç€ä¹å¤§ç‹å›½ã€‚", encoding='utf-8')
        (Path(input_dir) / "doc3.md").write_text(
            "# åŠ¿åŠ›æˆå‘˜è¡¨\n\n| å§“å | é—¨æ´¾ | èŒä½ |\n|---|---|---|\n| è§ç‚ | ç‚ç›Ÿ | ç›Ÿä¸» |\n| æ—åŠ¨ | æ­¦å¢ƒ | æ­¦ç¥– |\n\n## åŠŸæ³•æ¸…å•\n- ç„šå†³\n- å¤§è’èŠœç»",
            encoding='utf-8'
        )
        (Path(input_dir) / "doc4.json").write_text(
            json.dumps({"character": "è¯å°˜", "alias": "è¯è€", "occupation": "ç‚¼è¯å¸ˆ", "specialty": "å¼‚ç«"}, ensure_ascii=False),
            encoding='utf-8'
        )
        (Path(input_dir) / "empty.txt").write_text("", encoding='utf-8')
        logger.info(f"æµ‹è¯•æ–‡ä»¶å·²å†™å…¥: {input_dir}")

        # 3. æµ‹è¯• get_vector_store
        logger.info("--- 3. æµ‹è¯• get_vector_store ---")
        vector_store = get_vector_store(db_path=db_path, collection_name="test_collection")
        logger.info(f"æˆåŠŸè·å– VectorStore: {vector_store}")

        # 4. æµ‹è¯• vector_add_from_dir
        logger.info("--- 4. æµ‹è¯• vector_add_from_dir ---")
        vector_add_from_dir(vector_store, input_dir, _default_file_metadata)

        # 5. æµ‹è¯• vector_add (é¦–æ¬¡æ·»åŠ )
        logger.info("--- 5. æµ‹è¯• vector_add (å„ç§åœºæ™¯) ---")
        logger.info("--- 5.1. é¦–æ¬¡æ·»åŠ  ---")
        vector_add(
            vector_store, 
            "è™šç©ºä¹‹çŸ³æ˜¯ä¸€ä¸ªç¥ç§˜ç‰©å“ã€‚", 
            {"type": "item", "source": "manual_add_1"}, 
            doc_id="item_void_stone"
        )

        logger.info("--- 5.2. æ›´æ–°æ–‡æ¡£ ---")
        vector_add(
            vector_store, 
            "è™šç©ºä¹‹çŸ³æ˜¯ä¸€ä¸ªæå…¶ç¨€æœ‰çš„ç¥ç§˜ç‰©å“ï¼Œæ®è¯´è•´å«ç€å®‡å®™åˆå¼€çš„åŠ›é‡ã€‚", 
            {"type": "item", "source": "manual_add_2"}, 
            doc_id="item_void_stone"
        )

        logger.info("--- 5.3. æ·»åŠ  JSON å†…å®¹ ---")
        json_content = json.dumps({"event": "åŒå¸ä¹‹æˆ˜", "protagonist": ["è§ç‚", "é­‚å¤©å¸"]}, ensure_ascii=False)
        vector_add(
            vector_store,
            content=json_content,
            metadata={"type": "event", "source": "manual_json"},
            content_format="json",
            doc_id="event_doudi"
        )

        logger.info("--- 5.4. æ·»åŠ ç©ºå†…å®¹ (åº”è·³è¿‡) ---")
        added = vector_add(
            vector_store,
            content="  ",
            metadata={"type": "empty"},
            doc_id="empty_content"
        )
        assert not added

        # 6. æµ‹è¯• get_vector_query_engine (æ ‡å‡†æ¨¡å¼)
        logger.info("--- 6. æµ‹è¯• get_vector_query_engine (æ ‡å‡†æ¨¡å¼) ---")
        query_engine = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=2)
        logger.info(f"æˆåŠŸåˆ›å»ºæ ‡å‡†æŸ¥è¯¢å¼•æ“: {type(query_engine)}")
        
        questions1 = ["é¾™å‚²å¤©æ˜¯è°ï¼Ÿ", "è™šç©ºä¹‹çŸ³æœ‰ä»€ä¹ˆç”¨ï¼Ÿ", "è§ç‚æ˜¯ä»€ä¹ˆé—¨æ´¾çš„ï¼Ÿ", "è¯è€æ˜¯è°ï¼Ÿ", "åŒå¸ä¹‹æˆ˜çš„ä¸»è§’æ˜¯è°ï¼Ÿ"]
        results1 = await index_query(query_engine, questions1)
        logger.info(f"æ ‡å‡†æŸ¥è¯¢ç»“æœ:\n{results1}")
        assert any("é¾™å‚²å¤©" in r for r in results1)
        assert any("è™šç©ºä¹‹çŸ³" in r for r in results1)
        assert any("è§ç‚" in r and "ç‚ç›Ÿ" in r for r in results1)
        assert any("è¯å°˜" in r for r in results1)
        assert any("è§ç‚" in r and "é­‚å¤©å¸" in r for r in results1)

        # 7. æµ‹è¯• get_vector_query_engine (å¸¦å›ºå®šè¿‡æ»¤å™¨)
        logger.info("--- 7. æµ‹è¯• get_vector_query_engine (å¸¦å›ºå®šè¿‡æ»¤å™¨) ---")
        filters = MetadataFilters(filters=[MetadataFilters.ExactMatch(key="type", value="item")])
        query_engine_filtered = get_vector_query_engine(vector_store, filters=filters)
        questions2 = ["ä»‹ç»ä¸€ä¸‹é‚£ä¸ªçŸ³å¤´ã€‚"]
        results2 = await index_query(query_engine_filtered, questions2)
        logger.info(f"å¸¦è¿‡æ»¤å™¨çš„æŸ¥è¯¢ç»“æœ:\n{results2}")
        assert len(results2) > 0 and "è™šç©ºä¹‹çŸ³" in results2[0]
        
        questions3 = ["é¾™å‚²å¤©æ˜¯è°ï¼Ÿ"]  # è¿™ä¸ªæŸ¥è¯¢åº”è¯¥è¢«è¿‡æ»¤å™¨æŒ¡ä½
        results3 = await index_query(query_engine_filtered, questions3)
        logger.info(f"è¢«è¿‡æ»¤å™¨é˜»æŒ¡çš„æŸ¥è¯¢ç»“æœ:\n{results3}")
        assert len(results3) == 0

        # 8. æµ‹è¯•æ— é‡æ’å™¨å’ŒåŒæ­¥æŸ¥è¯¢
        logger.info("--- 8. æµ‹è¯•æ— é‡æ’å™¨å’ŒåŒæ­¥æŸ¥è¯¢ ---")
        query_engine_no_rerank = get_vector_query_engine(vector_store, similarity_top_k=5, rerank_top_n=0)
        sync_question = "æ—åŠ¨çš„åŠŸæ³•æ˜¯ä»€ä¹ˆï¼Ÿ"
        # ä½¿ç”¨ .query() æ¥æµ‹è¯•åŒæ­¥è·¯å¾„
        sync_response = query_engine_no_rerank.query(sync_question)
        logger.info(f"åŒæ­¥æŸ¥è¯¢ (æ— é‡æ’å™¨) ç»“æœ:\n{sync_response}")
        assert "å¤§è’èŠœç»" in str(sync_response)

        # 9. æµ‹è¯• get_vector_query_engine (è‡ªåŠ¨æ£€ç´¢æ¨¡å¼)
        logger.info("--- 9. æµ‹è¯• get_vector_query_engine (è‡ªåŠ¨æ£€ç´¢æ¨¡å¼) ---")
        query_engine_auto = get_vector_query_engine(vector_store, use_auto_retriever=True, similarity_top_k=5, rerank_top_n=2)
        logger.info(f"æˆåŠŸåˆ›å»ºè‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“: {type(query_engine_auto)}")
        
        # è¿™ä¸ªæŸ¥è¯¢åº”è¯¥èƒ½è¢« AutoRetriever è§£æä¸ºé’ˆå¯¹ metadata 'type'='item' çš„è¿‡æ»¤
        auto_question = "è¯·æ ¹æ®ç±»å‹ä¸º 'item' çš„æ–‡æ¡£ï¼Œä»‹ç»ä¸€ä¸‹é‚£ä¸ªç‰©å“ã€‚"
        auto_results = await index_query(query_engine_auto, [auto_question])
        logger.info(f"è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢ç»“æœ:\n{auto_results}")
        assert len(auto_results) > 0 and "è™šç©ºä¹‹çŸ³" in auto_results[0]

        # 10. æµ‹è¯•ç©ºæŸ¥è¯¢
        logger.info("--- 10. æµ‹è¯•ç©ºæŸ¥è¯¢ ---")
        empty_results = await index_query(query_engine, ["ä¸€ä¸ªä¸å­˜åœ¨çš„æ¦‚å¿µxyz"])
        logger.info(f"ç©ºæŸ¥è¯¢ç»“æœ: {empty_results}")
        assert len(empty_results) == 0

    try:
        asyncio.run(main())
        logger.success("æ‰€æœ‰ vector.py æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼")
    finally:
        # æ¸…ç†
        shutil.rmtree(test_dir)
        logger.info(f"æµ‹è¯•ç›®å½•å·²åˆ é™¤: {test_dir}")
