import os
import sys
import re
import threading
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple
import chromadb
from loguru import logger
from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_query_engine import BaseQueryEngine
from llama_index.core.indices.prompt_helper import PromptHelper
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter, get_leaf_nodes
from llama_index.core.postprocessor import LLMRerank
from llama_index.core.retrievers import VectorIndexAutoRetriever
from llama_index.core.tools import QueryEngineTool
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.schema import NodeWithScore
from llama_index.core.vector_stores import MetadataFilters, VectorStoreInfo
from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings_api.litellm import LiteLLMEmbedding
from llama_index.llms_api.litellm import LiteLLM
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.llm import call_react_agent, llm_temperatures, get_embedding_params, get_llm_params


_embed_model: Optional[LiteLLMEmbedding] = None
_embed_model_lock = threading.Lock()
def get_embed_model() -> LiteLLMEmbedding:
    global _embed_model
    if _embed_model is None:
        with _embed_model_lock:
            if _embed_model is None:
                embedding_params = get_embedding_params()
                embed_model_name = embedding_params.pop('model')
                _embed_model = LiteLLMEmbedding(model_name=embed_model_name, **embedding_params)
    return _embed_model


_vector_stores: Dict[Tuple[str, str], ChromaVectorStore] = {}
_vector_store_lock = threading.Lock()
def get_vector_store(db_path: str, collection_name: str) -> ChromaVectorStore:
    with _vector_store_lock:
        if (db_path, collection_name) in _vector_stores:
            return _vector_stores[(db_path, collection_name)]
        os.makedirs(db_path, exist_ok=True)
        db = chromadb.PersistentClient(path=db_path)
        chroma_collection = db.get_or_create_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        _vector_stores[(db_path, collection_name)] = vector_store
        return vector_store


def _default_file_metadata(file_path_str: str) -> dict:
    file_path = Path(file_path_str)
    stat = file_path.stat()
    creation_time = datetime.fromtimestamp(stat.st_ctime).strftime("%Y-%m-%d %H:%M:%S")
    modification_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "file_name": file_path.name,
        "file_path": file_path_str,
        "creation_date": creation_time,
        "modification_date": modification_time,
    }


def get_nodes_from_document(doc: Document) -> List[Document]:
    """
    ä½¿ç”¨æ··åˆç­–ç•¥å°†æ–‡æ¡£è§£æä¸ºç»†ç²’åº¦èŠ‚ç‚¹çš„è¾…åŠ©å‡½æ•°ã€‚
    - ç»“æ„åŒ–è§£æå™¨ï¼šä¼˜å…ˆä½¿ç”¨Markdownè§£æå™¨ä¿æŒæ–‡æ¡£ç»“æ„ã€‚
    - ç»†ç²’åº¦è§£æå™¨ï¼šå¯¹ç»“æ„åŒ–å—è¿›è¡ŒäºŒæ¬¡åˆ‡åˆ†ï¼Œç¡®ä¿èƒ½æ£€ç´¢åˆ°å°ç‰‡æ®µä¿¡æ¯ã€‚
    """
    structural_parser = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True)
    fine_grained_parser = SentenceSplitter(
        chunk_size=256,
        chunk_overlap=50,
        include_metadata=True,
        include_prev_next_rel=True
    )
    # æ­¥éª¤ 1: ç»“æ„åŒ–åˆ†å—
    structural_nodes = structural_parser.get_nodes_from_documents([doc])
    # æ­¥éª¤ 2: å¯¹å¤§å—è¿›è¡Œç»†ç²’åº¦åˆ‡åˆ†
    fine_grained_nodes = fine_grained_parser.get_nodes_from_documents(structural_nodes)
    # ä½¿ç”¨å¶å­èŠ‚ç‚¹è¿›è¡Œç´¢å¼•
    return get_leaf_nodes(fine_grained_nodes)


def vector_add_from_dir(
    vector_store: VectorStore,
    input_dir: str,
    file_metadata_func: Optional[Callable[[str], dict]] = None,
) -> bool:
    metadata_func = file_metadata_func or _default_file_metadata

    reader = SimpleDirectoryReader(
        input_dir=input_dir,
        required_exts=[".md", ".txt", ".json"],
        file_metadata=metadata_func,
        recursive=True,
        exclude_hidden=False
    )

    documents = reader.load_data()
    if not documents:
        logger.warning(f"ğŸ¤· åœ¨ '{input_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶ã€‚")
        return False

    logger.info(f"ğŸ” æ‰¾åˆ° {len(documents)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹è§£æå¹¶æ„å»ºèŠ‚ç‚¹...")

    all_nodes = []
    for doc in documents:
        file_path = Path(doc.metadata.get("file_path", doc.id_))
        if not doc.text.strip():
            logger.warning(f"âš ï¸ æ–‡ä»¶ '{file_path.name}' å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        nodes = []
        if file_path.suffix == ".json":
            nodes = [doc]
        else:
            # å¯¹äº .md å’Œ .txt, ä½¿ç”¨è¾…åŠ©å‡½æ•°è¿›è¡Œåˆ†å—
            nodes = get_nodes_from_document(doc)

        logger.info(f"  - æ–‡ä»¶ '{file_path.name}' è¢«è§£ææˆ {len(nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        all_nodes.extend(nodes)

    if not all_nodes:
        logger.warning("ğŸ¤·â€â™€ï¸ æ²¡æœ‰ä»æ–‡ä»¶ä¸­è§£æå‡ºä»»ä½•å¯ç´¢å¼•çš„èŠ‚ç‚¹ã€‚")
        return False

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=get_embed_model())
    index.insert_nodes(all_nodes, show_progress=True)
    logger.success(f"æˆåŠŸä»ç›®å½• '{input_dir}' æ·»åŠ  {len(all_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡åº“ã€‚")
    return True


def vector_add(
    vector_store: VectorStore,
    content: str,
    metadata: Dict[str, Any],
    content_format: Literal["markdown", "text", "json"] = "markdown",
    doc_id: Optional[str] = None,
) -> bool:
    if not content or not content.strip() or "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™" in content:
        logger.warning(f"ğŸ¤· å†…å®¹ä¸ºç©ºæˆ–åŒ…å«é”™è¯¯ï¼Œè·³è¿‡å­˜å…¥å‘é‡åº“ã€‚å…ƒæ•°æ®: {metadata}")
        return False
    
    final_metadata = metadata.copy()
    if "date" not in final_metadata:
        final_metadata["date"] = datetime.now().strftime("%Y-%m-%d")

    doc = Document(text=content, metadata=final_metadata, id_=doc_id)
    nodes = []
    if content_format == "json":
        nodes = [doc]
    else:
        nodes = get_nodes_from_document(doc)

    if not nodes:
        logger.warning(f"å†…å®¹ (doc_id: {doc_id}) æœªè§£æå‡ºä»»ä½•èŠ‚ç‚¹ï¼Œè·³è¿‡æ·»åŠ ã€‚")
        return False

    index = VectorStoreIndex.from_vector_store(
        vector_store, 
        embed_model=get_embed_model()
    )
    index.insert_nodes(nodes)
    logger.success(f"æˆåŠŸå°†å†…å®¹ (doc_id: {doc_id}, {len(nodes)}ä¸ªèŠ‚ç‚¹) æ·»åŠ åˆ°å‘é‡åº“ã€‚")
    return True


def get_vector_query_engine(
    vector_store: VectorStore,
    filters: Optional[MetadataFilters] = None,
    similarity_top_k: int = 15,
    rerank_top_n: Optional[int] = 3,
    use_auto_retriever: bool = False,
    vector_store_info: Optional[VectorStoreInfo] = None,
) -> BaseQueryEngine:
    logger.info("æ­£åœ¨åˆ›å»ºå‘é‡æŸ¥è¯¢å¼•æ“...")
    logger.debug(
        f"å‚æ•°: similarity_top_k={similarity_top_k}, rerank_top_n={rerank_top_n}, "
        f"use_auto_retriever={use_auto_retriever}, filters={filters}"
    )
    
    reasoning_llm_params = get_llm_params(llm="reasoning", temperature=llm_temperatures["reasoning"])
    reasoning_llm = LiteLLM(**reasoning_llm_params)

    synthesis_llm_params = get_llm_params(llm="reasoning", temperature=llm_temperatures["synthesis"])
    synthesis_llm = LiteLLM(**synthesis_llm_params)

    rerank_llm_params = get_llm_params(llm="fast", temperature=0.0)
    rerank_llm = LiteLLM(**rerank_llm_params)

    postprocessors = []
    if rerank_top_n and rerank_top_n > 0:
        logger.info(f"é…ç½® LLMRerank åå¤„ç†å™¨, top_n={rerank_top_n}")
        reranker = LLMRerank(choice_batch_size=5, top_n=rerank_top_n, llm=rerank_llm)
        postprocessors.append(reranker)

    response_synthesizer = CompactAndRefine(
        llm=synthesis_llm,
        prompt_helper=PromptHelper(
            context_window=synthesis_llm_params.get('context_window', 4096),
            num_output=synthesis_llm_params.get('max_tokens', 512),
            chunk_overlap_ratio=0.2
        )
    )

    index = VectorStoreIndex.from_vector_store(
        vector_store, 
        embed_model=get_embed_model()
    )

    if use_auto_retriever:
        logger.info("ä½¿ç”¨ VectorIndexAutoRetriever æ¨¡å¼ã€‚")
        if not vector_store_info:
            raise ValueError("ä½¿ç”¨è‡ªåŠ¨æ£€ç´¢å™¨æ—¶, å¿…é¡»æä¾› vector_store_infoã€‚")
        
        retriever = VectorIndexAutoRetriever(
            index,
            vector_store_info=vector_store_info,
            similarity_top_k=similarity_top_k,
            verbose=True
        )
        
        logger.success("è‡ªåŠ¨æ£€ç´¢æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
            node_postprocessors=postprocessors,
            use_async=True,
        )
    else:
        logger.info("ä½¿ç”¨æ ‡å‡† as_query_engine æ¨¡å¼ã€‚")
        logger.success("æ ‡å‡†æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸã€‚")
        return index.as_query_engine(
            llm=reasoning_llm,
            response_synthesizer=response_synthesizer,
            filters=filters,
            similarity_top_k=similarity_top_k,
            node_postprocessors=postprocessors,
            use_async=True,
        )


async def index_query(
    query_engine: BaseQueryEngine,
    questions: List[str],
) -> List[str]:
    if not questions:
        return []

    logger.info(f"æ¥æ”¶åˆ° {len(questions)} ä¸ªç´¢å¼•æŸ¥è¯¢é—®é¢˜ã€‚")
    logger.debug(f"é—®é¢˜åˆ—è¡¨: \n{questions}")

    all_nodes: Dict[str, NodeWithScore] = {}

    tasks = []
    for q in questions:
        query_text = f"{q}\n# è¯·ä½¿ç”¨ä¸­æ–‡å›å¤"
        tasks.append(query_engine.aquery(query_text))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    for question, result in zip(questions, results):
        if isinstance(result, Exception):
            logger.warning(f"æŸ¥è¯¢ '{question}' æ—¶å‡ºé”™: {result}")
            continue
        if result and result.source_nodes:
            for node in result.source_nodes:
                all_nodes[node.node.id_] = node

    if not all_nodes:
        logger.info("æ‰€æœ‰æŸ¥è¯¢å‡æœªæ‰¾åˆ°ç›¸å…³çš„æºèŠ‚ç‚¹ã€‚")
        return []

    nodes_in_order = list(all_nodes.values())
    final_content = [re.sub(r"\s+", " ", node.get_content()).strip() for node in nodes_in_order]
    logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œå…±èšåˆäº† {len(final_content)} ä¸ªç‹¬ç‰¹çš„çŸ¥è¯†ç‰‡æ®µã€‚")
    logger.debug(f"è¿”å›çš„çŸ¥è¯†ç‰‡æ®µå†…å®¹: \n{final_content}")
    return final_content


async def index_query_react(
    query_engine: BaseQueryEngine,
    query_str: str,
    agent_system_prompt: Optional[str] = None,
) -> str:
    vector_tool = QueryEngineTool.from_defaults(
        query_engine=query_engine,
        name="vector_search",
        description="ç”¨äºæŸ¥æ‰¾è®¾å®šã€æ‘˜è¦ç­‰è¯­ä¹‰ç›¸ä¼¼çš„å†…å®¹ (ä¾‹å¦‚: è§’è‰²èƒŒæ™¯, ä¸–ç•Œè§‚è®¾å®š, ç‰©å“æè¿°)ã€‚å½“é—®é¢˜æ¯”è¾ƒå¤æ‚æ—¶, ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨æ­¤å·¥å…·æ¥å›ç­”é—®é¢˜çš„ä¸åŒéƒ¨åˆ†, ç„¶åç»¼åˆç­”æ¡ˆã€‚"
    )
    result = await call_react_agent(
        system_prompt=agent_system_prompt,
        user_prompt=query_str,
        tools=[vector_tool],
        llm_type="reasoning",
        temperature=llm_temperatures["reasoning"]
    )
    if not isinstance(result, str):
        logger.warning(f"Agent è¿”å›äº†éå­—ç¬¦ä¸²ç±»å‹, å°†å…¶å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²: {type(result)}")
        result = str(result)
    return result
